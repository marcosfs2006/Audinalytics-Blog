[
  {
    "path": "posts/2022-01-02-visualizao-de-dados-simples-em-auditoria/",
    "title": "Visualização de Dados em Auditoria",
    "description": "Neste post mostramos como criar visualizações de dados simples usando\no pacote {ggplot2} e falamos da importância da visualização de dados\nna auditoria.",
    "author": [
      {
        "name": "Marcos F. Silva",
        "url": {}
      }
    ],
    "date": "2022-01-01",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nIntrodução\r\nDescrição do conjunto de dados\r\nProdução de gráficos\r\n\r\n\r\nIntrodução\r\nNeste post nosso objetivo é mostrar como utilizar o R para implementar visualização de dados em um contexto auditoria. Para isso vamos utilizar um conjunto de dados apresentado no artigo Data Analytics for Financial Statement Audits de autoria de Trevor R. Stewart. Este artigo é um dos seis que integram o livro Audit Analytics and Continuous Audit: Looking Towards the Future, disponível para download gratuitamente.\r\nO conjunto de dados está no arquivo on_the_go_stores.xlsx e é também utilizado no Capítulo 3 do Guia de Auditoria da AICPA Analytical Procedures onde, por meio de um estudo de caso, ilustra a aplicação de procedimentos analíticos tanto no planejamento quanto na aplicação de testes substantivos.\r\nVamos trabalhar o exemplo denominado Simple DA Visualization apresentado no artigo em referência com o objetivo de obter uma melhor compreensão do ambiente de negócio da entidade e identificar riscos de erros materiais.\r\nO conjunto de dados on_the_go_stores.xlsx utilizado neste post pode ser obtido no reposítório Usando R em Auditoria\r\nNo contexto de auditoria financeira a utilização de visualização de dados tem sido cada vez mais enfatizada como técnica indispensável à adequada implementação dos procedimentos analíticos.\r\nNa literatura mais tradicional de auditoria as técnicas mais comumente citadas para utilização em procedimentos analíticos são: (a) Análise de tendências, (b) Análise de quocientes, (c) Testes de razoabilidade, e (d) Análise de regressão.\r\nModernamente essas técnicas podem ser ampliadas para incluir também\r\ntécnicas que compõem o que se chama de aprendizado de máquina (machine learning)\r\nA NBC TA 520 define Procedimentos Analíticos como “…avaliações de informações contábeis por meio de análise das relações plausíveis entre dados financeiros e não financeiros. (…) compreendem, também, o exame necessário de flutuações ou relações identificadas que são inconsistentes com outras informações relevantes ou que diferem significativamente dos valores esperados.”\r\n\r\nDescrição do conjunto de dados\r\nOs dados dizem respeito a uma cadeia de lojas de conveniência chamada On the Go Stores. A rede possui 23 lojas de conveniência localizadas no sudeste dos Estados Unidos. Uma parte dos dados pode ser vista na figura a seguir:\r\n\r\n\r\n\r\nCinco das vinte e três lojas (lojas 1, 4, 10, 13 e 22) abriram durante o ano. As operações variam em razão da localização geográfica e do mix de produtos vendidos. A localização de uma loja baseia-se em diversos fatores tais como a concorrência e o ambiente econômico da localidade.\r\nDe modo geral as operações de uma loja não mudam muito, a não ser que uma nova linha de produtos seja introduzida, como por exemplo a venda de combustível, desconto de cheques ou venda de bilhetes de loteria. O mix de produtos e serviços pode variar e o fator mais importante é se a loja vende combustível. Essas linhas de produtos adicionais em geral afetam o volume de clientes bem como o número de empregados trabalhando em horário integral.\r\n\r\nProdução de gráficos\r\nUso no contexto de conhecimento do negócio\r\nO que vamos fazer é tentar replicar os gráficos apresentados no artigo, um dos quais é reproduzido a seguir:\r\n\r\n\r\n\r\nPara tanto vamos utilizar o pacote ggplot2 que integra o tidyverse.\r\nVamos carregar os pacotes necessários:\r\n\r\n\r\nlibrary(readxl)\r\nlibrary(ggplot2)\r\nlibrary(tidyr)\r\nlibrary(forcats)\r\nlibrary(dplyr)\r\n\r\n\r\n\r\nAgora vamos importar o conjunto de dados:\r\n\r\n\r\nstores <- read_excel(\"on_the_go_stores.xlsx\")\r\nhead(stores)\r\n\r\n\r\n# A tibble: 6 x 7\r\n  store vendas_ano_anterior_aud vendas_ano_corrente inventario_ano_co~\r\n  <dbl>                   <dbl>               <dbl>              <dbl>\r\n1     1                      NA              781793              48725\r\n2     2                 1165221             1146438              44171\r\n3     3                 1147430             1195004              45714\r\n4     4                      NA              951784              37218\r\n5     5                 2037463             1981409              45826\r\n6     6                 2257920             2300671              53862\r\n# ... with 3 more variables: area_deposito <dbl>,\r\n#   qtd_media_empregados <dbl>, vende_gasolina <dbl>\r\n\r\nPara a reprodução do gráfico, será necessário realizar uma pequena modificação no conjunto de dados. Essa modificação consiste em transformar as colunas vendas_ano_anterior_aud e vendas_ano_corrente em uma única variável que chamaremos periodo_vendas. Para isso, vamos utilizar a função pivot_longer() do pacote tidyr.\r\n\r\n\r\nstores_long <- stores %>%\r\n                  pivot_longer(cols = c(\"vendas_ano_anterior_aud\", \"vendas_ano_corrente\"),\r\n                               names_to = \"periodo_vendas\",\r\n                               values_to = \"valor_vendas\")\r\n\r\nhead(stores_long)\r\n\r\n\r\n# A tibble: 6 x 7\r\n  store inventario_ano_~ area_deposito qtd_media_empre~ vende_gasolina\r\n  <dbl>            <dbl>         <dbl>            <dbl>          <dbl>\r\n1     1            48725          2500             11                0\r\n2     1            48725          2500             11                0\r\n3     2            44171          2500             11.3              0\r\n4     2            44171          2500             11.3              0\r\n5     3            45714          2500             12.5              0\r\n6     3            45714          2500             12.5              0\r\n# ... with 2 more variables: periodo_vendas <chr>, valor_vendas <dbl>\r\n\r\nFeita essa modificação, o gráfico pode ser reproduzido da seguinte forma:\r\n\r\n\r\nggplot(stores_long, aes(x=factor(store), y=valor_vendas)) + \r\n  geom_point(aes(shape=periodo_vendas, size=periodo_vendas), color=\"blue\") +\r\n  scale_shape_manual(values = c(1, 20)) +\r\n  scale_size_manual(values = c(4, 3.5)) +\r\n  theme_bw() +\r\n  xlab(\"ID da Loja\") + \r\n  ylab(\"Valor das Vendas\") + \r\n  ggtitle(\"Vendas das Lojas - Ano Atual e Anterior\")\r\n\r\n\r\n\r\n\r\nO gráfico sugere haver um agrupamento das lojas em função da ordenação por ID. É possível identificar os seguintes grupos de lojas: 1-4, 5-9, 10-13, 14-21 e 22-23.\r\nEste aparente agrupamento das lojas pode ser resultado da localização das lojas ou algum outro fator relacionado ao ID da loja ou simplesmente um padrão espúrio que emergiu por coincidência. Alguma investigação adicional será necessária.\r\nO gráfico anterior pode ser modificado para que revele novas informações ao menos de duas outras formas. Em vez de plotar as vendas em função dos valores ordenados do ID das lojas pode-se plotar em função da ordenação dos valores das vendas das lojas com menores vendas para as lojas com maiores vendas. Além disso, pelo fato de sabermos que as lojas que vendem combustível tendem a ter um volume de vendas maior, parece razoável distinguir as que vendem combustível das que não vendem. O gráfico apresentado no artigo é o seguinte:\r\n\r\n\r\n\r\nO código a seguir a mostra como incorporar as duas alterações propostas de forma a produzir um gráfico semelhante ao apresentado no artigo.\r\n\r\n\r\nggplot(stores_long,\r\n       aes(x=fct_reorder2(factor(store), periodo_vendas,valor_vendas, .desc = FALSE),\r\n           y=valor_vendas)) + \r\n  geom_point(aes(shape=periodo_vendas,\r\n                 size=periodo_vendas),\r\n                 color=\"blue\") +\r\n  scale_shape_manual(values = c(1, 20)) +\r\n  scale_size_manual(values = c(4, 3.5)) +\r\n  theme_bw() +\r\n  xlab(\"ID da Loja\") + \r\n  ylab(\"Valor das Vendas\") + \r\n  ggtitle(\"Vendas das Lojas - Ano Atual e Anterior\")+\r\n  facet_wrap(~ vende_gasolina, scales = \"free_x\") \r\n\r\n\r\n\r\n\r\nO gráfico nos mostra que as cinco lojas que abriram durante o ano não vendem combustível e estão entre as que possuem as menores vendas, conforme esperado. Também como esperado, as lojas que vendem combustível possuem um volume de vendas bem maior que as outras. Um exceção notável é a loja n. 9 que parece ter uma performance de vendas igual às lojas que vendem combustível, o que não é algo esperado.\r\nO auditor deve conferir para ver se a loja está corretamente classificada e, se confirmada que é uma loja que não vende combustível, tentar entender as razões sua performance tão superior às demais.\r\n\r\nUso no contexto de testes de razoabilidade\r\nSeguindo ainda no exemplo contido no artigo em referência, outra aplicação de visualização de dados é feita no âmbito da aplicação de um teste de razoabilidade pelo qual faz-se a avaliação das vendas em função da área das lojas (vendas por unidade de área das lojas) comparando-se os valores obtidos com um valor de referência ( $490 ) fornecido pela Associação Nacional das Lojas de Conveniência (NACS, na sigla em inglês).\r\nO gráfico apresentado no artigo é mostrado a seguir:\r\n\r\nPara reproduzir o gráfico acima será necessário antes calcular o valor das vendas por unidade de área da loja. Estas duas variáveis são valor_vendas e area_deposito. A nova variável é o quociente de ambas.\r\n\r\n\r\nstores_long <- stores_long %>% \r\n  mutate(vendas_unid_area =  valor_vendas / area_deposito)\r\n\r\n\r\n\r\nAgora o gráfico acima pode ser replicado conforme mostrado a seguir:\r\n\r\n\r\nstores_long %>% \r\n  filter(periodo_vendas == \"vendas_ano_corrente\") %>%\r\n  mutate(vende_gasolina = factor(vende_gasolina)) %>% \r\nggplot(aes(x=fct_reorder2(factor(store), periodo_vendas,valor_vendas, .desc = FALSE),\r\n           y=vendas_unid_area)) + \r\n  geom_point(aes(shape=vende_gasolina), color=\"blue\", size=4) +\r\n  scale_shape_manual(values = c(1, 8)) +\r\n  geom_hline( yintercept = 490, color=\"red\", size=1 ) +\r\n  theme_bw() +\r\n  xlab(\"ID da Loja (Ordenado por Valor de Vendas)\") + \r\n  ylab(\"Valor das Vendas por 'Square Foot'\") + \r\n  ggtitle(\"Vendas por Unidade de Área das Lojas \\n ( Comparação com Valor de Referência )\")\r\n\r\n\r\n\r\n\r\nO gráfico mostra que as cinco lojas que abriram durante o ano possuem os menores valores de vendas por unidade de área. Lojas que não vendem combustível estão todas abaixo do valor de referência e três das que vendem combústível estão bem acima do valor de referência.\r\n\r\nA figura abaixo, também constante do artigo, mostra que a rotatividade do estoque ( vendas \\(\\div\\) estoque ) é significativamente maior para as lojas que vendem combustível (36 a 50 vezes) do que para as lojas que não vendem combustível (16 a 33 vezes).\r\n\r\nDa mesma forma como feito para o gráfico anterior, será necessário criar uma nova variável que indique a rotatividade do estoque. Isso é feito a seguir:\r\n\r\n\r\nstores_long <- stores_long %>% \r\n  filter(periodo_vendas == \"vendas_ano_corrente\") %>% \r\n  mutate(rotatividade = valor_vendas / inventario_ano_corrente)\r\n\r\n\r\n\r\nAgora o gráfico pode ser replicado conforme mostrado a seguir:\r\n\r\n\r\nstores_long %>%\r\nmutate(store = factor(store),\r\n       vende_gasolina = factor(vende_gasolina)) %>%   \r\nggplot(aes(x = fct_reorder(store, valor_vendas, .desc = FALSE),\r\n           y = rotatividade)) + \r\n  geom_point(aes(shape = vende_gasolina), color = \"blue\", size = 4) +\r\n  scale_shape_manual(values = c(1, 8)) +\r\n  theme_bw() +\r\n  xlab(\"ID da Loja \\n (Ordenado por Valor de Vendas no Ano Corrente)\") + \r\n  ylab(\"Rotatividade do Estoque\") + \r\n  ggtitle(\"Rotatividade do Estoque por Loja\")\r\n\r\n\r\n\r\n\r\nO gráfico evidencia que a rotatividade é significativamente maior para as lojas que vendem gasolina do que para as que não vendem.\r\nE para finalizar, o gráfico a seguir, mostra um diagrama de dispersão das vendas em função do número de empregados. Quando o conjunto de dados é dividido em dois subconjuntos com lojas que vendem combustível e das que não vendem verifica-se a existência de uma forte correlação, da ordem de 80% em cada caso, e o gráfico deixa claro que as lojas que vendem combustível possuem um volume de vendas por empregado maior que as lojas que não vendem combustível.\r\nCaso a análise fosse feita sem que os dados fossem dividos, o modelo de regressão não teria sido adequado e a correlação seria de apenas 15%. Este exemplo ilustra o seguinte ponto: os auditores devem utilizar a análise de dados para obter uma clara compreensão dos mesmos antes de tentar utilizar qualquer modelo.\r\nO artigo apresenta o seguinte gráfico: \r\n\r\nO código a seguir ilustra como obter o gráfico e modelos ajustados.\r\n\r\n\r\n# Correlação para as lojas que não vendem combustivel\r\n\r\nstores_long %>% \r\n  filter(vende_gasolina == 0) %>% \r\n  select(qtd_media_empregados, valor_vendas) %>%\r\n  cor()\r\n\r\n\r\n                     qtd_media_empregados valor_vendas\r\nqtd_media_empregados            1.0000000    0.8177876\r\nvalor_vendas                    0.8177876    1.0000000\r\n\r\n\r\n\r\n# Correlação para as lojas que vendem combustível\r\n\r\nstores_long %>% \r\n  filter(vende_gasolina == 1) %>% \r\n  select(qtd_media_empregados, valor_vendas) %>%\r\n  cor()\r\n\r\n\r\n                     qtd_media_empregados valor_vendas\r\nqtd_media_empregados            1.0000000    0.7805102\r\nvalor_vendas                    0.7805102    1.0000000\r\n\r\n\r\n\r\nstores_long %>% \r\n  mutate( vende_gasolina = factor(vende_gasolina)) %>%  \r\n  ggplot(aes(x = qtd_media_empregados, y = valor_vendas, shape = vende_gasolina)) +\r\n  geom_point( color = \"blue\", size = 4) +  \r\n  geom_smooth(method = \"lm\", se=FALSE, color=\"red\", formula = y ~ x) + \r\n  scale_shape_manual(values = c(1, 8)) +\r\n  geom_text(x = 8,  y = 2250000, label = \"r = 0.780\", stats = \"unique\", color=\"red\") +\r\n  geom_text(x = 12, y = 1500000, label = \"r = 0.817\", stats = \"unique\", color=\"red\") +\r\n  theme_bw() +\r\n  xlab(\"Qtd. Média de Empregados\") + \r\n  ylab(\"Vendas\") + \r\n  ggtitle(\"Correlação entre Vendas e Qtd. Empregados\")\r\n\r\n\r\n\r\n\r\nNota: O artigo inclui no gráfico o coeficiente de determinação (\\(R^2\\)) do modelo de regressão ajustado. Optei por incluir o coeficiente de correlação linear entre as variáveis.\r\nO artigo conclui esclarecendo que as conclusões e insights que o auditor pode extrair da análise realizada irá depender de questões específicas da entidade incluindo as expectativas do auditor.\r\nAs visualizações auxiliam o auditor a perceber padrões e relacionamentos e, ainda, eventuais resultados inesperados. Fica a cargo do auditor decidir, com base no conhecimento que tem da entidade, o que é relevante e o que, se houver, deverá ser objeto de análise adicional.\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-01-02-visualizao-de-dados-simples-em-auditoria/visualizao-de-dados-simples-em-auditoria_files/figure-html5/unnamed-chunk-4-1.png",
    "last_modified": "2022-01-08T21:58:34-03:00",
    "input_file": "visualizao-de-dados-simples-em-auditoria.knit.md",
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-11-22-uma-implmentao-em-r-do-rsf-test/",
    "title": "Uma Implementação em R do RSF Test",
    "description": "Nesse post nosso objetivo é mostrar como implementar o Relative Size Factor Test no R,\nque, sem dúvida nenhuma, é uma excelente opção para análise de dados em auditoria.",
    "author": [
      {
        "name": "Marcos F. Silva",
        "url": {}
      }
    ],
    "date": "2021-11-24",
    "categories": [
      "Detecção de Fraude"
    ],
    "contents": "\r\n\r\nContents\r\nIntrodução\r\nImplementação do Teste em R\r\nElaboração da Função rsf_factor_test()\r\nReferências e materiais adicionais\r\n\r\nIntrodução\r\nNo Capítulo 11 do livro Forensic Analytics (1a Edição), Mark Nigrini descreve e implementa, usando Access e Excel, o denominado Relative Size Factor Test - RSF Test, um teste para detecção de outliers em grupos de registros que compõem uma base de dados.\r\nEssencialmente o teste busca identificar grupos de registros, usualmente definidos por variáveis categóricas, para os quais o maior valor é significativamente maior que os demais valores do grupo.\r\nVamos ilustrar com um exemplo para tornar as coisas mais concretas. Suponha o seguinte conjunto de dados:\r\n\r\nA variável Vendor_No vai definir os grupos de registros, ou seja, cada código de vendedor define um grupo enquanto a variável Invoice_Amount vai ser onde vamos buscar os outliers, ou seja, onde vamos calcular o RSF Factor.\r\nO RSF Factor é calculado tomando-se o quociente entre o maior valor no grupo e o segundo maior valor, também no grupo. Simples assim.\r\nEm termos matemáticos:\r\n\\[\r\nRSF\\_factor = \\frac{maximo\\_valor\\_grupo}{segundo\\_maior\\_valor\\_grupo}\r\n\\]\r\nEntão para cada vendedor vamos identificar as faturas por ele emitidas, identificar os dois valores de interesse (primeiro e segundo maiores valores) e calcular o quociente.\r\nO autor levanta algumas questões de ordem prática relacionadas à implementação deste teste. A primeira questão diz respeito ao fato de que, grupos contendo apenas um valor não pode gerar um RSF_Factor pela simples razão de não existir um segundo maior valor. Também comenta ser uma boa ideia excluir valores menores que 1,00 ou 10,00 e não incluir valores negativos.\r\nO autor aponta também o desafio de identificar o segundo maior valor no grupo, o que requer estabelecer uma regra para definição do segundo maior valor nos casos em que se tem mais de um valor máximo no grupo.\r\nO autor propõe um passo a passo para a implementação do teste e mostra como implementá-lo usando o Microsoft Access e o Maicrosoft Excel.\r\nImplementação do Teste em R\r\nNosso objetivo neste documento é mostrar como o teste pode ser implementado usando o R e para isso vamos inicialmente escrever um script para a execução do teste que posteriormente será convertido em uma função.\r\nVamos utilizar um conjunto de dados simples (RSF.xlsx) para esse propósito.\r\nEsse conjunto de dados pode ser baixado na seguinte página: http://www.ashishmathur.com/compute-relative-size-factor-per-vendor/\r\nVamos à importação dos dados:\r\n\r\n\r\nlibrary(readxl)\r\nrsf <- read_excel(\"_RSF.xlsx\", range=\"A1:C25\")\r\nhead(rsf)\r\n\r\n\r\n# A tibble: 6 x 3\r\n  Vendor_No Invoice_No Invoice_Amount\r\n  <chr>     <chr>               <dbl>\r\n1 V4437     AP00048            21261 \r\n2 V4409     AP0008002          21392.\r\n3 V4550     AP000130           21542.\r\n4 V4437     AP000292           21729.\r\n5 V4526     AP0009             22758.\r\n6 V4429     AP0007402          23413.\r\n\r\nImportado o conjunto de dados e depois de algumas tentativas e erros e consultas à internet chegamos ao seguinte script para a execução do teste:\r\n\r\n\r\nlibrary(dplyr)\r\n\r\nresultado <- rsf %>% \r\n  group_by(Vendor_No) %>% \r\n  summarise(maior_valor = max(Invoice_Amount),\r\n            segundo_maior = nth(unique(Invoice_Amount), 2, order_by = desc(unique(Invoice_Amount))),\r\n            n = n()) %>% \r\n  filter(n > 1) %>% \r\n  mutate(rsf = round(maior_valor / segundo_maior, 3)) %>% \r\n  arrange(desc(rsf))\r\n\r\nresultado\r\n\r\n\r\n# A tibble: 7 x 5\r\n  Vendor_No maior_valor segundo_maior     n   rsf\r\n  <chr>           <dbl>         <dbl> <int> <dbl>\r\n1 V4550          25940.        21542.     2  1.20\r\n2 V4554          28747         25378.     2  1.13\r\n3 V4429          25098.        23413.     2  1.07\r\n4 V4439          25378.        24068.     5  1.05\r\n5 V4437          21729.        21261      2  1.02\r\n6 V4455          25472.        25004      4  1.02\r\n7 V4526          25940.        25753.     3  1.01\r\n\r\nUma coisa que não é feita no livro mas que nos parece interessante é montar um gráfico para visualizar o rsf.\r\n\r\n\r\nlibrary(ggplot2)\r\n\r\nresultado %>% \r\n    ggplot(aes(x=reorder(Vendor_No, rsf), y=rsf)) +\r\n    geom_point(color=\"blue\") +\r\n    xlab(\"Grupos\") +\r\n    ylab(\"RSF Factor\") +\r\n    theme_bw()\r\n\r\n\r\n\r\n\r\nDe acordo com o autor, a aplicação deste teste pode revelar erros de colocação de ponto decimal em dados de contas a pagar; situações nas quais, por exemplo, um valor de 3200.00 é inserido no sistema como 320000. Um erro parcial ocorre quando, por exemplo, um valor de 421.69 é inserido como 4216.90.\r\nOutra observação feita pelo autor é que o RSF Factor é mais assertivo quanto a existência de erro quando o grupo possui muitos registros.\r\nElaboração da Função rsf_factor_test()\r\nCom base na solução apresentada acima é razoavelmente simples elaborar uma função para a aplicação do teste em um conjunto de dados.\r\n\r\n\r\nrsf_factor_test <- function(df, group_column, value_column, exclude_low=FALSE){\r\n\r\n  if(exclude_low){\r\n    df <- df %>% filter(.data[[value_column]] > 1)\r\n  }\r\n  \r\n  df %>% \r\n    group_by(.data[[group_column]]) %>% \r\n    summarise(Max_Value = max(.data[[value_column]]),\r\n              Second_Max = nth(unique(.data[[value_column]]), 2, order_by = desc(unique(.data[[value_column]]))),\r\n              N = n()) %>% \r\n    filter(N > 1) %>% \r\n    mutate(RSF_Factor = round(Max_Value / Second_Max, 3)) %>% \r\n    arrange(desc(RSF_Factor))\r\n  \r\n}\r\n\r\n\r\n\r\nVamos aplicar a função ao nosso conjunto de dados:\r\n\r\n\r\nrsf_factor_test(rsf, \"Vendor_No\", \"Invoice_Amount\")\r\n\r\n\r\n# A tibble: 7 x 5\r\n  Vendor_No Max_Value Second_Max     N RSF_Factor\r\n  <chr>         <dbl>      <dbl> <int>      <dbl>\r\n1 V4550        25940.     21542.     2       1.20\r\n2 V4554        28747      25378.     2       1.13\r\n3 V4429        25098.     23413.     2       1.07\r\n4 V4439        25378.     24068.     5       1.05\r\n5 V4437        21729.     21261      2       1.02\r\n6 V4455        25472.     25004      4       1.02\r\n7 V4526        25940.     25753.     3       1.01\r\n\r\nNaturalmente que algumas melhorias e extensões podem ser incluídas na função. Por exemplo, a função pode calcular o quociente entre o valor máximo e a média do valores do grupo incluindo ou excluindo o valor máximo, como mencionado pelo autor. Por ora vamos deixar a função assim.\r\nReferências e materiais adicionais\r\nPara a elaboração deste documento a referência principal foi o Capítulo 11 do livro Forensic Analytics (1a Edição) do Mark Nigrini já mencionado anteriormente.\r\nPara mais informações sobre esse teste o leitor pode consultar também o seguinte link: https://www.ideascripting.com/Relative-Size-Factor-Test\r\nNo vídeo https://www.youtube.com/watch?v=fyRT84LLbyw Mark Nigrini faz uma revisão do capítulo 7 da 2a Ed. do livro Forensic Analytics que aborda esse tópico e comenta sobre casos reais de fraude.\r\nTambém tem um outro vídeo ( https://www.youtube.com/watch?v=5f-5ZE3uccQ )\r\nno qual o autor demonstra como implementar o teste com o Excel, também utilizando a 2a Edição do livro.\r\nDeixamos para o leitor testar a função no conjunto de dados examinado pelo autor no vídeo (2_Somerville_Sample.xlsx), que pode ser baixado no link disponibilizado na descrição do vídeo.\r\nOutro material interessante é o livro online Audit Analytics with R do Jonathan Lin que aborda esse teste no item “9.4.2 Relative size factor (RSF)”.\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-11-22-uma-implmentao-em-r-do-rsf-test/uma-implmentao-em-r-do-rsf-test_files/figure-html5/unnamed-chunk-3-1.png",
    "last_modified": "2021-11-24T01:47:00-03:00",
    "input_file": "uma-implmentao-em-r-do-rsf-test.knit.md",
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-02-04-expresses-regulares-e-auditoria-ser-que-d-match/",
    "title": "Expressões Regulares e Auditoria: Será que dá match?",
    "description": "O objetivo desse *post* é apresentar possíveis situações em que o uso de\nexpressões regulares pode ser útil em trabalhos de auditoria e com isso\nestimular os auditores a conhecer um pouquinho mais sobre esse recurso.",
    "author": [
      {
        "name": "Marcos F. Silva",
        "url": {}
      }
    ],
    "date": "2021-02-07",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nDuas situações para motivação…\r\nMas o que é uma expressão regular?\r\nOnde aprofundar os conhecimentos\r\n\r\nDuas situações para motivação…\r\nImagine que você está realizando uma auditoria e que após executar alguns testes baseados na Lei de Benford você tenha descoberto que os valores que iniciam pelos caracteres “50”1 tenham fugido muito ao que seria o esperado.\r\n\r\nA Lei de Benford é uma técnica de análise de dados bastante popular em auditoria para a detecção de não conformidades em valores numéricos e integra as denominadas Técnicas de Auditoria Assistidas por Computador - TAACs\r\nPara uma análise mais detalhada das não conformidades identificadas você resolve filtrar a base de dados para obter apenas os registros em que os valores da variável objeto de análise inicie com a string “50”. Expressões regulares podem ser utilizadas para auxiliar na realização desse filtro.\r\nUma outra situação em que conhecer expressões regulares pode ser bastante útil é realizar filtragem de bases de dados com base em variáveis cujos valores sejam texto. Um exemplo disso seria uma base de dados composta por notas de empenhos e se deseja filtrar os dados com base na variável que contenha a descrição da despesa para identificar os empenhos que tenham relação, por exemplo, com “COVID-19”.\r\nEssas são duas situações particulares de um caso mais geral que é aplicar filtros a uma base de dados utilizando expressões regulares.\r\nNa prática o uso de expressões regulares é muito mais amplo e auxilia bastante no processo de pré-processamento das bases de dados.\r\nMas acredito que a essa altura você deva estar se perguntando: “Legal, mas pra mim isso tá parecendo com o recurso de localizar existente no Word. Qual a vantagem de aprender isso?”\r\nDiferentemente do recurso de localizar, onde o “match” só ocorre se as strings coincidirem de forma exata, as expressões regulares são muito mais flexíveis para a definição de padrões que descrevam as strings desejadas e isso pode economizar muito tempo, em razão do auditor não precisar testar várias strings semelhantes.\r\n\r\nDiz-se que ocorreu um match quando o padrão de string expresso pela expressão regular coincide com um subconjunto das strings objeto de pesquisa. Isso ficará mais claro adiante.\r\nMas o que é uma expressão regular?\r\nDe acordo com o tutorial contido no pacote stringr, expressões regulares são ferramentas para a descrição de padrões em strings de caracteres de forma concisa e flexível.\r\nNão é minha intenção que este post seja um tutorial sobre expressões regulares, mas chamar a atenção dos auditores para esse recurso, mostrar possibilidades de aplicações e com isso estimular um aprofundamento dos conhecimentos. E para isso elenco mais adiante alguns recursos disponíveis na internet.\r\nVou usar inicialmente alguns exemplos muito simples para fins didáticos e depois faço aplicações em um conjunto de dados mais realista.\r\nUso funções do pacote stringr para demonstrar o uso das expressões regulares e aproveito para chamar a atenção dos auditores para o fato de que manipular valores textuais é uma habilidade que pode ser muito útil.\r\nConsidere o seguinte conjunto de strings:\r\n\r\n\r\nlibrary(stringr)\r\nlibrary(readr)\r\n\r\nnomes<- c(\"Marcolino\", \"Marcos\", \"José\", \"joão\", \"Marcio\")\r\nnomes\r\n\r\n\r\n[1] \"Marcolino\" \"Marcos\"    \"José\"      \"joão\"      \"Marcio\"   \r\n\r\nVamos agora supor que eu queira identificar nesse conjunto de strings (cada nome é uma string) aquelas que comecem com a letra jota maiúscula ou minúscula. Como deve ser a expressão que ‘casa’ com esse padrão?\r\nA função str_view() do pacote stringr permite verificar para quais strings houve um “match” com o padrão definido pela expressão regular. No exemplo, o padrão é: “string que inicia com a letra jota”.\r\n\r\n\r\nstr_view(string=nomes, pattern=\"^[Jj]\")\r\n\r\n\r\n\r\n{\"x\":{\"html\":\"<ul>\\n  <li>Marcolino<\\/li>\\n  <li>Marcos<\\/li>\\n  <li><span class='match'>J<\\/span>osé<\\/li>\\n  <li><span class='match'>j<\\/span>oão<\\/li>\\n  <li>Marcio<\\/li>\\n<\\/ul>\"},\"evals\":[],\"jsHooks\":[]}\r\nComo é possível ver pelo resultado, as strings José e joão ‘casaram’ com a expressão regular ^[Jj] que define o padrão desejado, ou seja, iniciam com a letra jota, seja ela maiúscula ou minúscula.\r\nA definição de expressões regulares faz uso de caracteres que possuem significado especial os quais são denominados de metacaracteres. Os metacaracteres ^ e [] tem funções bem específicas.\r\nA indicação de que a letra jota deve estar no ínício da string é dada pelo metacaractere ^, enquanto a indicação que o jota poderia ser maiúsculo ou minúsculo é dada pelo metacaractere [] que, no exemplo, informa que a string pode iniciar com qualquer dos caracteres em seu interior.\r\nE se quisermos identificar as strings que terminem com “o” ou “os”?\r\n\r\n\r\nstr_view(string=nomes, pattern=\"os?$\")\r\n\r\n\r\n\r\n{\"x\":{\"html\":\"<ul>\\n  <li>Marcolin<span class='match'>o<\\/span><\\/li>\\n  <li>Marc<span class='match'>os<\\/span><\\/li>\\n  <li>José<\\/li>\\n  <li>joã<span class='match'>o<\\/span><\\/li>\\n  <li>Marci<span class='match'>o<\\/span><\\/li>\\n<\\/ul>\"},\"evals\":[],\"jsHooks\":[]}\r\nEsse exemplo, embora ainda simples, já começa a indicar que as expressões regulares podem ser bem complexas. E de fato são.\r\nNesse exemplo, aparecem mais dois novos metacaracteres: $ e ?. O metacaractere $ é responsável por indicar que os caracteres desejados devem estar no final da string enquanto o metacaractere ? informa que o caractere que o precede é opcional, podendo aparecer ou não na string. Assim, essa expressão regular define o seguinte padrão: strings que terminem com o caractere o sucedido ou não do caractere s.\r\n\r\n\r\nstr_view(string=nomes, pattern=\"[nc]os?$\")\r\n\r\n\r\n\r\n{\"x\":{\"html\":\"<ul>\\n  <li>Marcoli<span class='match'>no<\\/span><\\/li>\\n  <li>Mar<span class='match'>cos<\\/span><\\/li>\\n  <li>José<\\/li>\\n  <li>joão<\\/li>\\n  <li>Marcio<\\/li>\\n<\\/ul>\"},\"evals\":[],\"jsHooks\":[]}\r\nNo exemplo acima, a expressão regular define o seguinte padrão: o caractere o precedido dos caracteres n ou c e sucedido ou não do caractere s no final da string.\r\nNaturalmente que existem muitos outros metacaracteres que não foram abordados nesses exemplos simples cujo objetivo é apenas passar a ideia do que sejam expressões regulares.\r\nConvido o leitor a consultar as referências elencadas mais adiante para um tratamento mais aprofundado do assunto.\r\nAgora vou usar um conjunto de dados um pouco mais realista. Trata-se de uma relação de empenhos obtida no portal da transparência do Tribunal de Contas do Estado da Paraíba.\r\n\r\nA relação de empenhos refere-se apenas à Secretaria Estado de Educação e compreende os meses de janeiro a dezembro de 2020.\r\n\r\n\r\nempenhos <- read_csv(\"_EmpenhosLista.csv\")\r\nhead(empenhos)\r\n\r\n\r\n# A tibble: 6 x 9\r\n  DT_EMPENHO DS_TIPO   NumNE       CD_ORGAO  ElementoDespesa DS_CREDOR\r\n  <chr>      <chr>     <chr>       <chr>     <chr>           <chr>    \r\n1 23/01/2020 PRINCIPAL 2020NE00001 VALOR DA~ 11-VENCIMENTOS~ 08.778.2~\r\n2 23/01/2020 PRINCIPAL 2020NE00002 VALOR DA~ 11-VENCIMENTOS~ 08.778.2~\r\n3 23/01/2020 PRINCIPAL 2020NE00003 VALOR DA~ 11-VENCIMENTOS~ 08.778.2~\r\n4 23/01/2020 PRINCIPAL 2020NE00004 VALOR DA~ 05-OUTROS BENE~ 08.778.2~\r\n5 23/01/2020 PRINCIPAL 2020NE00005 VALOR DA~ 11-VENCIMENTOS~ 08.778.2~\r\n6 23/01/2020 PRINCIPAL 2020NE00006 VALOR DA~ 11-VENCIMENTOS~ 08.778.2~\r\n# ... with 3 more variables: Valor_Despesa1 <chr>, Textbox7 <chr>,\r\n#   Valor_Despesa <chr>\r\n\r\nDurante o processo de exportação dos dados do site, algumas colunas indesejadas foram exportadas também, são elas: Textbox7 e Valor_Despesa. Vou eliminar essas colunas da base de dados:\r\n\r\n\r\nlibrary(dplyr)\r\nempenhos <- empenhos %>% \r\n              select(-Textbox7, -Valor_Despesa)\r\n\r\nglimpse(empenhos)\r\n\r\n\r\nRows: 6,781\r\nColumns: 7\r\n$ DT_EMPENHO      <chr> \"23/01/2020\", \"23/01/2020\", \"23/01/2020\", \"2~\r\n$ DS_TIPO         <chr> \"PRINCIPAL\", \"PRINCIPAL\", \"PRINCIPAL\", \"PRIN~\r\n$ NumNE           <chr> \"2020NE00001\", \"2020NE00002\", \"2020NE00003\",~\r\n$ CD_ORGAO        <chr> \"VALOR DA FOLHA DE PESSOAL - DEMAIS DA EDUCA~\r\n$ ElementoDespesa <chr> \"11-VENCIMENTOS E VANTAGENS FIXAS - PESSOAL ~\r\n$ DS_CREDOR       <chr> \"08.778.250/0001-69 - SECRETARIA DE ESTADO D~\r\n$ Valor_Despesa1  <chr> \"4.999.161,91\", \"2.388,72\", \"103.288,98\", \"1~\r\n\r\nA variável Valor_Despesa1 foi importada como caractere em vez de número. Com o uso de expressões regulares é possível fazer o pré-processamento com vistas à conversão para o formato numérico.\r\nVárias funções no R recebem como argumentos expressões regulares. No código a seguir vou utilizar a função str_replace_all() do pacote stringr para realizar a conversão da variável Valor_Despesa1 para o formato numérico.\r\nEssa função recebe como argumentos a variável contendo as strings, a expressão regular definindo o padrão a ser ‘casado’ e a string a ser utilizada para substituir a string que deu match com o padrão especificado.\r\nPara que a conversão possa ocorrer será necessário: (1) “remover” os pontos e (2) “substituir” as vírgulas por pontos.\r\nNo código a seguir eu utilizo a expressão regular \\\\. para ‘casar’ os pontos e substituir por uma string nula.\r\n\r\n\r\nempenhos <- empenhos %>% \r\n              mutate(Valor_Despesa1 = str_replace_all(Valor_Despesa1, \"\\\\.\", \"\"))\r\nglimpse(empenhos)\r\n\r\n\r\nRows: 6,781\r\nColumns: 7\r\n$ DT_EMPENHO      <chr> \"23/01/2020\", \"23/01/2020\", \"23/01/2020\", \"2~\r\n$ DS_TIPO         <chr> \"PRINCIPAL\", \"PRINCIPAL\", \"PRINCIPAL\", \"PRIN~\r\n$ NumNE           <chr> \"2020NE00001\", \"2020NE00002\", \"2020NE00003\",~\r\n$ CD_ORGAO        <chr> \"VALOR DA FOLHA DE PESSOAL - DEMAIS DA EDUCA~\r\n$ ElementoDespesa <chr> \"11-VENCIMENTOS E VANTAGENS FIXAS - PESSOAL ~\r\n$ DS_CREDOR       <chr> \"08.778.250/0001-69 - SECRETARIA DE ESTADO D~\r\n$ Valor_Despesa1  <chr> \"4999161,91\", \"2388,72\", \"103288,98\", \"14245~\r\n\r\nComo pode ser visto, os pontos foram removidos. A expressão regular \\\\. ‘casa’ os pontos. Como o ponto tem um significado especial em expressões regulares, para que seja possível casar o ‘ponto literal’ é necessário colocar as duas barras antes dele. Em expressões regulares, o ponto tem a função de ‘casar’ com qualquer caractere.\r\nAgora é necessário substituir a vírgula por ponto.\r\n\r\n\r\nempenhos <- empenhos %>% \r\n              mutate(Valor_Despesa1 = str_replace_all(Valor_Despesa1, \",\", \".\"))\r\nglimpse(empenhos)\r\n\r\n\r\nRows: 6,781\r\nColumns: 7\r\n$ DT_EMPENHO      <chr> \"23/01/2020\", \"23/01/2020\", \"23/01/2020\", \"2~\r\n$ DS_TIPO         <chr> \"PRINCIPAL\", \"PRINCIPAL\", \"PRINCIPAL\", \"PRIN~\r\n$ NumNE           <chr> \"2020NE00001\", \"2020NE00002\", \"2020NE00003\",~\r\n$ CD_ORGAO        <chr> \"VALOR DA FOLHA DE PESSOAL - DEMAIS DA EDUCA~\r\n$ ElementoDespesa <chr> \"11-VENCIMENTOS E VANTAGENS FIXAS - PESSOAL ~\r\n$ DS_CREDOR       <chr> \"08.778.250/0001-69 - SECRETARIA DE ESTADO D~\r\n$ Valor_Despesa1  <chr> \"4999161.91\", \"2388.72\", \"103288.98\", \"14245~\r\n\r\nAs vírgulas foram substituídas por pontos. Agora é só converter a variável para o formato numérico.\r\n\r\n\r\nempenhos <- empenhos %>% \r\n              mutate(Valor_Despesa1 = as.numeric(Valor_Despesa1))\r\nglimpse(empenhos)\r\n\r\n\r\nRows: 6,781\r\nColumns: 7\r\n$ DT_EMPENHO      <chr> \"23/01/2020\", \"23/01/2020\", \"23/01/2020\", \"2~\r\n$ DS_TIPO         <chr> \"PRINCIPAL\", \"PRINCIPAL\", \"PRINCIPAL\", \"PRIN~\r\n$ NumNE           <chr> \"2020NE00001\", \"2020NE00002\", \"2020NE00003\",~\r\n$ CD_ORGAO        <chr> \"VALOR DA FOLHA DE PESSOAL - DEMAIS DA EDUCA~\r\n$ ElementoDespesa <chr> \"11-VENCIMENTOS E VANTAGENS FIXAS - PESSOAL ~\r\n$ DS_CREDOR       <chr> \"08.778.250/0001-69 - SECRETARIA DE ESTADO D~\r\n$ Valor_Despesa1  <dbl> 4999161.91, 2388.72, 103288.98, 14245.66, 32~\r\n\r\nAgora a variável Valor_Despesa1 está no formato numérico.\r\nContinuando com nosso exemplo, vamos supor que eu queira filtrar a base de dados de forma que apenas os valores dos empenhos iniciados por “50” sejam selecionados. Usando a expressão regular já vista anteriormente posso fazer isso da seguinte forma:\r\n\r\n\r\nempenhos_50 <- empenhos %>% \r\n                  filter(str_detect(Valor_Despesa1, \"^50\"))\r\nknitr::kable(head(empenhos_50))\r\n\r\n\r\nDT_EMPENHO\r\nDS_TIPO\r\nNumNE\r\nCD_ORGAO\r\nElementoDespesa\r\nDS_CREDOR\r\nValor_Despesa1\r\n12/02/2020\r\nPRINCIPAL\r\n2020NE00253\r\nIMPORTANCIA EMPENHADA PARA A REALIZACAO DA DESPESA COM PAGAMENTO DE DIARIA CONFORME MAPA DE CONCESSAO EM ANEXO\r\n14-DIÁRIAS - CIVIL\r\n*.807.644- - JOSE PATRICIO DA SILVA\r\n50\r\n12/02/2020\r\nPRINCIPAL\r\n2020NE00254\r\nIMPORTANCIA EMPENHADA PARA A REALIZACAO DA DESPESA COM PAGAMENTO DE DIARIA CONFORME MAPA DE CONCESSAO EM ANEXO\r\n14-DIÁRIAS - CIVIL\r\n*.807.644- - JOSE PATRICIO DA SILVA\r\n50\r\n12/02/2020\r\nPRINCIPAL\r\n2020NE00257\r\nIMPORTANCIA EMPENHADA PARA A REALIZACAO DA DESPESA COM PAGAMENTO DE DIARIA CONFORME MAPA DE CONCESSAO EM ANEXO\r\n14-DIÁRIAS - CIVIL\r\n*.421.508- - TIAGO FRANCISO DE SOUSA DA SILVA\r\n50\r\n12/02/2020\r\nPRINCIPAL\r\n2020NE00258\r\nIMPORTANCIA EMPENHADA PARA A REALIZACAO DA DESPESA COM PAGAMENTO DE DIARIA CONFORME MAPA DE CONCESSAO EM ANEXO\r\n14-DIÁRIAS - CIVIL\r\n*.421.508- - TIAGO FRANCISO DE SOUSA DA SILVA\r\n50\r\n13/02/2020\r\nPRINCIPAL\r\n2020NE00261\r\nIMPORTANCIA EMPENHADA PARA A REALIZACAO DA DESPESA COM PAGAMENTO DE DIARIA CONFORME MAPA DE CONCESSAO EM ANEXO\r\n14-DIÁRIAS - CIVIL\r\n*.421.508- - TIAGO FRANCISO DE SOUSA DA SILVA\r\n50\r\n13/02/2020\r\nPRINCIPAL\r\n2020NE00262\r\nIMPORTANCIA EMPENHADA PARA A REALIZACAO DA DESPESA COM PAGAMENTO DE DIARIA CONFORME MAPA DE CONCESSAO EM ANEXO\r\n14-DIÁRIAS - CIVIL\r\n*.421.508- - TIAGO FRANCISO DE SOUSA DA SILVA\r\n50\r\n\r\nMais um exemplo. A variável DS_CREDOR possui a identificação do credor do empenho. Essa identificação consiste do número do CNPJ seguido do nome do fornecedor caso esse seja pessoa jurídica. No caso de pessoa física, essa descrição consiste de parte do CPF seguido do nome da pessoa.\r\nVamos supor que eu queira criar um novo campo na base de dados contendo apenas o CNPJ. Como eu posso fazer isso usando expressões regulares? Eu preciso criar uma expressão regular que defina o padrão de um CNPJ, que é: dois dígitos, ponto, três digitos, ponto, três dígitos, barra, três zeros, um, hífen, dois dígitos. Como eu crio uma expressão regular que “case” com esse padrão?\r\nUma possibilidade:\r\n\r\n\r\npadrao_cnpj <- \"\\\\d{2}\\\\.\\\\d{3}\\\\.\\\\d{3}/0001-\\\\d{2}\"\r\n\r\n\r\n\r\nExplicando um pouco. O metacaractere \\\\d{n} indica que o padrão buscado é n dígitos. O \\\\. indica que queremos ‘casar’ o ponto. Como o ponto possui um significado especial nas expressões regulares (é um metacaractere), é necessário precedê-lo com as duas barras.\r\nVamos agora criar a nova coluna.\r\n\r\n\r\nempenhos <- empenhos %>% \r\n              mutate(CNPJ_CREDOR = str_extract(DS_CREDOR, padrao_cnpj))\r\nglimpse(empenhos)\r\n\r\n\r\nRows: 6,781\r\nColumns: 8\r\n$ DT_EMPENHO      <chr> \"23/01/2020\", \"23/01/2020\", \"23/01/2020\", \"2~\r\n$ DS_TIPO         <chr> \"PRINCIPAL\", \"PRINCIPAL\", \"PRINCIPAL\", \"PRIN~\r\n$ NumNE           <chr> \"2020NE00001\", \"2020NE00002\", \"2020NE00003\",~\r\n$ CD_ORGAO        <chr> \"VALOR DA FOLHA DE PESSOAL - DEMAIS DA EDUCA~\r\n$ ElementoDespesa <chr> \"11-VENCIMENTOS E VANTAGENS FIXAS - PESSOAL ~\r\n$ DS_CREDOR       <chr> \"08.778.250/0001-69 - SECRETARIA DE ESTADO D~\r\n$ Valor_Despesa1  <dbl> 4999161.91, 2388.72, 103288.98, 14245.66, 32~\r\n$ CNPJ_CREDOR     <chr> \"08.778.250/0001-69\", \"08.778.250/0001-69\", ~\r\n\r\nA nova coluna foi criada apenas com o CNPJ como desejado. Mas vamos supor ainda que eu queira cruzar esta base de dados com uma outra tomando o CNPJ como chave para o cruzamento. Ocorre que nessa outra base o CNPJ está sem pontuação, ou seja, os CNPJ aparecem dessa forma: “08778250000169”. Assim eu preciso remover a pontuação na variável recém criada. Mais uma vez vou utilizar expressão regular.\r\n\r\n\r\nempenhos <- empenhos %>% \r\n              mutate(CNPJ_CREDOR = str_remove_all(CNPJ_CREDOR, \"[[:punct:]]\"))\r\nglimpse(empenhos)\r\n\r\n\r\nRows: 6,781\r\nColumns: 8\r\n$ DT_EMPENHO      <chr> \"23/01/2020\", \"23/01/2020\", \"23/01/2020\", \"2~\r\n$ DS_TIPO         <chr> \"PRINCIPAL\", \"PRINCIPAL\", \"PRINCIPAL\", \"PRIN~\r\n$ NumNE           <chr> \"2020NE00001\", \"2020NE00002\", \"2020NE00003\",~\r\n$ CD_ORGAO        <chr> \"VALOR DA FOLHA DE PESSOAL - DEMAIS DA EDUCA~\r\n$ ElementoDespesa <chr> \"11-VENCIMENTOS E VANTAGENS FIXAS - PESSOAL ~\r\n$ DS_CREDOR       <chr> \"08.778.250/0001-69 - SECRETARIA DE ESTADO D~\r\n$ Valor_Despesa1  <dbl> 4999161.91, 2388.72, 103288.98, 14245.66, 32~\r\n$ CNPJ_CREDOR     <chr> \"08778250000169\", \"08778250000169\", \"0877825~\r\n\r\nA expressão regular [[:punct:]] “casa” com os sinais de pontuação.\r\nMais um exemplo para finalizar. A variável CD_ORGAO contém a descrição da despesa objeto do empenho. Como poderíamos identificar os empenhos que se refiram a compra de merenda escolar, por exemplo?\r\nNo código a seguir vou usar expressão regular para “casar” a string MERENDA na descrição da despesa.\r\n\r\n\r\nempenhos_merenda <- empenhos %>% \r\n                      filter(str_detect(CD_ORGAO, \"MERENDA\")) \r\n\r\n\r\n\r\nO novo conjunto de dados empenhos_merenda contém apenas os registros referentes aos empenhos em que a string MERENDA aparece na descrição da despesa.\r\nEspero que estes exemplos tenham dado uma ideia do poder que as expressões regulares trazem para a análise de dados e, consequentemente, para a auditoria e que o post de modo geral tenha aguçado sua curiosidade para aprender mais sobre esta ferramenta fantástica.\r\nOnde aprofundar os conhecimentos\r\nCom uma rápida pesquisa na internet é possível encontrar uma grande quantidade de material sobre expressões regulares.\r\nListo a seguir alguns materiais que vão te ajudar a aprofundar o conhecimento sobre esse tópico.\r\nIntrodução ao regex com R\r\nBasic Regular Expressions in R - Cheat Sheet\r\nR for Data Science - Capítulo 14 Strings\r\nRegular expressions\r\nRegular Expressions in R - Albert Y. Kim\r\n\r\nNesse post vou adotar a seguinte terminologia: um caractere pode ser qualquer dígito, letra, pontuação, etc e uma string será um conjunto de caracteres. Assim, “@marcos2006”, “123456” e “Apt. 708” são strings↩︎\r\n",
    "preview": {},
    "last_modified": "2021-11-24T01:15:12-03:00",
    "input_file": "expresses-regulares-e-auditoria-ser-que-d-match.knit.md"
  },
  {
    "path": "posts/2021-01-27-audit-analytics/",
    "title": "Audit Analytics",
    "description": "Neste _post_ meu objetivo é falar um pouco sobre o que é _Audit Analytics_ e onde encontrar material para estudo.",
    "author": [
      {
        "name": "Marcos F. Silva",
        "url": {}
      }
    ],
    "date": "2021-01-31",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nO que é Audit Analytics?\r\nA Análise de Dados Aplicada à Auditoria Hoje\r\nVantagens da Audit Analytics\r\nMaterial para Estudo\r\n\r\nO que é Audit Analytics?\r\nO uso de análise de dados ou métodos quantitativos ou machine learning em auditoria tem sido apresentado com a denominação de Audit Analytics ou Audit Data Analytics e um breve resumo e introdução a esta ‘disciplina’ pode ser consultado neste documento produzido pela Escola de Negócios da Universidade de Rutgers em 2013.\r\nO documento inicia declarando que “Audit analytics é o uso da tecnologia de análise de dados em Auditoria”. Depois, complementa informando que “Audit analytics é o processo de identificar, colher, validar, analisar e interpretar várias formas de dados dentro de uma organização para ajudar no desenvolvimento do propósito e missão da auditoria.”\r\nTambém é interessante elencar as possíveis aplicações da Audit Analytics no processo de auditoria, ainda segundo o documento em referência:\r\nRevisão analítica;\r\nAvaliação e testes de controles;\r\nTestes substantivos;\r\nDetecção de fraudes;\r\nAnálises em geral e produção de relatórios;\r\nTransações financeiras e não financeiras.\r\nO Guide to Audit Data Analytics da AICPA apresenta uma ‘definição’ de Audit Data Analytics (ADA) da qual gosto muito e que está no artigo Reimagining Auditing in a Wired World de autoria de Paul Byrnes e outros .\r\nDe acordo com os autores do artigo Audit Data Analytics é “a ciência e a arte de descobrir e analisar padrões, identificar anomalias e extrair outras informações úteis de dados subjacentes ou relacionados a um objeto de auditoria através de análise, modelagem ou visualização com o objetivo de planejar ou realizar a auditoria.”\r\nAinda de acordo com o Guia, a ADA “são técnicas que que podem ser usadas para realizar vários procedimentos de auditoria incluindo elementos de avaliação de risco, testes de controles, procedimentos substantivos (isto é, testes de detalhes ou procedimentos analíticos substantivos),ou procedimentos de fechamento de auditoria. ADAs e procedimentos analíticos estão interrelacionados mas nem todas as ADAs são procedimentos analíticos.”\r\nAs definições acima remetem a algumas ideias que me parecem centrais para o entendimento do que seja Audit Analytics.\r\nNo documento da Rutgers essas ideias aparecem bem explícitas na definição, que é o uso de tecnologia, para realizar análise de dados na área de auditoria.\r\nNa definição contida no artigo de Byrnes et all, aparecem as ideias de “descobrir e analisar padrões”, “identificar anomalias”, “extrair outras informações úteis de dados”, “através de análise, modelagem ou visualização” que estão associadas a análise de dados e, como decorrência, a todo o conhecimento técnico relacionado a essa área do conhecimento.\r\nOs dados objeto de análise são aqueles “subjacentes ou relacionados a um objeto de auditoria” com o objetivo de “planejar ou realizar a auditoria”, que fala da área de conhecimento na qual a análise de dados será aplicada, a auditoria. Também aqui, existe um corpo de conhecimento específico com o qual os profissionais oriundos das ciências contábeis estão mais familiarizados.\r\nNessa definição não há menção a nada que remeta à tecnologia. Mas me parece claro que essa omissão talvez tenha relação com o fato de que as tecnologias são passageiras, a todo momento estão surgindo novas e melhores ferramentas capazes de realizar a análise dos dados de forma satisfatória.\r\nA definição do Guia da AICPA faz alusão ao conceito de análise de dados ao associar a Audit Analytic a um conjunto de técnicas e enfatiza bastante a área de aplicação, ao detalhar onde na auditoria podem ser utilizadas.\r\nTambém aqui não há menção à tecnologia. O foco da definição está nas áreas de aplicação e faz todo o sentido que assim seja, visto que o objetivo do Guia é, declaradamente, “fornecer uma introdução e visão geral às técnicas de análise de dados para auxiliar os auditores de demonstrações financeiras na aplicação dessas técnicas durante realização de uma auditoria”.\r\nComo pode ser visto, o Guia da AICPA é direcionado a auditores, auditores de demonstrações financeiras, os quais usualmente são contadores cuja formação, ao menos aqui no Brasil, é deficiente em análise de dados e uso de tecnologia. Não é incomum que os livros texto nacionais de auditoria ainda ensinem a executar testes de auditoria utilizando papel colunado!\r\nPara esse público, o interesse está em saber como essas técnicas podem ser incorporadas em seu trabalho, quais são as vantagens, quais são os custos de adoção e desvantagens.\r\nNesse site, o meu foco será a implementação prática das técnicas de análise de dados em auditoria, utilizando como tecnologia o ambiente R. Não é meu objetivo discutir conceitos de auditoria financeira ou de qualquer outro tipo de auditoria. Para quem quiser conhecer esses conceitos recomento o Manual de Auditoria Financeira do TCU.\r\nUma questão que pode surgir é: que técnicas de análise de dados são essas?\r\nElas variam desde as técnicas mais básicas então conhecidas por Técnicas de Auditoria Assistidas por Computador - TAACs até as modernas técnicas de mineração de dados passando por visualização de dados.\r\nSem a pretensão de ser exaustivo, elenco a seguir algumas técnicas citadas na literatura como utilizadas em trabalhos de auditoria:\r\nTécnicas de Auditoria Assistidas por Computador - TAACs: estatistica descritiva, distribuição de frequencias e tabulação cruzada, sumarização, estratificação, análise de tendências, Aging, teste de duplicidade e gaps, lei de Benford, amostragem de auditoria, etc.\r\nVisualização de Dados:histograma, diagrama de dispersão, gráfico de linhas, gráfico de barras (simples, empilhados e justapostos), boxplot, etc.\r\nTécnicas Preditivas (aprendizado supervisionado): regressão (simples, múltipla, logística), árvores de decisão, máquinas de vetor suporte, etc.\r\nTécnicas Descritivas/Exploratórias (aprendizado não supervisionado): regras de associação, análise de cluster, análise de componentes principais, análise de redes sociais, mineração de texto, mineração de processos, etc.\r\n* etc.\r\nA Análise de Dados Aplicada à Auditoria Hoje\r\nNa introdução do livro Audit Analytics in the Financial Industry Jun Dai e Miklos Vasarhelyi colocam a seguinte questão: O que é Audit Analytics?\r\nEste livro é uma coletânea de artigos editada por Jun Dai, Miklos A. Vasarhelyi e Ann F. Medinets, publicado em 2019.\r\nOs autores observam que a tecnologia emergente da Audit Analytics (AA) vem sendo cada vez mais utilizada pelos auditores para extrair e processar dados oriundos de uma variedade de fontes para identificação de risco, coleta de evidências e, em última análise, dar suporte à tomada de decisão.\r\nAtualmente estão acessíveis aos auditores além dos dados oriundos dos sistemas contábeis dos clientes, dados públicos como os disponibilizados nas mídias sociais e na internet de modo geral, dados abertos governamentais, dados climáticos e dados oriundos da ‘internet das coisas’ (IoT).\r\nO auditor moderno não pode, e não deve, ficar limitado aos dados internos produzidos pela entidade auditada.\r\nOs autores chamam a atenção para o seguinte fato: “Os procedimentos analíticos tradicionais se baseiam fortemente em amostragens dos dados relacionados à auditoria. Não obstante, à medida que os sistemas de ERP estão rapidamente crescendo em popularidade entre as empresas, evidência suficiente não pode mais ser colhida apenas de uma amostra de dados. A Audit Analytics aumenta a população testada de amostras limitadas (subjetiva ou estatística) para milhões de transações na testagem de toda a população o que amplia a cobertura da auditoria de um pequeno percentual das transações para toda a população.”\r\nOs autores vêem a Audit Analytics como sucessora dos procedimentos analíticos que já a bastante tempo vem sendo utilizados pelos auditores externos como técnica para o planejamento, testes substantivos e fase de conclusão de auditoria.\r\nConsiderando que os procedimentos analíticos realizados na fase de planejamento da auditoria tipicamente usam dados agregados em alto nível os resultados obtidos com estes procedimentos fornecem apenas indicação geral inicial sobre a existência de erros materiais.\r\nPor outro lado as técnicas de AA podem ser utilizadas em dados ao nível de transações visto que estas técnicas mantém boa performance ainda quando utilizadas em grandes bases de dados e bases com alta dimensionaliade.\r\nComo resultado a AA pode aumentar a acurácia da avaliação de riscos e melhorar a qualidade do planejamento da auditoria.\r\nEm um artigo de 2003 chamado Audit at a Crossroads, Conan C. Albrech, em vista dos então recentes escândalos de fraude ocorridos em empresas como Enron, WorldCom, Homestore, Quest, Global Crossing, Adelphia, Xerox, Waste Management, Sunbeam e outras, que colocaram a atividade de auditoria independente em xeque e do claro gap de expectativa existente entre o que os auditores afirmam ser sua responsabilidade e o que os usuários das informações contábeis acreditam ser o produto de seu trabalho, já chamava a atenção para a necessidade de uma revisão no modelo de auditoria com vistas a focar na fraude de demostrações financeiras, propondo duas grandes modificações: análise de toda a população e detecção de fraude pró-ativa.\r\nNa visão do autor, existe uma necessidade de que métodos estatísticos e tecnológicos sejam inseridos no processo de auditoria com vistas a focar nas fraudes de demonstrações financeiras e para isso propõe as duas modificações acima elencadas.\r\nEssa proposição assenta no entendimento do autor de que “a disponibilidade de tecnologia e dados em formato digital torna possível realizar rotinas de mineração de dados de formas que historicamente tem sido muito custosa ou mesmo impossível.”\r\nDestaco que a aplicação de técnicas de mineração de dados na detecção de fraudes tem sido tão bem sucedida que uma disciplina própria chamada Fraud Analytics vem se desenvolvendo e ganhando cada vez mais espaço.\r\nVantagens da Audit Analytics\r\nVoltando ao artigo de Jun Dai e Miklos Vasarhelyi, as tecnologias emergentes de análise de dados possuem a capacidade de explorar vastas quantidades de dados em várias estruturas e formatos que não podem ser manipulados pelos procedimentos analíticos tradicionais.\r\nComo vantagens da AA sobre as técnicas mais tradicionais os autories citam: (1) audit data analytics tem um melhor custo benefício em termos de coleta de evidências, (2) muitas das técnicas de análise de dados são escaláveis, isto é, em geral ainda mantém boa performance quando trabalham com grandes quantidades de dados com alta dimensionalidade (muitas variáveis) e (3) algumas técnicas de AA também possuem a habilidade de identificar padrões nos dados com o uso de técnicas não supervisionadas, o que dispensa a necessidade de dados ‘rotulados’ (variáveis alvo) no conjunto de dados.\r\nMaterial para Estudo\r\nInfelizmente não há muito material de estudo em português sobre o tema Audit Data Analytics, mas em inglês já é possível encontrar alguns materiais interessantes.\r\nLivros\r\nAinda são poucos os livros dedicados ao assunto. Listo a seguir os que conheço:\r\nAudit Analytics in the Financial Industry\r\nAudit Analytics and Continuous Audit - Looking Toward the Future Coletânea de artigos disponível para download.\r\nGuide to Audit Data Analytics\r\nAudit Analytics - Data Science for the Accounting Profession\r\nBasic Audit Data Analytics with R\r\nData analytics for internal auditors\r\nData Analytics: Elevating Internal Audit’s Value\r\nTeses e Dissertações\r\nA universidade de Rutgers é um forte centro de pesquisa no uso das modernas técnicas de mineração de dados em auditoria financeira e ao longo dos últimos anos diversas teses de doutoramento foram produzidas abordando a aplicação de análise de dados em auditoria.\r\nListo abaixo algumas das teses que tratam da temática Análise de Dados em Auditoria as quais estão disponíveis para download e podem ser obtidas no seguinte link: https://rucore.libraries.rutgers.edu/\r\nTitulo\r\nAutor\r\nAno de Produção\r\nCluster Analysis for Anomaly Detection in Accounting\r\nSutapat Thiprungsri\r\nJan. 2011\r\nPredictive Audit Analytics: Evolving to a New Era\r\nSiripan Kuenkaikaew\r\nOut. 2013\r\nThe Application of Exploratory Data Analytis in Auditing\r\nQi Liu\r\nOut. 2014\r\nThe Application of Data Visualization in Auditing\r\nAbdullah Alawaddhi\r\nMai. 2015\r\nDeveloping Automated Applications for Clustering and Outlier Detection: Data Mining Implications for Auditing Practice\r\nPaul Eric Byrnes\r\nOut 2015\r\nAnalytics with Exception Prioritization, Consumer Search Volume, and Social Capital\r\nPei Li\r\nMai. 2016\r\nAuditing in Environments of Diverse Data\r\nBasma Moharram\r\nOut. 2016\r\nThree Essays on Audit Technology: Audit 4.0, blockchain, and Audit App\r\nJun Dai\r\nOut. 2017\r\nPublic Auditing, Analytics, and Big Data in the Modern Economy\r\nDeniz Appelbaum\r\nMai., 2017\r\nDesigning Continuous Audit Analytics and Fraud Prevention Systems Using Emerging Technologies\r\nYunsen Wang\r\nMai. 2018\r\nExploring New Audit Evidence: The Application of Process Mining in Auditing\r\nTiffany Chiu\r\nMai. 2018\r\nDeep Learning Applications in Audit Decision Making\r\nTing Sun\r\nMai. 2018\r\nThree Essays on Emerging Technologies in Accounting\r\nFeiqi Huang\r\nJan. 2019\r\nThree Essays on Audit Innovation: Using Social Media Information and Disruptive Technologies to Enhance Audit Quality\r\nAndrea M. Rozario\r\nMai. 2019\r\nApplying Textual Analysis to Auditing\r\nYue Liu\r\nMai. 2019\r\nThree Essays on Open Government Data and Data Analytics\r\nZamil S. Alzamil\r\nMai. 2019\r\nAudit Focused Process Mining: The Evolution of Process Mining and Internal Control\r\nAbddulrahman Alrefai\r\nMai. 2019\r\nThree Essays on the Adoption and Application of Emerging Technologies in Accounting\r\nZhaokai Yan\r\nOut. 2019\r\nArtigos\r\nTembem vale a pena dar uma conferida nos seguintes artigos disponíveis online:\r\nEmbracing the automated audit\r\nThe next frontier in data analytics\r\nIntroduction to Data Analysis for Auditors and Accountants\r\nRethinking the audit\r\nAudit at a Crossroads\r\nMateriais online diversos\r\nListo a seguir alguns materiais que estão disponíveis na internet:\r\nAudit Data Analytics - AICPA\r\nAudit Analytics - An innovative course at Rutgers\r\nAUDIT QUALITY THEMATIC REVIEW: THE USE OF DATA ANALYTICS IN THE AUDIT OF FINANCIAL STATEMENTS\r\nAudit Data Standards Python Example\r\nAudit Solution in R- Case Study: Analysis of General Ledger\r\nAudit Solution in R- Case Study: Analysis of Sales/AR\r\nAnalytical Procedures in R - Audit Data Analytics (ADA) Use Case\r\nraudit\r\nAudit Data Analytics (ADA) - stewartli\r\nData mining your general ledger with Excel\r\nAuditinsight\r\nAudit Analytics: Data Science for the Accounting Profession\r\nAudit Analytics with R - Jonathan Lin\r\nAudit Analytics: Data Science for the Accounting Profession\r\nJon Lin - repositório no GitHub\r\nCaso o leitor tenha conhecimento de algum material não elencado neste post, pode me encaminhar. Vou atualizando o post à medida que for tomando conhecimento de mais materiais.\r\nEspero que gostem.\r\nBoa leitura!\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-01-31T21:58:27-03:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-23-sobre-bigodes-e-violinos/",
    "title": "Sobre Bigodes e Violinos",
    "description": "Algumas considerações sobre alguns gráficos apresentados no capítulo Foundations of Audit Analystics do livro [Audit Analytics](https://www.springer.com/gp/book/9783030490904) do J. C. Westland.",
    "author": [
      {
        "name": "Marcos F. Silva",
        "url": {}
      }
    ],
    "date": "2021-01-24",
    "categories": [
      "Livro Audit Analytics"
    ],
    "contents": "\r\n\r\nContents\r\nConsiderações iniciais\r\nGráficos apresentados no capítulo\r\n\r\nConsiderações iniciais\r\nEm linhas gerais o capítulo trata da exploração grafica de variáveis em um conjunto de dados, dos tipos de variáveis e comenta sobre as estruturas de dados existentes no R (vetores, matrizes, arrays, data frames, listas e fatores) e comenta rapidamente sobre a importância de outras estruturas de dados importantes em contabilidade como as séries temporais, os dados geolocalizados e os grafos.\r\nUma novidade pra mim foi o pacote plotluck que eu não conhecia. O pacote se propõe a apresentar a melhor visualização dos dados com base em suas características.\r\nO livro possui um pacote associado chamado auditanalytics e pode ser instalado a partir do repositório do livo no GitHub onde estão disponíveis os dados utilizados, bem como notebooks com resumos dos capítulos do livro.\r\nO resumo do capítulo 2 está contido no arquivo ch_2_statistics_analytics.Rmd.\r\nA instalação do pacote pode ser feita da seguinte forma:\r\n\r\n\r\n# install.packages(\"devtools\")\r\ndevtools::install_github(\"westland/auditanalytics\")\r\n\r\n\r\n\r\nOs dados utilizados estão armazenados em arquivos csv no próprio pacote e nesse aspecto acho que um ponto de melhoria seria disponibilizá-los no formato .RData com a correspondente documentação como usualmente ocorre nos pacotes do R.\r\nA falta de documentação dos dados, tanto no livro como no repositório e no pacote é uma falha importante visto que uma boa análise de dados pressupõe um bom conhecimento dos mesmos.\r\nA disponibilização dos dados em arquivos csv torna a importação um pouco mais trabalhosa.\r\nGráficos apresentados no capítulo\r\nA motivação para eu escrever este post foi a percepção de que alguns dos gráficos apresentados no capítulo não me pareceram uma boa escolha ou talvez eu não tenha compreendido corretamente a proposta do autor.\r\nO capítulo apresenta ao leitor quatro tipos de gráficos: histograma, gráfico de violino, boxplot e diagrama de dispersão como formas de explorar os dados, o que me faz chamar a atenção para o fato de que os gráficos a serem utilizados pelo auditor em grande medida dependem do tipo de dado que se queira visualizar e que infelizmente o uso de gráficos pelos auditores como uma ferramenta de exploração é ainda bastante incipiente.\r\nO primeiro gráfico que eu gostaria de comentar consta da página 29 e tem por objetivo, nas palavras do autor:\r\n\r\n“Na figura a seguir estamos interessados em saber se a fraude em cartões de crédito é influenciada pelo valor pago ao auditor. A gente analisa uma variável binária examinando a variação em uma outra variável conforme os valores da mesma estejam associados ao valor 0 ou 1 da variável binária.”\r\n\r\nO gráfico em questão, utilizado com vistas a ilustrar a exploração de variáveis binárias, é o seguinte:\r\n\r\nShow code\r\nlibrary(auditanalytics)\r\nlibrary(ggplot2)\r\nlibrary(dplyr)\r\n\r\n# Importação dos dados\r\nsox_stats <- read.csv(system.file(\"extdata\", \"ch_2_data_types.csv\", package=\"auditanalytics\", mustWork=TRUE))\r\n\r\n# Gráfico\r\nggplot(sox_stats, aes(x=non_audit_fee, y=audit_fee, col=as.factor(card)))+\r\n  geom_violin() +\r\n  labs(col=\"Fraud = 1 (green)\")\r\n\r\n\r\n\r\n\r\nA primeira coisa a comentar é o uso do gráfico de violino, que por ser um gráfico muito pouco conhecido, certamente não seria uma opção para a grande maioria dos auditores.\r\nA ajuda da função geom_violin() diz que “O gráfico de violino é uma representação compacta de uma distribuição contínua. É uma mistura do geom_boxplot() e do geom_density(): o gráfico de violino é um gráfico de densidade ‘espelhado’ representado da mesma forma que um boxplot.”\r\nO “box and whisker plot” (gráfico de caixa e bigodes) ou simplesmente boxplot, é um gráfico que tem por objetivo apresentar a distribuição de uma variável quantitativa por intermédio dos quartis da distribuição e indicação de limites superiores e inferiores denominados “cercas”.\r\nO boxplot e o gráfico de violino tem função semelhante, sendo que o gráfico de violino tem a vantagem de mostrar além da variabilidade dos dados e os quartis a forma da distribuição da variável por intermédio de sua densidade.\r\nA seguir apresento o mesmo conjunto de dados usando um boxplot, um gráfico de densidade e um gráfico de violino para tentar realçar a diferença entre os dois. Os dados são apresentados em cinza para dar uma ideia da localização dos mesmos.\r\n\r\nShow code\r\n# Boxplot\r\nsox_stats %>% \r\n  filter(card %in% c(0, 1)) %>% \r\nggplot(aes(y=audit_fee, x=as.factor(card)))+\r\n  geom_boxplot() +\r\n  geom_jitter(color=\"grey\", width = 0.2)\r\n\r\n\r\nShow code\r\n# Density\r\nsox_stats %>% \r\n  filter(card %in% c(0, 1)) %>% \r\nggplot(aes(y=audit_fee))+\r\n  geom_density(fill=\"lightblue\") +\r\n  facet_wrap(~ as.factor(card)) \r\n\r\n\r\nShow code\r\n# Violino\r\nsox_stats %>% \r\n  filter(card %in% c(0, 1)) %>% \r\nggplot(aes(y=audit_fee, x=as.factor(card)))+\r\n  geom_violin(draw_quantiles = c(0.25, 0.5, 0.75), color=\"blue\") +\r\n  geom_jitter(color=\"grey\", width = 0.2)\r\n\r\n\r\n\r\n\r\nExaminando os três gráficos é possível perceber que o gráfico de violino é um gráfico de densidade refletido, o que lhe dá a simetria observada e é também parecido com um boxplot, mostrando a variabilidade dos dados e os quartis.\r\nOnde existe maior concentração de dados, o gráfico de violino expressa isso na “largura” da curva, ou seja, onde tem maior concentração de dados a curva é mais larga.\r\nVoltando ao gráfico apresentado pelo autor, comentei que o objetivo declarado desse gráfico seria avaliar se a fraude em cartões de crédito é influencidada pelo valor pago aos auditores contratados.\r\nO gráfico faz uso de 3 variáveis: audit_fee, non_audit_fee e card e como já disse a base de dados não está comentada, não possuindo informações sobre o significado de cada variável e a estrutura geral dos dados. Assim, será necessário deduzir algumas coisas, o que não é o que se deve fazer na prática. O auditor deve buscar compreender com a maior clareza possível o significado de cada variável e suas particularidades para que possa ter condições de identificar situações que fujam da normalidade.\r\nAparentemente a variável card indica se a observação refere-se à fraude ou não. As outras variáveis indicam o valor pago aos auditores em razão de serviços contratados de auditoria e serviços não relacionados a auditoria.\r\nA primeira coisa que não fica clara no gráfico apresentado é o uso da variável non_audit_fee, uma variável quantitativa contínua, no eixo dos x. Considerando que o objetivo de um gráfico de violino é representar a distribuição da variável no eixo y, que deve ser quantitativa, a variável non_audit_fee não traz informação adicional para o gráfico.\r\nO mesmo problema ocorre com o gráfico apresentado na página 30, e que reproduzo a seguir utilizando o código apresentado no livro:\r\n\r\nShow code\r\nlibrary(tidyr)\r\n\r\nsox_stats$card <- as.integer(sox_stats$card)\r\n\r\nsox_stats1 <- gather(sox_stats,\r\n                     key=\"metric\",\r\n                     value = value,\r\n                     effective_303,\r\n                     mat_weak_303,\r\n                     sig_def_303,\r\n                     effective_404,\r\n                     auditor_agrees_303)\r\n\r\nggplot(sox_stats1, aes(x=non_audit_fee, y=audit_fee, col=metric)) +\r\n  geom_violin() +\r\n  scale_x_continuous(trans = \"log2\") +\r\n  scale_y_continuous(trans = \"log2\")\r\n\r\n\r\n\r\n\r\nO gráfico mostra os “violinos” igualmente espaçados e mostrando a mesma distribuição para a variável audit_fee para todos os valores da variável metric.\r\nNovamente aqui a variável non_audit_fee parece não ter qualquer influência no gráfico. Chama a atenção também o fato dos “violinos” serem todos iguais. O fato de não conhecermos em mais detalhes a base de dados dificulta a inspeção em busca de confirmação quanto a correção do resultado apresentado.\r\nO fato é que não é possível extrair maiores informações nem confirmar a exatidão do resultado sem um maior conhecimento dos dados.\r\nTambém os gráficos apresentados nas páginas 31 e 32 não me pareceram uma boa escolha para o propósito desejado.\r\nAqui o objetivo do autor é ilustrar a análise de variáveis ordinais, mais especificamente omissões ou duplicidades em variáveis que possuem valores sequenciais, tais como os números das notas fiscais.\r\nO gráfico a seguir, apresentado pelo autor, tem o objetivo de permitir a identificação visual de faturas duplicadas.\r\n\r\nShow code\r\nlibrary(lubridate)\r\n#library(kableExtra)\r\n\r\n## função para gerar datas aleatórias no ano corrente\r\nrdate <- function(x,\r\n                  min = paste0(format(Sys.Date(), '%Y'), '-01-01'),\r\n                  max = paste0(format(Sys.Date(), '%Y'), '-12-31'),\r\n                  sort = TRUE) {\r\n  dates <- sample(seq(as.Date(min), as.Date(max), by = \"day\"), x, replace = TRUE)\r\n  if (sort == TRUE) {\r\n    sort(dates)\r\n  } else {\r\n    dates\r\n  }\r\n}\r\n\r\n## Cria um data frame com 2 coluna e preenche com os valores 1 a 1000 \r\ninvoice_no <- date <- 1:1000  ## placeholder\r\njournal_ent_no <- cbind.data.frame(invoice_no,date)\r\n\r\n# Sorteia 1000 datas entre 01-01-2021 e 31-12-2021 e ordena\r\ndate <- rdate(1000)\r\n\r\n# Substitui os valores no campo 'data' pelas datas sorteadas\r\njournal_ent_no$date <- date[order(date)]\r\n\r\n# Adiciona duplicidades \r\njournal_ent_no$invoice_no <- seq(1,1000) + rbinom(1000,1,.1) # add some errors\r\n\r\n# Cria um novo data frame com identificação das duplicidades.\r\nduplicates <- duplicated(journal_ent_no$invoice_no)\r\nraw <- seq(1,1000)\r\njournal_dups <- cbind.data.frame(raw,duplicates)\r\n\r\n# Faz o gráfico\r\nggplot(journal_dups, aes(x=invoice_no, y=raw, col=duplicates)) + \r\n  geom_point()\r\n\r\n\r\n\r\n\r\nComo pode ser visto as duplicidades, em azul, não sobressaem muito. Como o espaço para o gráfico é pequeno os pontos se sobrepõem, dificultando a visualização.\r\nVou mostrar os registros iniciais do conjunto de dados utilizado para fazer esse gráfico:\r\n\r\nShow code\r\nhead(journal_dups, 10)\r\n\r\n\r\n   raw duplicates\r\n1    1      FALSE\r\n2    2      FALSE\r\n3    3      FALSE\r\n4    4      FALSE\r\n5    5      FALSE\r\n6    6      FALSE\r\n7    7      FALSE\r\n8    8      FALSE\r\n9    9      FALSE\r\n10  10      FALSE\r\n\r\nO conjunto de dados consiste em uma coluna indicando a numeração sequencial de 1 a 1000 e outra indicando se o número está duplicado na base ou não. Esta base de dados é derivada do que seria a base “original” que apresento a seguir:\r\n\r\nShow code\r\nhead(journal_ent_no, 10)\r\n\r\n\r\n   invoice_no       date\r\n1           1 2021-01-01\r\n2           2 2021-01-01\r\n3           3 2021-01-04\r\n4           4 2021-01-04\r\n5           5 2021-01-04\r\n6           6 2021-01-04\r\n7           7 2021-01-04\r\n8           8 2021-01-05\r\n9           9 2021-01-05\r\n10         10 2021-01-05\r\n\r\nA base de dados possui o número sequencial (invoice_no) e a data de emissão (date). Será que tem uma forma melhor de “visualizar” as duplicidades?\r\nVou apresentar aqui minha proposta:\r\n\r\nShow code\r\nggplot(journal_dups) + \r\n  geom_vline(xintercept = invoice_no,\r\n             color=ifelse(duplicates, \"blue\", \"white\"))+\r\n  labs(x=\"Numeração Sequencial\")\r\n\r\n\r\n\r\n\r\nAs linhas em azul indicam as faturas duplicadas. Podemos ver que a distribuição das duplicidades não aparenta ter um padrão definido.\r\nNa minha opinião a visualização das duplicidades ficou um pouco melhor. Naturalmente que quanto maior a quantidade de dados mais difícil ficará a visualização, principalmente se a mesma referir-se a visualizar os dados inidividualmente, como é o caso apresentado pelo autor.\r\nÉ claro que com um simples filtro é possível obter exatamente os números das faturas duplicadas, mas seria difícil perceber qualquer padrão nos dados caso eles existissem.\r\nO gráfico na página 32 é muito parecido mas busca identificar faturas omitidas. Acredito que a mesma solução pode ser usada na visualização.\r\nBem, por ora é tudo. Espero que tenham gostado.\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-01-23-sobre-bigodes-e-violinos/sobre-bigodes-e-violinos_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2021-01-24T23:57:17-03:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  }
]
