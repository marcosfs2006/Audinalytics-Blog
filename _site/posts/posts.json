[
  {
    "path": "posts/2022-02-01-reproduzindo-anlises-feitas-no-idea-parte-i/",
    "title": "Reproduzindo Análises Feitas no IDEA - Parte I",
    "description": "Neste _post_ nosso objetivo é reproduzir análises feitas no IDEA com o objetivo\nde mostrar os recursos disponíveis no R para a realização de análises típicas\nde auditoria.",
    "author": [
      {
        "name": "Marcos F. Silva",
        "url": {}
      }
    ],
    "date": "2022-02-01",
    "categories": [
      "Forensic Analytics",
      "IDEA",
      "Análise de Dados"
    ],
    "contents": "\r\n\r\nContents\r\nIntrodução\r\nBase de Dados\r\nTarefasTarefa 1\r\nTarefa 2\r\nTarefa 3\r\nTarefa 4\r\nTarefa 5\r\nTarefa 6\r\nTarefa 7\r\nTarefa 8\r\nTarefa 9\r\nTarefa 10\r\nTarefa 11\r\nTarefa 12\r\nTarefa 13\r\nTarefa 14\r\nTarefa 15\r\nTarefa 16\r\n\r\nOutras TarefasTarefa 17\r\nTarefa 18\r\nTarefa 19\r\nTarefa 20\r\nTarefa 21\r\n\r\nConclusão\r\n\r\nIntrodução\r\nNosso objetivo neste post é tentar repoduzir as análises feitas por Mark Nigrini em seu workshop na Conferência de Usuários do Software IDEA ocorrido em 1o de outubro de 2020.\r\nAs análises podem ser vistas no vídeo disponibilizado pelo autor em seu canal do YouTube.\r\nOs materiais utilizados no vídeo em referência podem ser baixados no link: https://www.dropbox.com/sh/8g8ox3cs8po25tw/AACUfFFtZPUW4g6Co1yBClGwa?dl=0\r\nOs arquivos também estão disponíveis no repositório Usando R em Auditoria no arquivo ForensicAnalytics2nd_IDEA.zip.\r\nOs documentos disponibilizados pelo autor são os seguintes:\r\nNigrini_FraudNumbers_HandsOn.docx\r\nHands-On_Notes.docx\r\nSomerville_2013-2016.xlsx\r\nSampleMeans.pptx\r\nO primeiro documento contém a descrição das análises realizadas e os dados utilizados estão contidos no arquivo indicado no terceiro item. Os outros dois são documentos auxiliares sem maior importância.\r\nEste post refere-se à Parte I. A nossa intenção para uma possível Parte II seria mostrar as análises feitas no IDEA contidas em 3 vídeos disponíveis no YouTube nos quais são mostradas as análises descritas no documento CI202 IDEA Data Analysis Workbook.\r\nBase de Dados\r\nOs dados referem-se a pagamentos feitos pela cidade de Somerville nos anos de 2013 a 2016 e possuem os seguintes campos:\r\nID: identifica de forma única cada pagamentoGovCategory: indica o setor que realizou a transação podendo ser ‘Education’, ‘General Government’, ou ‘Public Works’, as três divisões que fazem pagamentos na cidade de SomervilleVendorName: informa o nome do fornecedor para o qual o pagamento foi feitoAmount: valor do pagamento realizadoCheckDate: data de emissão do cheque de pagamentoDepartment: nome do departamento para o qual a compra foi realizadaCheckNum: número dos cheques emitidos em numeração sequencialOrgDescription: detalhes adicionais sobre o departamentoAcctDescription: mais detalhes sobre o tipo de despesa\r\nNa seção seguinte vamos elencar as tarefas requeridas e mostrar como executá-las no R.\r\nTarefas\r\nAs “tarefas” a serem executadas nos dados estão descritas no documento Nigrini_FraudNumbers_HandsOn.docx.\r\nAs tarefas solicitadas são as seguintes:\r\nTarefa 1\r\nImporte os dados e mostre apenas os 10 primeiros registros das quatro primeiras variáveis (ID, GovCategory, VendorName e Amount).\r\nVamos carregar os pacotes necessários. Estamos assumindo que todos já estejam instalados.\r\n\r\n\r\n# Carregando os pacotes necessários\r\noptions(digits = 12, scipen = 999)\r\nlibrary(readxl)\r\nlibrary(dplyr)\r\nlibrary(stringr)\r\nlibrary(lubridate)\r\nlibrary(ggplot2)\r\nlibrary(benford.analysis)\r\n\r\n\r\n\r\nA importação dos dados pode ser feita da seguinte forma:\r\n\r\n\r\nsomervile <- read_excel(\"Somerville_2013-2016.xlsx\")\r\n\r\n\r\n\r\nUma rápida olhada nos dados:\r\n\r\n\r\nglimpse(somervile)\r\n\r\n\r\nRows: 55,746\r\nColumns: 10\r\n$ ID              <chr> \"8730-2013\", \"15337-2013\", \"22980-2013\", \"41~\r\n$ GovCategory     <chr> \"Public Works\", \"Education\", \"General Govern~\r\n$ VendorName      <chr> \"GLOBAL PETROLEUM CORP.\", \"MAY INSTITUTE INC~\r\n$ Amount          <dbl> 75648.03, 32701.33, 25525.00, 20263.00, 1954~\r\n$ CheckDate       <dttm> 2013-01-02, 2013-01-02, 2013-01-02, 2013-01~\r\n$ Department      <chr> \"DEPARTMENT OF PUBLIC WORKS\", \"SCHOOL ADMINI~\r\n$ CheckNum        <dbl> 555771, 555808, 555875, 2312, 555799, 555803~\r\n$ OrgDescription  <chr> \"DPW-BLGD&GRNDS ORDINARY MAINT\", \"SPED-TUITN~\r\n$ AcctDescription <chr> \"OIL\", \"SPED-TUITNPSCH-DW-SPED-P&T\", \"SITE I~\r\n$ ExpenseType     <chr> \"Miscellaneous line item\", \"Education-relate~\r\n\r\nMostrar os 10 primeiros registros da base de dados\r\n\r\n\r\nsomervile %>% \r\n  select(ID:Amount) %>% \r\n  slice_head(n=10)\r\n\r\n\r\n# A tibble: 10 x 4\r\n   ID         GovCategory        VendorName                     Amount\r\n   <chr>      <chr>              <chr>                           <dbl>\r\n 1 8730-2013  Public Works       GLOBAL PETROLEUM CORP.         75648.\r\n 2 15337-2013 Education          MAY INSTITUTE INC              32701.\r\n 3 22980-2013 General Government SEQUOIA CONSTRUCTION INC       25525 \r\n 4 4179-2013  General Government CENTERS FOR MEDICARE & MEDICA~ 20263 \r\n 5 14696-2013 Education          MANAGED HEALTH RESOURCES INC   19547 \r\n 6 14862-2013 General Government MARKINGS INC                   17224.\r\n 7 2972-2013  Education          BOSTON HIGASHI SCHOOL INC      16716.\r\n 8 7266-2013  Public Works       EAST COAST PETROLEUM STATE     14246.\r\n 9 15117-2013 Education          MASS.ASSOC.FOR THE BLIND       12458.\r\n10 7704-2013  General Government ELIZABETH A FLEMING            11810.\r\n\r\nImportados os dados é sempre necessário conferir se os mesmos estão completos, ou seja, se não está faltando nenhum registro, e uma forma clássica de se fazer isso é “totalizar” a base de dados e conferir o valor obtido com os registros contábeis da entidade auditada.\r\n\r\n\r\nsum(somervile$Amount)\r\n\r\n\r\n[1] 677014836.63\r\n\r\nNo documento Hands-On_Notes.docx o autor informa que os “Totais de Controle” são: $677,014,836.63 e 55.746 registros. Aparentemente está tudo ok.\r\nTarefa 2\r\nCrie um gráfico de período para o valor total pago por mês (Janeiro de 2013 a Dezembro de 2016).\r\nA elaboração de um gráfico de período foi vista no post “Análise Exploratória de Dados e Forensic Analytics”.\r\n\r\n\r\nsomervile %>% \r\n  mutate(AnoMes = format(CheckDate, \"%Y-%m\")) %>% \r\n  group_by(AnoMes) %>% \r\n  summarise(TotalMes = sum(Amount)) %>% \r\n  ggplot(aes(x=AnoMes, y=TotalMes)) +\r\n  geom_bar(stat=\"identity\", color=\"white\", fill=\"lightblue\") +\r\n  theme_bw() +\r\n  theme(axis.text.x = element_text(angle = 90, vjust=0.3, size=5)) \r\n\r\n\r\n\r\n\r\nTarefa 3\r\nQual mês/ano no período de janeiro de 2013 a dezembro de 2016 possui o maior valor de pagamentos em todo o período?\r\nO gráfico mostra que a resposta a esta questão é junho de 2016. Também seria possível obter esta resposta da seguinte forma:\r\n\r\n\r\nsomervile %>%\r\n  mutate(AnoMes = format(CheckDate, \"%Y-%m\")) %>% \r\n  group_by(AnoMes) %>%\r\n  summarise(TotalMes = sum(Amount)) %>% \r\n  slice_max(TotalMes)\r\n\r\n\r\n# A tibble: 1 x 2\r\n  AnoMes   TotalMes\r\n  <chr>       <dbl>\r\n1 2016-06 46053320.\r\n\r\nTarefa 4\r\nCalcular os valores abaixo relacionados, mostrando-os em oito linhas consecutivas. Arredonde os valores para duas casas decimais.\r\nValores a serem calculados: soma, quantidade de registros, número de registros faltantes, média, mediana, moda, valor mínimo e valor máximo.\r\n\r\n\r\nedescr <- somervile %>% \r\n  summarise(Soma = sum(Amount),\r\n            QtdReg = n(),\r\n            Media = mean(Amount),\r\n            Mediana = median(Amount),\r\n            Minimo = min(Amount),\r\n            Maximo = max(Amount))\r\n\r\nedescr %>% t() %>% round(2)                     \r\n\r\n\r\n                [,1]\r\nSoma    677014836.63\r\nQtdReg      55746.00\r\nMedia       12144.64\r\nMediana       504.30\r\nMinimo     -16772.40\r\nMaximo   10000000.00\r\n\r\nFicaram de fora a moda e o número de registros faltantes. No vídeo, o autor faz o cálculo da moda tabulando a frequência de ocorrência de cada número, ordenando e verificando o número com a maior frequência de ocorrência, algo assim:\r\n\r\n\r\nsomervile %>%\r\n  count(Amount) %>% \r\n  slice_max(n=1, order_by=n)\r\n\r\n\r\n# A tibble: 1 x 2\r\n  Amount     n\r\n   <dbl> <int>\r\n1    135   690\r\n\r\nEmbora o R não possua uma função nativa para o cálculo da moda, alguns pacotes fornecem funções para o seu cálculo, como por exemplo a função Mode() do pacote {pracma}.\r\nO número de registros faltantes não foi calculado em razão de não ter uma indicação no vídeo, nem no livro, de como obter esse valor. Mas em princípio se a totalização dos valores na base de dados confere com o que está registrado na contabilidade, então não há registros faltantes.\r\nUma outra possibilidade seria explorar a numeração sequencial dos cheques para verificar se existe lacunas. Vamos deixar isso para mais adiante.\r\nTarefa 5\r\nEste item está faltando no documento disponibilizado pelo autor,\r\nTarefa 6\r\nA média dos valores do conjunto de dados (todos os 55.746 pagamentos) é $12.144,64. Você esperaria que o valor médio dos pagamentos de uma amostra de 200 registros seja igual a esse valor? Se não, você esperaria que seja superior ou inferior a $12,144.64\r\nNo documento SampleMeans.pptx o autor apresenta um pouco da teoria relacionada à distribuição amostral da média. A leitura do último slide nos permite responder à questão.\r\nA média amostral é um estimador não viesado da média populacional, e para amostras maiores que 30 a distribuição amostral da média aproxima-se bastante de uma distribuição normal independentemente da distribuição populacional da qual a amostra foi retirada. A média dos valores de uma amostra de 200 elementos da população irá ficar próxima à média de todos os valores da base de dados (população). Em algumas amostras essa média será inferior, em outras será superior à média populacional.\r\nTarefa 7\r\nVocê espera que os dados de pagamento se conformem à Lei de Benford? Porque?\r\nSim. Em geral os dados contábeis contém as características necessárias a que um conjunto de valores se ajuste à distribuição de dígitos preconizada pela Lei de Benford.\r\nTarefa 8\r\nAplique o teste dos dois primeiros dígitos ao conjunto de dados. Apresente o gráfico do teste realizado.\r\n\r\n\r\nsomervile_bfd <- benford(somervile$Amount, discrete = FALSE)\r\nplot(somervile_bfd,\r\n     multiple=FALSE,\r\n     except = c(\"second order\", \"summation\", \"mantissa\", \"chi square\", \"abs diff\", \"ex summation\"))\r\n\r\n\r\n\r\n\r\nTarefa 9\r\nQual o nível de conformidade que os dados apresenetam com a Lei de Benford? Use o desvio médio absoluto (MAD - mean absolute deviation) para o teste de dois dígitos. A resposta deve ser: “conformidade estrita”, “conformidade aceitável”, “conformidade marginalmente aceitável”, ou “não conforme”.\r\nPara responder de forma adequada a essa questão, é necessário conhecer o critério para se fazer a classificação solicitada no exercício. Observando o vídeo\r\nindicado por volta do minuto 15:51 o autor remete para a página 114 de seu livro Forensic Analytics onde consta uma tabela que permite fazer a classificação em função do valor do MAD obtido.\r\n\r\n\r\nsomervile_bfd\r\n\r\n\r\n\r\nBenford object:\r\n \r\nData: somervile$Amount \r\nNumber of observations used = 55688 \r\nNumber of obs. for second order = 29494 \r\nFirst digits analysed = 2\r\n\r\nMantissa: \r\n\r\n   Statistic   Value\r\n        Mean  0.4944\r\n         Var  0.0861\r\n Ex.Kurtosis -1.2463\r\n    Skewness  0.0048\r\n\r\n\r\nThe 5 largest deviations: \r\n\r\n  digits absolute.diff\r\n1     50        692.07\r\n2     13        415.70\r\n3     15        413.14\r\n4     75        371.66\r\n5     25        322.45\r\n\r\nStats:\r\n\r\n    Pearson's Chi-squared test\r\n\r\ndata:  somervile$Amount\r\nX-squared = 3585.798622, df = 89, p-value <\r\n0.0000000000000002220446\r\n\r\n\r\n    Mantissa Arc Test\r\n\r\ndata:  somervile$Amount\r\nL2 = 0.0008912906245, df = 2, p-value <\r\n0.0000000000000002220446\r\n\r\nMean Absolute Deviation (MAD): 0.00194393945934\r\nMAD Conformity - Nigrini (2012): Marginally acceptable conformity\r\nDistortion Factor: -1.09234095524\r\n\r\nRemember: Real data will never conform perfectly to Benford's Law. You should not focus on p-values!\r\n\r\nO resultado acima mostra que o conjunto de dados possui uma conformidade marginal à Lei de Benford. (Marginally acceptable conformity)\r\nTarefa 10\r\nIdentifique os dois primeiros dígitos (10, 11, …, 99) que possuem os três maiores valores no gráfico, ou seja, o par de dígitos que mais excedam a proporção esperada.\r\nO resultado do item anterior nos mostra que os dígitos são: 50, 13 e 15. Não obstante, esses valores diferem dos que são obtidos pelo autor no vídeo: 50, 75 e 78.\r\nNão conseguimos checar a origem da divergência, mas possivelmente deve-se ao critério utilizado para avaliar o distanciamento dos dígitos ao valor esperado.\r\nTarefa 11\r\nIdentifique os pagamentos “redondos” na base de dados. Um número redondo é definido como sendo um múltiplo de 100.000. Mostre uma tabela com os números redondos identificados mostrando as colunas CheckDate, VendorName, Amount e AcctDescription.\r\nOrdene o resultado de forma decrescente em função da coluna Amount.\r\nCalcule a percentagem de números redondos no total de registros da base de dados. Forneça o resultado no formato xx.xx%.\r\n\r\n\r\nnum_redondo <- somervile %>% \r\n  mutate(NumRedondo = if_else(Amount != 0 & \r\n                              Amount %% 1e5 == 0, 1, 0)) %>% \r\n  filter(NumRedondo == 1) %>% \r\n  arrange(Amount) %>% \r\n  select(\"CheckDate\", \"VendorName\", \"Amount\", \"AcctDescription\")\r\n\r\nhead(num_redondo)\r\n\r\n\r\n# A tibble: 6 x 4\r\n  CheckDate           VendorName Amount AcctDescription          \r\n  <dttm>              <chr>       <dbl> <chr>                    \r\n1 2013-06-26 00:00:00 US BANK    100000 BOND ANTICIPATION NOTE   \r\n2 2013-10-23 00:00:00 US BANK    100000 BOND ANTICIPATION NOTE   \r\n3 2014-08-13 00:00:00 US BANK    100000 PRINCIPAL ON LNG TRM DEBT\r\n4 2016-06-08 00:00:00 US BANK    200000 BOND ANTICIPATION NOTE   \r\n5 2016-06-08 00:00:00 US BANK    300000 BOND ANTICIPATION NOTE   \r\n6 2016-06-08 00:00:00 US BANK    300000 BOND ANTICIPATION NOTE   \r\n\r\nCálculo do percentual de registros relativos a números redondos.\r\n\r\n\r\nround(nrow(num_redondo) / nrow(somervile) * 100, 2) %>% \r\n  paste0(\"%\")\r\n\r\n\r\n[1] \"0.06%\"\r\n\r\nTarefa 12\r\nCrie uma nova coluna na base de dados (DayWeek) contendo o dia da semana a que se refere o pagamento (CheckDate). Assim, por exemplo, se a data do cheque é 2/1/2013, que é uma quarta-feira, o valor da nova coluna será “4”.\r\nExclua as colunas OrgDescription e AcctDescription. Mostre as 8 primeiras linhas do resultado obtido que deve incluir a coluna recém criada DayWeek.\r\n\r\n\r\nsomervile <- somervile %>% \r\n  mutate(DayWeek = wday(CheckDate)) \r\n\r\nsomervile %>% \r\n  select(-OrgDescription, -AcctDescription) %>% \r\n  slice_head(n=8)\r\n\r\n\r\n# A tibble: 8 x 9\r\n  ID     GovCategory VendorName  Amount CheckDate           Department\r\n  <chr>  <chr>       <chr>        <dbl> <dttm>              <chr>     \r\n1 8730-~ Public Wor~ GLOBAL PET~ 75648. 2013-01-02 00:00:00 DEPARTMEN~\r\n2 15337~ Education   MAY INSTIT~ 32701. 2013-01-02 00:00:00 SCHOOL AD~\r\n3 22980~ General Go~ SEQUOIA CO~ 25525  2013-01-02 00:00:00 OSPCD ADM~\r\n4 4179-~ General Go~ CENTERS FO~ 20263  2013-01-02 00:00:00 PERSONNEL~\r\n5 14696~ Education   MANAGED HE~ 19547  2013-01-02 00:00:00 SCHOOL AD~\r\n6 14862~ General Go~ MARKINGS I~ 17224. 2013-01-02 00:00:00 TRAFFIC A~\r\n7 2972-~ Education   BOSTON HIG~ 16716. 2013-01-02 00:00:00 SCHOOL AD~\r\n8 7266-~ Public Wor~ EAST COAST~ 14246. 2013-01-02 00:00:00 DEPARTMEN~\r\n# ... with 3 more variables: CheckNum <dbl>, ExpenseType <chr>,\r\n#   DayWeek <dbl>\r\n\r\nTarefa 13\r\nCalcule a quantidade de cheques emitidos em cada dia da semana. Algum cheque cuja data seja sábado ou domingo? Mostre uma tabela com os resultados.\r\n\r\n\r\nsomervile %>% \r\n  count(DayWeek)\r\n\r\n\r\n# A tibble: 6 x 2\r\n  DayWeek     n\r\n    <dbl> <int>\r\n1       1     1\r\n2       2  1573\r\n3       3  2394\r\n4       4 48494\r\n5       5  2582\r\n6       6   702\r\n\r\nUm cheque emitido no domingo.\r\nTarefa 14\r\nElabore um gráfico mostrando a quantidade de pagamentos feitos por trimestre. Comente o resultado obtido.\r\nNota: No vídeo, por volta do minuto 24:42 o autor apresenta o resultado do teste. A figura abaixo, retirada do vídeo em questão, dá uma ideia do resultado a ser obtido.\r\n\r\n\r\n\r\nsomervile %>% \r\n  mutate(Trimestre = quarter(CheckDate, type = \"date_last\")) %>% \r\n  group_by(Trimestre) %>% \r\n  summarise(QtdRegistros = n()) %>% \r\n  ggplot(aes(x=Trimestre, y=QtdRegistros)) +\r\n  geom_line(color=\"blue\") +\r\n  geom_point(color=\"blue\") +\r\n  theme_bw()\r\n\r\n\r\n\r\n\r\nTarefa 15\r\nQuantos valores negativos existem na base de dados?\r\n\r\n\r\nsum(somervile$Amount < 0)\r\n\r\n\r\n[1] 43\r\n\r\nTarefa 16\r\nEncontre todos os pagamentos cujos fornecedores possuam a palavra Christmas em seu nome.\r\n\r\n\r\nchristimas_pay <- somervile %>% \r\n  filter(str_detect(VendorName,\r\n                    coll(\"Christmas\", ignore_case = TRUE)))\r\n\r\nchristimas_pay\r\n\r\n\r\n# A tibble: 3 x 11\r\n  ID     GovCategory  VendorName Amount CheckDate           Department\r\n  <chr>  <chr>        <chr>       <dbl> <dttm>              <chr>     \r\n1 7742-~ Public Works ELSIE AND~   9625 2013-01-09 00:00:00 DEPARTMEN~\r\n2 8651-~ Public Works ELSIE AND~   9900 2013-12-26 00:00:00 DEPARTMEN~\r\n3 9017-~ Public Works ELSIE AND~   4490 2015-01-14 00:00:00 DEPARTMEN~\r\n# ... with 5 more variables: CheckNum <dbl>, OrgDescription <chr>,\r\n#   AcctDescription <chr>, ExpenseType <chr>, DayWeek <dbl>\r\n\r\nOutras Tarefas\r\nAlém das tarefas (ou análises se preferir) apresentadas no documento disponibilizado pelo autor e acima elencadas, vamos aproveitar para adicionar mais algumas que podem ser úteis a quem trabalha com auditoria.\r\nTarefa 17\r\nSorteie aleatoriamente uma amostra de 200 registros (sem reposição) da base de dados.\r\n\r\n\r\nset.seed(0710)\r\namostra <- somervile %>% \r\n              slice_sample(n=200)\r\n\r\nnrow(amostra)\r\n\r\n\r\n[1] 200\r\n\r\nTarefa 18\r\nElabore um histograma para evidenciar a distribuição das médias de 1000 amostras de tamanho 200 sorteadas aleatoriamente da base de dados.\r\n\r\n\r\nmedias_amostrais <- replicate(1000,\r\n                              {\r\n                               somervile %>% \r\n                               slice_sample(n=200) %>%\r\n                               pull(Amount) %>% \r\n                               mean()\r\n                              })\r\n  \r\n  \r\n  \r\nhist(medias_amostrais,\r\n     breaks = 30,\r\n     main = \"Distribuição Amostral da Média\")\r\n\r\n\r\n\r\n\r\nTarefa 19\r\nObtenha os registros da base de dados cujos valores dos pagamentos iniciem com os dígitos “75”. Quantos registros são?\r\n\r\n\r\ndigitos_75 <- somervile %>% \r\n  filter(str_detect(Amount, \"^75\"))\r\n\r\nnrow(digitos_75)\r\n\r\n\r\n[1] 690\r\n\r\nTarefa 20\r\nQuais são os departamentos associados/vinculados a cada “GovCategory”?\r\n\r\n\r\nsomervile %>% \r\n  distinct(GovCategory, Department)\r\n\r\n\r\n# A tibble: 36 x 2\r\n   GovCategory        Department                \r\n   <chr>              <chr>                     \r\n 1 Public Works       DEPARTMENT OF PUBLIC WORKS\r\n 2 Education          SCHOOL ADMINISTRATION     \r\n 3 General Government OSPCD ADMINISTRATION      \r\n 4 General Government PERSONNEL DEPARTMENT      \r\n 5 General Government TRAFFIC AND PARKING       \r\n 6 General Government WORKER COMPENSATION       \r\n 7 General Government COUNCIL ON AGING          \r\n 8 General Government SOMERVILLE PUBLIC LIBRARY \r\n 9 General Government RECREATION AND YOUTH      \r\n10 General Government POLICE DEPARTMENT         \r\n# ... with 26 more rows\r\n\r\nVamos deixar para o leitor/auditor incluir mais 4 colunas indicando o valor gasto nos anos de 2013, 2014, 1015 e 2016. Aí fica simples fazer análises horizontais e verticais.\r\nTarefa 21\r\nOs cheques emitidos seguem de fato uma numeração sequencial? Em caso positivo, existem lacunas na numeração?\r\nVamos fazer um teste geral para identificar se existem números de cheques duplicados na base de dados.\r\n\r\n\r\nany(duplicated(somervile$CheckNum))\r\n\r\n\r\n[1] TRUE\r\n\r\nHummm mal sinal. A base de dados possui mais de um pagamento cujos números de cheques são iguais. Mas talvez a numeração dos cheque reinicie a cada novo ano…Será? Ou a numeração seria única por\r\nOs cheques de mesma numeração possuem a mesma data? Referem-se a a diferentes fornecedores? Como poderíamos obter os registros cujos números de cheques possuem numeração em duplicidade (o regitro “original” e os duplicados)?\r\nDeixaremos para o colega auditor responder a essas questões.\r\nConclusão\r\nEste conjunto de dados oferece muitas outras oportunidades de análise. Talvez voltemos a ele em uma outra oportunidade.\r\nNosso objetivo foi mostrar que todas a análises que possam ser feitas em softwares específicos para análise de dados em auditoria podem também ser feitas no R, o que o torna um forte cadidato a tornar-se a ferramenta analítica para a realização de auditorias não só contábil-financeira, como também operacional, de conformidade (aí incluída a auditoria de fraudes) e avaliações de políticas públicas.\r\nBem, é isso. Espero que tenham gostado.\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-02-01-reproduzindo-anlises-feitas-no-idea-parte-i/reproduzindo-anlises-feitas-no-idea-parte-i_files/figure-html5/unnamed-chunk-6-1.png",
    "last_modified": "2022-02-05T13:42:27-03:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2022-01-23-anlise-exploratria-de-dados-e-forensic-analytics/",
    "title": "Análise Exploratória de Dados e _Forensic Analytics_",
    "description": "Implementação em R dos \"testes\" apresentados no Capítulo 2 do livro **Forensic\nAnalytics** 2a Ed. do Mark Nigrini",
    "author": [
      {
        "name": "Marcos F. Silva",
        "url": {}
      }
    ],
    "date": "2022-01-23",
    "categories": [
      "Livro Forensic Analytics",
      "EDA"
    ],
    "contents": "\r\n\r\nContents\r\nIntrodução\r\nImportação dos dados\r\nPerfil dos dados\r\nHistograma\r\nGráfico de período\r\nEstatísticas descritivas\r\nConclusão\r\n\r\nIntrodução\r\nNeste post nosso objetivo é mostrar como implementar no R as análises contidas no capítulo 2 da segunda edição do livro Forensic Analytics do Mark Nigrini.\r\nNeste capítulo o autor apresenta o que ele chama dos “quatro testes de visão geral” que tem por objetivo a obtenção de uma compreensão geral dos dados e eventualmente de alguns insights para guiar análises posteriores.\r\nTrata-se, essencialmente, da aplicação de Análise Exploratória de Dados que deve preceder sempre qualquer análise de dados.\r\nEsses “quatro testes” são os seguintes: (a) perfil dos dados, (b) histograma, (c) gráfico de período e (d) estatísticas descritivas e, nos dizeres do autor, esses quatro testes formam a base de uma auditoria forense orientada a dados.\r\nO autor possui uma playlist no YouTube onde faz uma rápida revisão dos capítulos do livro e o link a seguir é para o vídeo relativo ao Capítulo 2.\r\nhttps://www.youtube.com/watch?v=ibFG3Zn9-UQ&list=PLqQLvWWovXwUBF3YAbSGjUxKpgETwuRQR&index=4\r\nVamos mostrar como implementar no R cada um dos “testes” apresentados no capítulo.\r\nImportação dos dados\r\nO primeiro passo é realizar a importação dos dados que estão armazenados em uma planilha do Excel chamada PCard_Data2009-2014.xlsx.\r\nO conjunto de dados utilizado no capítulo diz respeito a transações realizadas com cartões de crédito corporativo. Mais especificamente dados de compras do distrito de Columbia compreendendo o período de 2009 a 2014.\r\nVamos importar os pacotes que precisaremos para nossas análises.\r\n\r\n\r\nlibrary(readxl)\r\nlibrary(dplyr)\r\nlibrary(ggplot2)\r\n\r\n\r\n\r\nFeita a importação dos pacotes, passamos à importação dos dados que, por estarem armazenados em uma planilha do Excel, serão importados com a função read_excel() do pacote readxl.\r\n\r\n\r\n# Impotação dos dados\r\npcard <- read_excel(\"PCard_Data2009-2014.xlsx\")\r\n\r\n# Inspeção dos dados\r\nglimpse(pcard)\r\n\r\n\r\nRows: 193,791\r\nColumns: 6\r\n$ Date        <dttm> 2009-01-02, 2009-01-02, 2009-01-02, 2009-01-02,~\r\n$ Amount      <dbl> -1700.00, -23.16, 9.48, 24.61, 31.65, 71.10, 100~\r\n$ Merchant    <chr> \"WASHINGTON HISPANIC\", \"STAPLES DIRECT00209908\",~\r\n$ State       <chr> \"MD\", \"CA\", \"WA\", \"DC\", \"NY\", \"MD\", \"NJ\", \"CA\", ~\r\n$ Description <chr> \"Advertising Services\", \"Stationery,Office Suppl~\r\n$ Agency      <chr> \"Department of Motor Vehicles\", \"Metropolitan Po~\r\n\r\nA saída da função glimpse() do pacote dplyr já nos fornece um conjunto interessante de informações sobre esse conjunto de dados. De início já é possível saber tratar-se de um conjunto de dados que possui 193.791 linhas ou registros ou observações e 6 colunas ou variáveis.\r\nPara cada variável temos informações sobre o tipo de dado de cada variável. Por exemplo, a variável Date é do tipo data, a variável Amount é numérica e todas as demais são caracteres.\r\nToda análise de dados requer que o analista conheça em bastante detalhes o que cada variável significa. Vamos nos abster aqui de fazer maiores comentários sobre os dados em razão do objetivo do post não necessitar desse conhecimento.\r\nPerfil dos dados\r\nA elaboração de um perfil dos dados, o que autor chama de “The Data Profile”, consiste, basicamente, em estratificar a base de dados em função dos valores da variável de interesse, em geral o valor das transações ou dos saldos.\r\nA estratificação consiste em se definir intervalos de valores dos dados e contar a quantidade de registros que possuem valores em cada um dos intervalos definidos bem como calcular a soma dos valores relativos aos registros classificados em cada intervalo.\r\nDuas outras colunas consistem apenas em obter o percentual que cada estrato representa do total geral tanto em termos da quantidade de registros, como dos valores associados a cada estrato..\r\nA figura a seguir, extraída do livro em referência, dá uma indicação do contéudo do resultado do “teste”.\r\n\r\nPara obter os dados apresentados na tabela acima podemos fazer da seguinte forma:\r\n\r\n\r\nestratificacao <- pcard %>%\r\n  mutate(estrato = case_when( Amount >= 500 ~ \"(500, +Inf)\",\r\n                              Amount > 0 & Amount < 500 ~ \"(0, 500)\",\r\n                              Amount == 0 ~ \"0\",\r\n                              Amount > -500 & Amount < 0 ~ \"(-500, 0)\",\r\n                              Amount <= -500 ~ \"(-Inf, -500]\")) %>% \r\n  group_by(estrato) %>% \r\n  summarise(qtd_registros = n(),\r\n            vlr_estrato = sum(Amount)) %>% \r\n  ungroup() %>% \r\n  mutate(pct_registros = round(qtd_registros / sum(qtd_registros) * 100, 2),\r\n         pct_vlr_estrato = round(vlr_estrato / sum(vlr_estrato) * 100, 2))\r\n\r\nestratificacao\r\n\r\n\r\n# A tibble: 4 x 5\r\n  estrato      qtd_registros vlr_estrato pct_registros pct_vlr_estrato\r\n  <chr>                <int>       <dbl>         <dbl>           <dbl>\r\n1 (-500, 0)             6848    -738588.          3.53           -0.69\r\n2 (-Inf, -500]          1163   -1499416.          0.6            -1.39\r\n3 (0, 500)            123856   20305809.         63.9            18.8 \r\n4 (500, +Inf)          61924   89745910.         32.0            83.2 \r\n\r\nPor razões históricas, segundo o autor, o teste divide os dados em cinco estratos cobrindo todo o espaço de variação dos dados e a elaboração de mais dois estratos cujo objetivo é mostrar os menores e maiores valores no conjunto de dados.\r\nNa figura acima verifica-se que o autor optou por identificar a participação dos registros cujos valores sejam positivos e menores ou iguais a 50 e os valores superiores a 50.000 para definir os estratos relativos aos maiores e menores valores.\r\nA escolha desses valores é uma questão de sensibilidade do auditor e função do contexto e características específicas dos dados em análise, não existindo uma regra pré definida para a escolha dos mesmos.\r\nA elaboração dos estratos suplementares segue a mesma lógica utilizada para a criação da estratificação principal.\r\n\r\n\r\nextremos <- pcard %>% \r\n  mutate(extremos = case_when(between(Amount, 0.01, 50) ~ \"(0, 50]\",\r\n                              between(Amount, 50000, +Inf) ~ \"[50.000, +Inf]\",\r\n                              TRUE ~ \"(50, 50.000)\")) %>%  \r\n  group_by(extremos) %>% \r\n  summarise(qtd_registros = n(),\r\n            vlr_estrato = sum(Amount)) %>% \r\n  ungroup() %>% \r\n  mutate(pct_registros = round(qtd_registros / sum(qtd_registros) * 100, 2),\r\n         pct_vlr_estrato = round(vlr_estrato / sum(vlr_estrato) * 100, 2)) %>% \r\n  filter(extremos != \"(50, 50.000)\")\r\n  \r\nextremos        \r\n\r\n\r\n# A tibble: 2 x 5\r\n  extremos     qtd_registros vlr_estrato pct_registros pct_vlr_estrato\r\n  <chr>                <int>       <dbl>         <dbl>           <dbl>\r\n1 (0, 50]              33888     861035.         17.5             0.8 \r\n2 [50.000, +I~            63    5035139.          0.03            4.67\r\n\r\nHistograma\r\nO segundo “teste” é o histograma, gráfico estatístico muito utilizado para avaliar a distribuição de uma variável quantitativa contínua, que no âmbito das auditorias usualmente são os valores das transações ou dos saldos.\r\nA figura abaixo mostra o histograma apresenado no livro para o conjunto de dados em análise.\r\n\r\nA produção de um histograma não é algo complicado de se fazer no R. Usando o pacote {ggplot2} pode ser feito da seguinte forma:\r\n\r\n\r\npcard %>%\r\n  filter(between(Amount, 0, 3500)) %>% \r\n  ggplot(aes(x=Amount)) +\r\n  geom_histogram(fill=\"blue\", color=\"white\")\r\n\r\n\r\n\r\n\r\nPara a produção do histograma acima, foram excluídos os valores negativos e os valores superiores a 3.500. Isso foi feito para melhorar a visualização dos dado já que, incluindo os demais valores que são muito maiores ou muito menores que a grande maiorida dos dados a visualização ficaria prejudicada.\r\nA função summary() nos dá uma visão geral da amplitude de variação dos dados:\r\n\r\n\r\nsummary(pcard$Amount)\r\n\r\n\r\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \r\n-15445.20     65.74    235.48    556.34    680.96 143500.00 \r\n\r\nGráfico de período\r\nDe acordo com o autor este é o terceiro “teste” geral usado para a obtenção de insights quanto a distribuição dos dados.\r\nEssencialmente este gráfico, que o autor chama de “Periodic Graphic”, é uma série temporal em que o autor fez a opção de mostrar a evolução temporal da quantidade de interesse por intermédio de barras em vez de uma linha horizontals, o que seria o mais comum.\r\nO teste consiste um dividir os dados em períodos de tempo, calcular o valor de interesse para cada período e mostrar esses valores num gráfico onde o período de tempo é mostrado no eixo x e os valores no eixo y.\r\nA figura a seguir é o gráfico apresentado pelo autor:\r\n\r\nEsse gráfico pode ser obtido da seguinte forma:\r\n\r\n\r\noptions(scipen = 999)\r\n\r\npcard %>% \r\n  mutate(ano_mes = format(Date, \"%Y-%m\")) %>% \r\n  group_by(ano_mes) %>% \r\n  summarise(total_mes = sum(Amount)) %>% \r\n  ggplot(aes(x=ano_mes, y=total_mes)) +\r\n  geom_bar(stat=\"identity\", color=\"white\", fill=\"lightblue\") +\r\n  theme_bw() +\r\n  theme(axis.text.x = element_text(angle = 90, vjust=0.3, size=5)) \r\n\r\n\r\n\r\n\r\nAlgumas melhorias estéticas são ainda possíveis, mas essencialmente o gráfico está pronto.\r\nEstatísticas descritivas\r\nDe acordo com o autor, o objetivo ao se calcular as estatísticas descritivas é para auxiliar na avaliação quanto a existência de fraude ou erros nos dados ou se houve mudanças no fenômeno mensurado.\r\nAs estatísticas descritivas apresentadas são as seguintes:\r\nVisão Geral\r\nSoma dos valores (deve ser reconciliado com os registros contábeis)\r\nQuantidade de registros\r\nQuantidade de registros faltantes\r\nTendência Central\r\nMédia, mediana e moda\r\nVariabilidade\r\nMáximo, mínimo, intervalor interquartil, amplitude, desvio padrão\r\nMedidas de assimetria\r\nA figura a seguir, retirada do livro em referência, apresenta o valor das estatísticas para o conjunto de dados em análise.\r\n\r\nVamos ao cálculo dessas medidas:\r\nA função sum() nos retorna a soma de um conjunto de valores.\r\n\r\n\r\noptions(digits = 13)\r\n\r\nsum(pcard$Amount)\r\n\r\n\r\n[1] 107813715.47\r\n\r\nA função nrow() nos retorna a quantidade de linhas que um conjunto de dados no formato data frame possui.\r\n\r\n\r\nnrow(pcard)\r\n\r\n\r\n[1] 193791\r\n\r\nA média e a mediana podem ser obtidas, respectivamente, com as funções mean() e median().\r\n\r\n\r\nmean(pcard$Amount) %>% round(2)\r\n\r\n\r\n[1] 556.34\r\n\r\nmedian(pcard$Amount)\r\n\r\n\r\n[1] 235.48\r\n\r\nOs valores mínimo e máximo podem ser obtidos, respectivamente, com as funções min() e max().\r\n\r\n\r\nmin(pcard$Amount)\r\n\r\n\r\n[1] -15445.2\r\n\r\n\r\n\r\nmax(pcard$Amount)\r\n\r\n\r\n[1] 143500\r\n\r\nO range pode ser obtido a partir das funções max() e min().\r\n\r\n\r\nmax(pcard$Amount) - min(pcard$Amount)\r\n\r\n\r\n[1] 158945.2\r\n\r\nQuartis, decis e percentis podem ser obtidos com a função quantile(), Especificamente o primeiro e terceiro quartis são calculados como mostrado a seguir:\r\n\r\n\r\nquantile(pcard$Amount, probs=c(0.25, 0.75))\r\n\r\n\r\n    25%     75% \r\n 65.740 680.965 \r\n\r\nA função IQR() calcula O intervalo interquartil.\r\n\r\n\r\nIQR(pcard$Amount)\r\n\r\n\r\n[1] 615.225\r\n\r\nO desvio padrão pode ser calculado com a função sd().\r\n\r\n\r\nsd(pcard$Amount) %>% round(2)\r\n\r\n\r\n[1] 1726.79\r\n\r\nNão apresentamos as funções para o cálculo da moda e da assimetria. O R não possui funções nativas para o cálculo dessas quantidades e seria necessário usar um pacote para isso.\r\nA moda não é uma medida em geral utilizada nas análises. Uma percepção melhor desse valor pode ser obtida por intermédio de visualizaões dos dados. O próprio desvio padrão não é uma medida de fácil interpretação.\r\nO coeficiente de variação é uma medida muito utilizada para se avaliar a representatividade da média em um conjunto de dados.\r\nTambém não são muito utilizadas medidas de assimetria. Uma boa opção é utilizar gráficos de quantis (“QQ-plot”) para avaliar o quanto uma distribuição se “aproxima” de uma distribuição de referência, em geral a distribuição normal.\r\nConclusão\r\nA Análise Exploratória de Dados é uma etapa essencial em qualquer trabalho com dados e não seria diferente com a Forensic Analytics.\r\nOs “testes” apresentados no Capítulo 2 representam uma pequena fração do que pode ser feito nessa etapa. Talvez em outra postagem possamos apresentar outras possibilidades de análise além das que são apresentadas no texto de referência.\r\nOs dados faltantes ou missing values são um aspecto importante a ser considerado na análise preliminar dos dados mas que não foi abordado no capítulo 2 do livro.\r\nUm aspecto positivo na abordagem apresentada no livro é a importância dada à visualização de dados. Com os recursos computacionais hoje disponíveis não há porque negligenciar sua utilização nos trabalhos de auditoria. Confiar apenas em medidas numéricas descritivas não é uma boa opção.\r\nComo o leitor deve ter pecebido, não houve preocupação em colocar as tabelas produzidas em um formato que fosse adequado para constar de um relatório. Procuramos apenas mostrar a implementação dos testes usando o R.\r\nO mesmo podemos dizer dos gráficos. Vários recursos de customização estão disponíveis para tornar os gráficos visualmente mais agradáveis, o que é essencial para a produção de relatórios mas que não tem tanta relevância quando estamos apenas fazendo exploração dos dados.\r\nEm algum momento vamos fazer um post tratando especificamente da produção de tabelas no R, visto ser um tópico relevante quando se trata de elaborar relatórios.\r\nEntão é isso. Espero que tenham gostado!\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-01-23-anlise-exploratria-de-dados-e-forensic-analytics/anlise-exploratria-de-dados-e-forensic-analytics_files/figure-html5/unnamed-chunk-5-1.png",
    "last_modified": "2022-01-23T23:07:10-03:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2022-01-26-usando-r-em-forensic-analytics/",
    "title": "Usando R em Forensic Analytics",
    "description": "Neste _post_, inspirado nos Capítulos 1, 2 e 3 do livro Forensic Analytics\ndo Mark Nigrini (1^a^ Edição), nosso objetivo é apresentar alguns recursos\nque o R possui que o tornam uma ferramenta mais que adequada para implementação\ndas técnicas de auditoria forense computacional.",
    "author": [
      {
        "name": "Marcos F. Silva",
        "url": {}
      }
    ],
    "date": "2022-01-23",
    "categories": [
      "Livro Forensic Analytics"
    ],
    "contents": "\r\n\r\nContents\r\nIntrodução\r\nForensic Analytics\r\nObtenção dos Dados\r\nPreparação dos Dados\r\nAnálise dos dados\r\nComunicação dos ResultadosRelatórios\r\nPainéis\r\nApresentações\r\n\r\nOutros pacotes de interesse\r\nConsiderações Finais\r\n\r\nIntrodução\r\nA ideia desse post surgiu após a leitura de alguns capítulos do livro Forensic Analytics do Mark Nigrini para a elaboração do post intitulado “Uma Implementação em R do RSF Test”.\r\nNa primeira edição do livro (atualmente o livro está em sua 2a edição) os três primeiros capítulos são intitulados, respectivamente, “Usando Access em Investigações Forenses”, “Usando Excel em Investigações Forenses” e “Usando PowerPoint em Apresentações Forenses” e meu objetivo com este post é mostrar que o R possui muito mais condições de ser a ferramenta para implementação não só dos procedimentos descritos na literatura de “forensic analytics” mas também das diversas outras técnicas de mineração de dados descritas na literatura de detecção de fraudes.\r\nAlém disso o R oferece ótimos recursos para apresentação dos resultados, tanto na forma de relatórios como de painéis ( dashboards ) e de apresentações.\r\nLogo no início do Capítulo 1 Nigrini afirma que são quatro os principais passos em auditoria forense:\r\nobtenção dos dados;\r\npreparação dos dados;\r\nanálise dos dados e\r\nreporte dos resultados\r\nNa verdade, essas etapas são comuns a todos os projetos de análise de dados. No livro R for Data Science o autor apresenta o seguinte framework para o trabalho com dados:\r\n\r\nEste processo engloba as etapas mencionadas por Nigrini como pode ser facilmente visto na figura.\r\nO R, como será mostrado mais adiante, possui excelentes recursos para uso em todas as quatro etapas mencionadas pelo autor.\r\nNos capítulos que se seguem, mostramos os recursos que o R ofecere para a execução de cada uma das etapas.\r\nOs recursos a que nos referimos são funções disponibilizadas por meio de pacotes. Esses são dois conceitos importantes para o trabalho com o R.\r\nNo R, diferentemente de outros aplicativos, o usuário em geral irá digitar na linha de comando as funções que ele deseja utilizar para realizar uma determinada tarefa. Isso não é muito diferente do que o usário faz ao inserir uma função numa célula de uma planilha, por exemplo, para realizar um cálculo qualquer ou outra ação de interesse.\r\nAs funções estão reunidas em pacotes. Por exemplo, o pacote {readxl} disponibiliza funções úteis para a importação de dados armazenados em planihas do Excel. Da mesma forma, outros pacotes irão disponibilizar funções escritas para relizar determinado tipo de tarefa.\r\nUma coisa interessante no R é que qualquer usuário pode criar suas próprias funções e disponibilizar para que outras pessoas possam também utilizá-las. Essa facilidade de extender as funcionalidades básicas do R é uma das coisas que o tornam bastante popular.\r\nForensic Analytics\r\nO objetivo da auditoria financeira é disponibilizar aos usuáios das demonstrações financeiras uma opinião quanto à adequada apresentação das mesmas em todos os aspectos materiais e de acordo com normas que orientam sua elaboração. A finalidade é, então, fornecer aos interessados algum grau de confiança nas demonstrações fnanceiras objeto de análise.\r\nAinda que a Forensic Analytics guarde bastante similaridade com a auditoria financeira, dela se diferencia pelo fato de ter por objetivo identificar (detecção) erros intencionais e não intencionais e os vieses nos dados objeto de análise.\r\nAs circunstâncias em que a Forensic Analytics é utilizada caracteriza-se pela ação pró-ativa da entidade buscando pela ocorrência de fraudes e outras anomalias (abuso e desperdício) ou ainda em situações em que a fraude foi descoberta e objetiva-se avaliar a extenção da mesma e se outros esquemas semelhantes estão em operação.\r\nAs principais abordagens utilizadas em Forensic Analytics, segundo Nigrini, são os testes de alto nível (análise preliminar dos dados), os testes de duplicação excessiva de dígitos, testes para números redondos, testes para outliers e testes para mudanças no padrão temporal dos dados.\r\nAs ferramentas apresentadas no livro de Nigrini e também no livro Fraud and Fraud Detection de Sunder Gee, baseadas na aplicação dos testes acima mencionados bem como em outros do gênero, diferem bastante da abordagem feita, por exemplo, no livro Fraud Analytics: Using Descriptive, Predictive, and Social Network Techniques - A Guide to Data Science for Fraud Detection, de Bart Baesens, Veronique Van Vlasselaer e Wouter Werbeke, que propõe o uso de técnicas de aprendizado de máquina como ferramentas para detecção de fraudes.\r\nClaramente o R tem muito mais a oferecer na implementação de técnicas de machine learning do que o Excel e o Access.\r\nA propósito, destacamos que na 2a edição de seu livro, Nigrini já traz implementações de algumas das técnicas apresentadas no livro usando também o IDEA, R, SAS, Tableau e Minitab.\r\nEspecificamente em relação ao R o autor menciona que “(…) R é um programa flexível capaz de fazer virtualmente qualquer coisa relacionada à análise de dados, e para tanto utiliza uma interface baseada em linha de comando e sua própria sintaxe e linguagem de programação”\r\nBoa parte do que é discutido no Capítulo 2 da 2a edição do livro de Nigrini é abordado em outros contextos como Análise Exploratória de Dados e o R oferece excelentes recursos para a execução desses procedimentos que em essência tem por objetivo a obtenção de uma visão geral e compreensão dos dados em análise.\r\nVamos agora dar uma olhada nos recursos que o R tem a oferecer para a execução de cada uma das etapas da auditoria forense mencionadas por Nigrini.\r\nObtenção dos Dados\r\nA importação dos dados é sem dúvida uma etapa crítica no processo de análise. Os dados estão armazenados em arquivos e estes podem ser de diversos tipos, o que impõe ao usuário alguns desafios a depender do tipo de arquivo. Por exemplo, importar dados armazenados em arquivos .csv é bem mais fácil do que importar dados armazenados em arquivos .pdf.\r\nExistem diversos pacotes no R que disponibilizam funções para acessar dados armazenados em diversos formatos, sejam abertos ou proprietários.\r\nVamos listar a seguir alguns pacotes disponíveis e dar a indicação dos tipos de arquivos a que se destinam:\r\nPacote\r\nTipo de arquivo\r\n{readxl}\r\nplanilhas do Excel.\r\n{readr}\r\narquivos texto em geral.\r\n{haeven}\r\narquivos dos softwarre estatísticos SAS, SPSS e Stata\r\n{rio}\r\ndiversos tipos de arquivo, incluindo os acima mencionados\r\n{jsonlite}\r\narquivos json\r\n{xml2}\r\narquivos xml e html\r\nAlém desses, existem pacotes que possibilitam o acesso a bancos de dados como os pacotes {DBI}, {odbc}, {RSQLite} dentre diversos outros.\r\nA obtenção de dados em páginas web mediante um procedimento conhecido por web scraping ou “raspagem de dados” pode ser feito com o pacotes {rvest} em combinação outros pacotes.\r\nDados disponibilizados em APIs podem ser obtidos com o pacote {httr}.\r\nNaturalmente existem muitos outros pacotes disponíveis, mas com estes já é possível fazer muita coisa.\r\nPreparação dos Dados\r\nDe forma bem simples, a etapa de preparação dos dados consiste em colocá-los em condições de serem submetidos à modelagem ou aos testes como os discutidos nos livros já citados, ou ainda serem visualizados, após sua importação.\r\nIsso implica que os dados, em geral, após serem importados precisam ser “limpos” e precisam ter sua consistência e integridade avaliados.\r\nEm relação à preparação e organização dos dados recomendamos a leitura dos artigos Tidy Data do Hadley Wickham e Data Organization in Spreadsheets de de Karl W. Broman & Kara H. Woo.\r\nEssa é muitas vezes uma etapa demorada e ter à disposição ferramentas adequadas que permitem o tratamento dos dados de forma simples faz muita diferença, e aqui é onde o R realmente brilha!\r\nPara realizar essa etapa de forma eficiente o R disponibiliza um amplo conjunto de pacotes e funções dos quais mencionaremos apenas o que mais utilizamos, sendo certo que muitos outros existem e podem ser tão bons ou melhores que os aqui elencados.\r\nPodemos iniciar com os pacotes que integram o denominado {tidyverse}. Da página (https://www.tidyverse.org/) deste “metapacote” tira-se a seguinte “definição”: \"The tidyverse is an opinionated collection of R packages designed for data science. All packages share an underlying design philosophy, grammar, and data structures.\r\nTrata-se de um conjunto de pacotes projetados para fazer ciência de dados o que claramente compreende o tipo de tarefa que encontramos na literatura de Forensic Analytics ou mesmo de Audit Data Analytics.\r\nTambém vale a pena destacar que os pacotes compartilham uma filosofia comum acerca da abordagem de análise de dados que integra a estrutura de dados utilizada e um conjunto de regras definindo como essa estrutra deve ser manipulada.\r\nOs pacotes que integram o {tidyverse} básico são:\r\n{dplyr} - manipulação de dados;\r\n{ggplot2} - produção de gráficos;\r\n{stringr} - manipulação de dados textuais;\r\n{tidyr} - organização dos dados;\r\n{readr} - importação de dados;\r\n{purrr} - programação funcional;\r\n{tibble} - extenção dos data frames;\r\n{forcats} - manipulação de dados categóricos expressos como fatores\r\nUm outro pacote importante que não integra o {tidyverse} básico é o {lubridate} que destina-se à manipulação de datas e horas.\r\nÀs vezes será necessário manipular uma grande quantidade de dados que não podem ser alocados na memória do computador, e para ajudar resolver esse problema alguns pacotes estão disponíveis, dentre os quais citamos: {sparklyr} e {disk.frame}.\r\nAnálise dos dados\r\nPara efetivamente realizar a análise dos dados, aqui entendida como a modelagem dos mesmos, também existem diversos outros pacotes, além dos que já foram mencionados nas seções precedentes, que podem auxiliar nessa tarefa.\r\nDada a ampla gama de possibilidade de modelagem, vamos remeter o leitor para a página de alguns livros disponíveis online para que o leitor veja algumas possibilidades oferecidas pelo R:\r\nAudit Analytics with R\r\nTidy Modeling with R\r\nText Mining with R\r\nFundamentals of Data Visualization\r\nOs exemplos acima apenas dão uma rápida visão das possibilidades que o R pode oferecer nessa área.\r\nUm site interessante é o https://bookdown.org/ que disponibiliza online\r\nlivros sobre o R de forma gratuita em sua grande maioria. O The Big Book of R do Oscar Baruffa é outro site na mesma linha.\r\nO livro Audit Analytics: Data Science for the Accounting Profession mostra o uso do R como ferramenta analítica num contexto puramente de auditoria. Aparentemente é o primeiro livro publicado nessa linha.\r\nComunicação dos Resultados\r\nNos trabalhos de auditoria essa é uma etapa fundamental sem a qual as etapas anteriores talvez não produzam os resultados esperados por melhor que tenham sido realizadas, visto que se os destinatários das análises não tomarem conhecimento dos resultados obtidos talvez nenhuma ação com base no trabalho realizado seja tomada.\r\nOs resultados obtidos podem ser apresentados por intermédio de relatórios, painéis ou apresentações. Veremos o que o R tem a oferecer para cada um desses formatos de comunicação.\r\nRelatórios\r\nPara a produção de relatórios o R apresenta um conjunto de opções que se fundamentam numa ferramenta básica: o markdown.\r\nO pacote {rmarkdown} oferece a infraestrutura básica sobre a qual outros pacotes mais especializados são construídos. Essencialmente o {rmarkdown} possibilita que texto e código em R (atualmente também Python e SQL) sejam combinados em um único arquivo .Rmd para a produção de um documento que pode ser um html, um pdf, ou um docx para citar os mais comuns.\r\nPara conhecer um pouco mais de rmarkdown recomendamos a leitura do texto RMarkdown: O mínimo que você precisa saber\r\nMais recentemente uma outra ferramenta chamada Quarto (também baseada em rmarkdown) foi incorporada ao conjunto de ferramentas disponíveis para a produção de textos.\r\nPacotes como o {pagedown} e o {pagedreport}, também facilitam muito a produção de relatórios.\r\nTambém os pacotes {officer} e {officedown} podem ser usados para a produção de documentos em Word ou PowerPoint a partir de documentos escritos em rmarkdown.\r\nPara mais informações sobre esses pacotes e alguns outros consultar https://ardata-fr.github.io/officeverse/\r\nA criação de tabelas é um elemento essencial nos relatórios e também aqui o R tem muito a oferecer. A função kable() do pacote {knitr} pode ser usada para criar tabelas mais básicas e caso haja a necessidade de elaborar tabelas mais complexas os pacotes {kableExtra}, {gt} e {flextable}, para citar apenas três, podem dar conta do recado. Mas vários outros estão disponíveis para essa finalidade.\r\nPainéis\r\nÉ cada vez mais comum que os resultados de análises sejam mostrados não só atrávés de relatórios, mas também de painéis ou, como gostam alguns, dashboards.\r\nOs pacotes {flexdashboard}, {shinydashboard} e {shiny} disponibilizam recursos para a construção de painéis estáticos ou interativos.\r\nOs gráficos dinâmicos são elementos essenciais nesses “artefatos” e pacotes como o {plotly}, {echarts4r}, {leaflet}, {dygraphs}, {highcharter}, {gganimate}, {visNetwork} {networkD3}, {rAmCharts} podem ser muito úteis.\r\nApresentações\r\nAs apresentações são, também, uma forma muito comum de apresentação dos resultados e também aqui o R oferece excelentes opções. Além dos pacotes {officer} e {flexdashboard} já citados, o pacote {xaringan} possibilita a criação de apresentações no formato html.\r\nPara uma visão geral do {xaringan} recomenamos ao leitor dar uma olhada nesse material da Bia Milz: https://beatrizmilz.com/talk/2021-3rday-xaringan/.\r\nOutros pacotes de interesse\r\nAlém dos pacotes acima mencionados, alguns outros podem ser usados para implementar técnicas típicas de Forensic Analytics. Por exemplo, os pacotes {benford.analysis} e {benford} e {BenfordTests} implementam análises baseadas na Lei de Benford.\r\nPara a detecção de outliers também existem algumas opções, uma delas sendo o pacote {outliers}.\r\nConsiderações Finais\r\nA popularidade do Excel entre os profissionais de área de auditoria é certamente a razão de ser a ferramenta escolhida para a implementação das técnicas apresentadas no livro, apesar de suas limitações para realização de análise de dados.\r\nO R, por outro lado, apesar de oferecer enormes vantagens, não é uma ferramenta popular entre os profissionais de auditoria, ao menos por enquanto.\r\nComparado ao Excel o R possui, certamente, uma curva de aprendizado mais longa, exigindo uma dedicação muito maior para o seu aprendizado o que por vezes pode tornar-se uma barreira intransponível. Por outro lado, uma vez vencida essa dificuldade inicial, esse esforço adicional acaba por ser compensado em razão da eficiência que se obtém na realização das tarefas de análise de dados.\r\nO uso do R traz como um benefício adicional a possibilidade de reprodutibilidade das análises realizadas, já que todo o código utilizado para a realização das análises fica disponível, possibilitando a qualquer pessoa replicar o que foi feito, o que não é tão simples quando de usa uma planilha eletrônica.\r\nBem, era isso. Espero que tenham gostado.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-02-05T17:47:02-03:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-01-02-visualizao-de-dados-simples-em-auditoria/",
    "title": "Visualização de Dados em Auditoria",
    "description": "Neste post mostramos como criar visualizações de dados simples usando\no pacote {ggplot2} e falamos da importância da visualização de dados\nna auditoria.",
    "author": [
      {
        "name": "Marcos F. Silva",
        "url": {}
      }
    ],
    "date": "2022-01-01",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nIntrodução\r\nDescrição do conjunto de dados\r\nProdução de gráficos\r\n\r\n\r\nIntrodução\r\nNeste post nosso objetivo é mostrar como utilizar o R para implementar visualização de dados em um contexto auditoria. Para isso vamos utilizar um conjunto de dados apresentado no artigo Data Analytics for Financial Statement Audits de autoria de Trevor R. Stewart. Este artigo é um dos seis que integram o livro Audit Analytics and Continuous Audit: Looking Towards the Future, disponível para download gratuitamente.\r\nO conjunto de dados está no arquivo on_the_go_stores.xlsx e é também utilizado no Capítulo 3 do Guia de Auditoria da AICPA Analytical Procedures onde, por meio de um estudo de caso, ilustra a aplicação de procedimentos analíticos tanto no planejamento quanto na aplicação de testes substantivos.\r\nVamos trabalhar o exemplo denominado Simple DA Visualization apresentado no artigo em referência com o objetivo de obter uma melhor compreensão do ambiente de negócio da entidade e identificar riscos de erros materiais.\r\nO conjunto de dados on_the_go_stores.xlsx utilizado neste post pode ser obtido no reposítório Usando R em Auditoria\r\nNo contexto de auditoria financeira a utilização de visualização de dados tem sido cada vez mais enfatizada como técnica indispensável à adequada implementação dos procedimentos analíticos.\r\nNa literatura mais tradicional de auditoria as técnicas mais comumente citadas para utilização em procedimentos analíticos são: (a) Análise de tendências, (b) Análise de quocientes, (c) Testes de razoabilidade, e (d) Análise de regressão.\r\nModernamente essas técnicas podem ser ampliadas para incluir também\r\ntécnicas que compõem o que se chama de aprendizado de máquina (machine learning)\r\nA NBC TA 520 define Procedimentos Analíticos como “…avaliações de informações contábeis por meio de análise das relações plausíveis entre dados financeiros e não financeiros. (…) compreendem, também, o exame necessário de flutuações ou relações identificadas que são inconsistentes com outras informações relevantes ou que diferem significativamente dos valores esperados.”\r\n\r\nDescrição do conjunto de dados\r\nOs dados dizem respeito a uma cadeia de lojas de conveniência chamada On the Go Stores. A rede possui 23 lojas de conveniência localizadas no sudeste dos Estados Unidos. Uma parte dos dados pode ser vista na figura a seguir:\r\n\r\n\r\n\r\nCinco das vinte e três lojas (lojas 1, 4, 10, 13 e 22) abriram durante o ano. As operações variam em razão da localização geográfica e do mix de produtos vendidos. A localização de uma loja baseia-se em diversos fatores tais como a concorrência e o ambiente econômico da localidade.\r\nDe modo geral as operações de uma loja não mudam muito, a não ser que uma nova linha de produtos seja introduzida, como por exemplo a venda de combustível, desconto de cheques ou venda de bilhetes de loteria. O mix de produtos e serviços pode variar e o fator mais importante é se a loja vende combustível. Essas linhas de produtos adicionais em geral afetam o volume de clientes bem como o número de empregados trabalhando em horário integral.\r\n\r\nProdução de gráficos\r\nUso no contexto de conhecimento do negócio\r\nO que vamos fazer é tentar replicar os gráficos apresentados no artigo, um dos quais é reproduzido a seguir:\r\n\r\n\r\n\r\nPara tanto vamos utilizar o pacote ggplot2 que integra o tidyverse.\r\nVamos carregar os pacotes necessários:\r\n\r\n\r\nlibrary(readxl)\r\nlibrary(ggplot2)\r\nlibrary(tidyr)\r\nlibrary(forcats)\r\nlibrary(dplyr)\r\n\r\n\r\n\r\nAgora vamos importar o conjunto de dados:\r\n\r\n\r\nstores <- read_excel(\"on_the_go_stores.xlsx\")\r\nhead(stores)\r\n\r\n\r\n# A tibble: 6 x 7\r\n  store vendas_ano_anterior_aud vendas_ano_corrente inventario_ano_co~\r\n  <dbl>                   <dbl>               <dbl>              <dbl>\r\n1     1                      NA              781793              48725\r\n2     2                 1165221             1146438              44171\r\n3     3                 1147430             1195004              45714\r\n4     4                      NA              951784              37218\r\n5     5                 2037463             1981409              45826\r\n6     6                 2257920             2300671              53862\r\n# ... with 3 more variables: area_deposito <dbl>,\r\n#   qtd_media_empregados <dbl>, vende_gasolina <dbl>\r\n\r\nPara a reprodução do gráfico, será necessário realizar uma pequena modificação no conjunto de dados. Essa modificação consiste em transformar as colunas vendas_ano_anterior_aud e vendas_ano_corrente em uma única variável que chamaremos periodo_vendas. Para isso, vamos utilizar a função pivot_longer() do pacote tidyr.\r\n\r\n\r\nstores_long <- stores %>%\r\n                  pivot_longer(cols = c(\"vendas_ano_anterior_aud\", \"vendas_ano_corrente\"),\r\n                               names_to = \"periodo_vendas\",\r\n                               values_to = \"valor_vendas\")\r\n\r\nhead(stores_long)\r\n\r\n\r\n# A tibble: 6 x 7\r\n  store inventario_ano_~ area_deposito qtd_media_empre~ vende_gasolina\r\n  <dbl>            <dbl>         <dbl>            <dbl>          <dbl>\r\n1     1            48725          2500             11                0\r\n2     1            48725          2500             11                0\r\n3     2            44171          2500             11.3              0\r\n4     2            44171          2500             11.3              0\r\n5     3            45714          2500             12.5              0\r\n6     3            45714          2500             12.5              0\r\n# ... with 2 more variables: periodo_vendas <chr>, valor_vendas <dbl>\r\n\r\nFeita essa modificação, o gráfico pode ser reproduzido da seguinte forma:\r\n\r\n\r\nggplot(stores_long, aes(x=factor(store), y=valor_vendas)) + \r\n  geom_point(aes(shape=periodo_vendas, size=periodo_vendas), color=\"blue\") +\r\n  scale_shape_manual(values = c(1, 20)) +\r\n  scale_size_manual(values = c(4, 3.5)) +\r\n  theme_bw() +\r\n  xlab(\"ID da Loja\") + \r\n  ylab(\"Valor das Vendas\") + \r\n  ggtitle(\"Vendas das Lojas - Ano Atual e Anterior\")\r\n\r\n\r\n\r\n\r\nO gráfico sugere haver um agrupamento das lojas em função da ordenação por ID. É possível identificar os seguintes grupos de lojas: 1-4, 5-9, 10-13, 14-21 e 22-23.\r\nEste aparente agrupamento das lojas pode ser resultado da localização das lojas ou algum outro fator relacionado ao ID da loja ou simplesmente um padrão espúrio que emergiu por coincidência. Alguma investigação adicional será necessária.\r\nO gráfico anterior pode ser modificado para que revele novas informações ao menos de duas outras formas. Em vez de plotar as vendas em função dos valores ordenados do ID das lojas pode-se plotar em função da ordenação dos valores das vendas das lojas com menores vendas para as lojas com maiores vendas. Além disso, pelo fato de sabermos que as lojas que vendem combustível tendem a ter um volume de vendas maior, parece razoável distinguir as que vendem combustível das que não vendem. O gráfico apresentado no artigo é o seguinte:\r\n\r\n\r\n\r\nO código a seguir a mostra como incorporar as duas alterações propostas de forma a produzir um gráfico semelhante ao apresentado no artigo.\r\n\r\n\r\nggplot(stores_long,\r\n       aes(x=fct_reorder2(factor(store), periodo_vendas,valor_vendas, .desc = FALSE),\r\n           y=valor_vendas)) + \r\n  geom_point(aes(shape=periodo_vendas,\r\n                 size=periodo_vendas),\r\n                 color=\"blue\") +\r\n  scale_shape_manual(values = c(1, 20)) +\r\n  scale_size_manual(values = c(4, 3.5)) +\r\n  theme_bw() +\r\n  xlab(\"ID da Loja\") + \r\n  ylab(\"Valor das Vendas\") + \r\n  ggtitle(\"Vendas das Lojas - Ano Atual e Anterior\")+\r\n  facet_wrap(~ vende_gasolina, scales = \"free_x\") \r\n\r\n\r\n\r\n\r\nO gráfico nos mostra que as cinco lojas que abriram durante o ano não vendem combustível e estão entre as que possuem as menores vendas, conforme esperado. Também como esperado, as lojas que vendem combustível possuem um volume de vendas bem maior que as outras. Um exceção notável é a loja n. 9 que parece ter uma performance de vendas igual às lojas que vendem combustível, o que não é algo esperado.\r\nO auditor deve conferir para ver se a loja está corretamente classificada e, se confirmada que é uma loja que não vende combustível, tentar entender as razões sua performance tão superior às demais.\r\n\r\nUso no contexto de testes de razoabilidade\r\nSeguindo ainda no exemplo contido no artigo em referência, outra aplicação de visualização de dados é feita no âmbito da aplicação de um teste de razoabilidade pelo qual faz-se a avaliação das vendas em função da área das lojas (vendas por unidade de área das lojas) comparando-se os valores obtidos com um valor de referência ( $490 ) fornecido pela Associação Nacional das Lojas de Conveniência (NACS, na sigla em inglês).\r\nO gráfico apresentado no artigo é mostrado a seguir:\r\n\r\nPara reproduzir o gráfico acima será necessário antes calcular o valor das vendas por unidade de área da loja. Estas duas variáveis são valor_vendas e area_deposito. A nova variável é o quociente de ambas.\r\n\r\n\r\nstores_long <- stores_long %>% \r\n  mutate(vendas_unid_area =  valor_vendas / area_deposito)\r\n\r\n\r\n\r\nAgora o gráfico acima pode ser replicado conforme mostrado a seguir:\r\n\r\n\r\nstores_long %>% \r\n  filter(periodo_vendas == \"vendas_ano_corrente\") %>%\r\n  mutate(vende_gasolina = factor(vende_gasolina)) %>% \r\nggplot(aes(x=fct_reorder2(factor(store), periodo_vendas,valor_vendas, .desc = FALSE),\r\n           y=vendas_unid_area)) + \r\n  geom_point(aes(shape=vende_gasolina), color=\"blue\", size=4) +\r\n  scale_shape_manual(values = c(1, 8)) +\r\n  geom_hline( yintercept = 490, color=\"red\", size=1 ) +\r\n  theme_bw() +\r\n  xlab(\"ID da Loja (Ordenado por Valor de Vendas)\") + \r\n  ylab(\"Valor das Vendas por 'Square Foot'\") + \r\n  ggtitle(\"Vendas por Unidade de Área das Lojas \\n ( Comparação com Valor de Referência )\")\r\n\r\n\r\n\r\n\r\nO gráfico mostra que as cinco lojas que abriram durante o ano possuem os menores valores de vendas por unidade de área. Lojas que não vendem combustível estão todas abaixo do valor de referência e três das que vendem combústível estão bem acima do valor de referência.\r\n\r\nA figura abaixo, também constante do artigo, mostra que a rotatividade do estoque ( vendas \\(\\div\\) estoque ) é significativamente maior para as lojas que vendem combustível (36 a 50 vezes) do que para as lojas que não vendem combustível (16 a 33 vezes).\r\n\r\nDa mesma forma como feito para o gráfico anterior, será necessário criar uma nova variável que indique a rotatividade do estoque. Isso é feito a seguir:\r\n\r\n\r\nstores_long <- stores_long %>% \r\n  filter(periodo_vendas == \"vendas_ano_corrente\") %>% \r\n  mutate(rotatividade = valor_vendas / inventario_ano_corrente)\r\n\r\n\r\n\r\nAgora o gráfico pode ser replicado conforme mostrado a seguir:\r\n\r\n\r\nstores_long %>%\r\nmutate(store = factor(store),\r\n       vende_gasolina = factor(vende_gasolina)) %>%   \r\nggplot(aes(x = fct_reorder(store, valor_vendas, .desc = FALSE),\r\n           y = rotatividade)) + \r\n  geom_point(aes(shape = vende_gasolina), color = \"blue\", size = 4) +\r\n  scale_shape_manual(values = c(1, 8)) +\r\n  theme_bw() +\r\n  xlab(\"ID da Loja \\n (Ordenado por Valor de Vendas no Ano Corrente)\") + \r\n  ylab(\"Rotatividade do Estoque\") + \r\n  ggtitle(\"Rotatividade do Estoque por Loja\")\r\n\r\n\r\n\r\n\r\nO gráfico evidencia que a rotatividade é significativamente maior para as lojas que vendem gasolina do que para as que não vendem.\r\nE para finalizar, o gráfico a seguir, mostra um diagrama de dispersão das vendas em função do número de empregados. Quando o conjunto de dados é dividido em dois subconjuntos com lojas que vendem combustível e das que não vendem verifica-se a existência de uma forte correlação, da ordem de 80% em cada caso, e o gráfico deixa claro que as lojas que vendem combustível possuem um volume de vendas por empregado maior que as lojas que não vendem combustível.\r\nCaso a análise fosse feita sem que os dados fossem dividos, o modelo de regressão não teria sido adequado e a correlação seria de apenas 15%. Este exemplo ilustra o seguinte ponto: os auditores devem utilizar a análise de dados para obter uma clara compreensão dos mesmos antes de tentar utilizar qualquer modelo.\r\nO artigo apresenta o seguinte gráfico: \r\n\r\nO código a seguir ilustra como obter o gráfico e modelos ajustados.\r\n\r\n\r\n# Correlação para as lojas que não vendem combustivel\r\n\r\nstores_long %>% \r\n  filter(vende_gasolina == 0) %>% \r\n  select(qtd_media_empregados, valor_vendas) %>%\r\n  cor()\r\n\r\n\r\n                     qtd_media_empregados valor_vendas\r\nqtd_media_empregados            1.0000000    0.8177876\r\nvalor_vendas                    0.8177876    1.0000000\r\n\r\n\r\n\r\n# Correlação para as lojas que vendem combustível\r\n\r\nstores_long %>% \r\n  filter(vende_gasolina == 1) %>% \r\n  select(qtd_media_empregados, valor_vendas) %>%\r\n  cor()\r\n\r\n\r\n                     qtd_media_empregados valor_vendas\r\nqtd_media_empregados            1.0000000    0.7805102\r\nvalor_vendas                    0.7805102    1.0000000\r\n\r\n\r\n\r\nstores_long %>% \r\n  mutate( vende_gasolina = factor(vende_gasolina)) %>%  \r\n  ggplot(aes(x = qtd_media_empregados, y = valor_vendas, shape = vende_gasolina)) +\r\n  geom_point( color = \"blue\", size = 4) +  \r\n  geom_smooth(method = \"lm\", se=FALSE, color=\"red\", formula = y ~ x) + \r\n  scale_shape_manual(values = c(1, 8)) +\r\n  geom_text(x = 8,  y = 2250000, label = \"r = 0.780\", stats = \"unique\", color=\"red\") +\r\n  geom_text(x = 12, y = 1500000, label = \"r = 0.817\", stats = \"unique\", color=\"red\") +\r\n  theme_bw() +\r\n  xlab(\"Qtd. Média de Empregados\") + \r\n  ylab(\"Vendas\") + \r\n  ggtitle(\"Correlação entre Vendas e Qtd. Empregados\")\r\n\r\n\r\n\r\n\r\nNota: O artigo inclui no gráfico o coeficiente de determinação (\\(R^2\\)) do modelo de regressão ajustado. Optei por incluir o coeficiente de correlação linear entre as variáveis.\r\nO artigo conclui esclarecendo que as conclusões e insights que o auditor pode extrair da análise realizada irá depender de questões específicas da entidade incluindo as expectativas do auditor.\r\nAs visualizações auxiliam o auditor a perceber padrões e relacionamentos e, ainda, eventuais resultados inesperados. Fica a cargo do auditor decidir, com base no conhecimento que tem da entidade, o que é relevante e o que, se houver, deverá ser objeto de análise adicional.\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-01-02-visualizao-de-dados-simples-em-auditoria/visualizao-de-dados-simples-em-auditoria_files/figure-html5/unnamed-chunk-4-1.png",
    "last_modified": "2022-01-08T21:58:52-03:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-11-22-uma-implmentao-em-r-do-rsf-test/",
    "title": "Uma Implementação em R do RSF Test",
    "description": "Nesse post nosso objetivo é mostrar como implementar o Relative Size Factor Test no R,\nque, sem dúvida nenhuma, é uma excelente opção para análise de dados em auditoria.",
    "author": [
      {
        "name": "Marcos F. Silva",
        "url": {}
      }
    ],
    "date": "2021-11-24",
    "categories": [
      "Detecção de Fraude"
    ],
    "contents": "\r\n\r\nContents\r\nIntrodução\r\nImplementação do Teste em R\r\nElaboração da Função rsf_factor_test()\r\nReferências e materiais adicionais\r\n\r\nIntrodução\r\nNo Capítulo 11 do livro Forensic Analytics (1a Edição), Mark Nigrini descreve e implementa, usando Access e Excel, o denominado Relative Size Factor Test - RSF Test, um teste para detecção de outliers em grupos de registros que compõem uma base de dados.\r\nEssencialmente o teste busca identificar grupos de registros, usualmente definidos por variáveis categóricas, para os quais o maior valor é significativamente maior que os demais valores do grupo.\r\nVamos ilustrar com um exemplo para tornar as coisas mais concretas. Suponha o seguinte conjunto de dados:\r\n\r\nA variável Vendor_No vai definir os grupos de registros, ou seja, cada código de vendedor define um grupo enquanto a variável Invoice_Amount vai ser onde vamos buscar os outliers, ou seja, onde vamos calcular o RSF Factor.\r\nO RSF Factor é calculado tomando-se o quociente entre o maior valor no grupo e o segundo maior valor, também no grupo. Simples assim.\r\nEm termos matemáticos:\r\n\\[\r\nRSF\\_factor = \\frac{maximo\\_valor\\_grupo}{segundo\\_maior\\_valor\\_grupo}\r\n\\]\r\nEntão para cada vendedor vamos identificar as faturas por ele emitidas, identificar os dois valores de interesse (primeiro e segundo maiores valores) e calcular o quociente.\r\nO autor levanta algumas questões de ordem prática relacionadas à implementação deste teste. A primeira questão diz respeito ao fato de que, grupos contendo apenas um valor não pode gerar um RSF_Factor pela simples razão de não existir um segundo maior valor. Também comenta ser uma boa ideia excluir valores menores que 1,00 ou 10,00 e não incluir valores negativos.\r\nO autor aponta também o desafio de identificar o segundo maior valor no grupo, o que requer estabelecer uma regra para definição do segundo maior valor nos casos em que se tem mais de um valor máximo no grupo.\r\nO autor propõe um passo a passo para a implementação do teste e mostra como implementá-lo usando o Microsoft Access e o Maicrosoft Excel.\r\nImplementação do Teste em R\r\nNosso objetivo neste documento é mostrar como o teste pode ser implementado usando o R e para isso vamos inicialmente escrever um script para a execução do teste que posteriormente será convertido em uma função.\r\nVamos utilizar um conjunto de dados simples (RSF.xlsx) para esse propósito.\r\nEsse conjunto de dados pode ser baixado na seguinte página: http://www.ashishmathur.com/compute-relative-size-factor-per-vendor/\r\nVamos à importação dos dados:\r\n\r\n\r\nlibrary(readxl)\r\nrsf <- read_excel(\"_RSF.xlsx\", range=\"A1:C25\")\r\nhead(rsf)\r\n\r\n\r\n# A tibble: 6 x 3\r\n  Vendor_No Invoice_No Invoice_Amount\r\n  <chr>     <chr>               <dbl>\r\n1 V4437     AP00048            21261 \r\n2 V4409     AP0008002          21392.\r\n3 V4550     AP000130           21542.\r\n4 V4437     AP000292           21729.\r\n5 V4526     AP0009             22758.\r\n6 V4429     AP0007402          23413.\r\n\r\nImportado o conjunto de dados e depois de algumas tentativas e erros e consultas à internet chegamos ao seguinte script para a execução do teste:\r\n\r\n\r\nlibrary(dplyr)\r\n\r\nresultado <- rsf %>% \r\n  group_by(Vendor_No) %>% \r\n  summarise(maior_valor = max(Invoice_Amount),\r\n            segundo_maior = nth(unique(Invoice_Amount), 2, order_by = desc(unique(Invoice_Amount))),\r\n            n = n()) %>% \r\n  filter(n > 1) %>% \r\n  mutate(rsf = round(maior_valor / segundo_maior, 3)) %>% \r\n  arrange(desc(rsf))\r\n\r\nresultado\r\n\r\n\r\n# A tibble: 7 x 5\r\n  Vendor_No maior_valor segundo_maior     n   rsf\r\n  <chr>           <dbl>         <dbl> <int> <dbl>\r\n1 V4550          25940.        21542.     2  1.20\r\n2 V4554          28747         25378.     2  1.13\r\n3 V4429          25098.        23413.     2  1.07\r\n4 V4439          25378.        24068.     5  1.05\r\n5 V4437          21729.        21261      2  1.02\r\n6 V4455          25472.        25004      4  1.02\r\n7 V4526          25940.        25753.     3  1.01\r\n\r\nUma coisa que não é feita no livro mas que nos parece interessante é montar um gráfico para visualizar o rsf.\r\n\r\n\r\nlibrary(ggplot2)\r\n\r\nresultado %>% \r\n    ggplot(aes(x=reorder(Vendor_No, rsf), y=rsf)) +\r\n    geom_point(color=\"blue\") +\r\n    xlab(\"Grupos\") +\r\n    ylab(\"RSF Factor\") +\r\n    theme_bw()\r\n\r\n\r\n\r\n\r\nDe acordo com o autor, a aplicação deste teste pode revelar erros de colocação de ponto decimal em dados de contas a pagar; situações nas quais, por exemplo, um valor de 3200.00 é inserido no sistema como 320000. Um erro parcial ocorre quando, por exemplo, um valor de 421.69 é inserido como 4216.90.\r\nOutra observação feita pelo autor é que o RSF Factor é mais assertivo quanto a existência de erro quando o grupo possui muitos registros.\r\nElaboração da Função rsf_factor_test()\r\nCom base na solução apresentada acima é razoavelmente simples elaborar uma função para a aplicação do teste em um conjunto de dados.\r\n\r\n\r\nrsf_factor_test <- function(df, group_column, value_column, exclude_low=FALSE){\r\n\r\n  if(exclude_low){\r\n    df <- df %>% filter(.data[[value_column]] > 1)\r\n  }\r\n  \r\n  df %>% \r\n    group_by(.data[[group_column]]) %>% \r\n    summarise(Max_Value = max(.data[[value_column]]),\r\n              Second_Max = nth(unique(.data[[value_column]]), 2, order_by = desc(unique(.data[[value_column]]))),\r\n              N = n()) %>% \r\n    filter(N > 1) %>% \r\n    mutate(RSF_Factor = round(Max_Value / Second_Max, 3)) %>% \r\n    arrange(desc(RSF_Factor))\r\n  \r\n}\r\n\r\n\r\n\r\nVamos aplicar a função ao nosso conjunto de dados:\r\n\r\n\r\nrsf_factor_test(rsf, \"Vendor_No\", \"Invoice_Amount\")\r\n\r\n\r\n# A tibble: 7 x 5\r\n  Vendor_No Max_Value Second_Max     N RSF_Factor\r\n  <chr>         <dbl>      <dbl> <int>      <dbl>\r\n1 V4550        25940.     21542.     2       1.20\r\n2 V4554        28747      25378.     2       1.13\r\n3 V4429        25098.     23413.     2       1.07\r\n4 V4439        25378.     24068.     5       1.05\r\n5 V4437        21729.     21261      2       1.02\r\n6 V4455        25472.     25004      4       1.02\r\n7 V4526        25940.     25753.     3       1.01\r\n\r\nNaturalmente que algumas melhorias e extensões podem ser incluídas na função. Por exemplo, a função pode calcular o quociente entre o valor máximo e a média do valores do grupo incluindo ou excluindo o valor máximo, como mencionado pelo autor. Por ora vamos deixar a função assim.\r\nReferências e materiais adicionais\r\nPara a elaboração deste documento a referência principal foi o Capítulo 11 do livro Forensic Analytics (1a Edição) do Mark Nigrini já mencionado anteriormente.\r\nPara mais informações sobre esse teste o leitor pode consultar também o seguinte link: https://www.ideascripting.com/Relative-Size-Factor-Test\r\nNo vídeo https://www.youtube.com/watch?v=fyRT84LLbyw Mark Nigrini faz uma revisão do capítulo 7 da 2a Ed. do livro Forensic Analytics que aborda esse tópico e comenta sobre casos reais de fraude.\r\nTambém tem um outro vídeo ( https://www.youtube.com/watch?v=5f-5ZE3uccQ )\r\nno qual o autor demonstra como implementar o teste com o Excel, também utilizando a 2a Edição do livro.\r\nDeixamos para o leitor testar a função no conjunto de dados examinado pelo autor no vídeo (2_Somerville_Sample.xlsx), que pode ser baixado no link disponibilizado na descrição do vídeo.\r\nOutro material interessante é o livro online Audit Analytics with R do Jonathan Lin que aborda esse teste no item “9.4.2 Relative size factor (RSF)”.\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-11-22-uma-implmentao-em-r-do-rsf-test/uma-implmentao-em-r-do-rsf-test_files/figure-html5/unnamed-chunk-3-1.png",
    "last_modified": "2021-11-24T01:47:04-03:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-02-04-expresses-regulares-e-auditoria-ser-que-d-match/",
    "title": "Expressões Regulares e Auditoria: Será que dá match?",
    "description": "O objetivo desse *post* é apresentar possíveis situações em que o uso de\nexpressões regulares pode ser útil em trabalhos de auditoria e com isso\nestimular os auditores a conhecer um pouquinho mais sobre esse recurso.",
    "author": [
      {
        "name": "Marcos F. Silva",
        "url": {}
      }
    ],
    "date": "2021-02-07",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nDuas situações para motivação…\r\nMas o que é uma expressão regular?\r\nOnde aprofundar os conhecimentos\r\n\r\nDuas situações para motivação…\r\nImagine que você está realizando uma auditoria e que após executar alguns testes baseados na Lei de Benford você tenha descoberto que os valores que iniciam pelos caracteres “50”1 tenham fugido muito ao que seria o esperado.\r\n\r\nA Lei de Benford é uma técnica de análise de dados bastante popular em auditoria para a detecção de não conformidades em valores numéricos e integra as denominadas Técnicas de Auditoria Assistidas por Computador - TAACs\r\nPara uma análise mais detalhada das não conformidades identificadas você resolve filtrar a base de dados para obter apenas os registros em que os valores da variável objeto de análise inicie com a string “50”. Expressões regulares podem ser utilizadas para auxiliar na realização desse filtro.\r\nUma outra situação em que conhecer expressões regulares pode ser bastante útil é realizar filtragem de bases de dados com base em variáveis cujos valores sejam texto. Um exemplo disso seria uma base de dados composta por notas de empenhos e se deseja filtrar os dados com base na variável que contenha a descrição da despesa para identificar os empenhos que tenham relação, por exemplo, com “COVID-19”.\r\nEssas são duas situações particulares de um caso mais geral que é aplicar filtros a uma base de dados utilizando expressões regulares.\r\nNa prática o uso de expressões regulares é muito mais amplo e auxilia bastante no processo de pré-processamento das bases de dados.\r\nMas acredito que a essa altura você deva estar se perguntando: “Legal, mas pra mim isso tá parecendo com o recurso de localizar existente no Word. Qual a vantagem de aprender isso?”\r\nDiferentemente do recurso de localizar, onde o “match” só ocorre se as strings coincidirem de forma exata, as expressões regulares são muito mais flexíveis para a definição de padrões que descrevam as strings desejadas e isso pode economizar muito tempo, em razão do auditor não precisar testar várias strings semelhantes.\r\n\r\nDiz-se que ocorreu um match quando o padrão de string expresso pela expressão regular coincide com um subconjunto das strings objeto de pesquisa. Isso ficará mais claro adiante.\r\nMas o que é uma expressão regular?\r\nDe acordo com o tutorial contido no pacote stringr, expressões regulares são ferramentas para a descrição de padrões em strings de caracteres de forma concisa e flexível.\r\nNão é minha intenção que este post seja um tutorial sobre expressões regulares, mas chamar a atenção dos auditores para esse recurso, mostrar possibilidades de aplicações e com isso estimular um aprofundamento dos conhecimentos. E para isso elenco mais adiante alguns recursos disponíveis na internet.\r\nVou usar inicialmente alguns exemplos muito simples para fins didáticos e depois faço aplicações em um conjunto de dados mais realista.\r\nUso funções do pacote stringr para demonstrar o uso das expressões regulares e aproveito para chamar a atenção dos auditores para o fato de que manipular valores textuais é uma habilidade que pode ser muito útil.\r\nConsidere o seguinte conjunto de strings:\r\n\r\n\r\nlibrary(stringr)\r\nlibrary(readr)\r\n\r\nnomes<- c(\"Marcolino\", \"Marcos\", \"José\", \"joão\", \"Marcio\")\r\nnomes\r\n\r\n\r\n[1] \"Marcolino\" \"Marcos\"    \"José\"      \"joão\"      \"Marcio\"   \r\n\r\nVamos agora supor que eu queira identificar nesse conjunto de strings (cada nome é uma string) aquelas que comecem com a letra jota maiúscula ou minúscula. Como deve ser a expressão que ‘casa’ com esse padrão?\r\nA função str_view() do pacote stringr permite verificar para quais strings houve um “match” com o padrão definido pela expressão regular. No exemplo, o padrão é: “string que inicia com a letra jota”.\r\n\r\n\r\nstr_view(string=nomes, pattern=\"^[Jj]\")\r\n\r\n\r\n\r\n{\"x\":{\"html\":\"<ul>\\n  <li>Marcolino<\\/li>\\n  <li>Marcos<\\/li>\\n  <li><span class='match'>J<\\/span>osé<\\/li>\\n  <li><span class='match'>j<\\/span>oão<\\/li>\\n  <li>Marcio<\\/li>\\n<\\/ul>\"},\"evals\":[],\"jsHooks\":[]}\r\nComo é possível ver pelo resultado, as strings José e joão ‘casaram’ com a expressão regular ^[Jj] que define o padrão desejado, ou seja, iniciam com a letra jota, seja ela maiúscula ou minúscula.\r\nA definição de expressões regulares faz uso de caracteres que possuem significado especial os quais são denominados de metacaracteres. Os metacaracteres ^ e [] tem funções bem específicas.\r\nA indicação de que a letra jota deve estar no ínício da string é dada pelo metacaractere ^, enquanto a indicação que o jota poderia ser maiúsculo ou minúsculo é dada pelo metacaractere [] que, no exemplo, informa que a string pode iniciar com qualquer dos caracteres em seu interior.\r\nE se quisermos identificar as strings que terminem com “o” ou “os”?\r\n\r\n\r\nstr_view(string=nomes, pattern=\"os?$\")\r\n\r\n\r\n\r\n{\"x\":{\"html\":\"<ul>\\n  <li>Marcolin<span class='match'>o<\\/span><\\/li>\\n  <li>Marc<span class='match'>os<\\/span><\\/li>\\n  <li>José<\\/li>\\n  <li>joã<span class='match'>o<\\/span><\\/li>\\n  <li>Marci<span class='match'>o<\\/span><\\/li>\\n<\\/ul>\"},\"evals\":[],\"jsHooks\":[]}\r\nEsse exemplo, embora ainda simples, já começa a indicar que as expressões regulares podem ser bem complexas. E de fato são.\r\nNesse exemplo, aparecem mais dois novos metacaracteres: $ e ?. O metacaractere $ é responsável por indicar que os caracteres desejados devem estar no final da string enquanto o metacaractere ? informa que o caractere que o precede é opcional, podendo aparecer ou não na string. Assim, essa expressão regular define o seguinte padrão: strings que terminem com o caractere o sucedido ou não do caractere s.\r\n\r\n\r\nstr_view(string=nomes, pattern=\"[nc]os?$\")\r\n\r\n\r\n\r\n{\"x\":{\"html\":\"<ul>\\n  <li>Marcoli<span class='match'>no<\\/span><\\/li>\\n  <li>Mar<span class='match'>cos<\\/span><\\/li>\\n  <li>José<\\/li>\\n  <li>joão<\\/li>\\n  <li>Marcio<\\/li>\\n<\\/ul>\"},\"evals\":[],\"jsHooks\":[]}\r\nNo exemplo acima, a expressão regular define o seguinte padrão: o caractere o precedido dos caracteres n ou c e sucedido ou não do caractere s no final da string.\r\nNaturalmente que existem muitos outros metacaracteres que não foram abordados nesses exemplos simples cujo objetivo é apenas passar a ideia do que sejam expressões regulares.\r\nConvido o leitor a consultar as referências elencadas mais adiante para um tratamento mais aprofundado do assunto.\r\nAgora vou usar um conjunto de dados um pouco mais realista. Trata-se de uma relação de empenhos obtida no portal da transparência do Tribunal de Contas do Estado da Paraíba.\r\n\r\nA relação de empenhos refere-se apenas à Secretaria Estado de Educação e compreende os meses de janeiro a dezembro de 2020.\r\n\r\n\r\nempenhos <- read_csv(\"_EmpenhosLista.csv\")\r\nhead(empenhos)\r\n\r\n\r\n# A tibble: 6 x 9\r\n  DT_EMPENHO DS_TIPO   NumNE       CD_ORGAO  ElementoDespesa DS_CREDOR\r\n  <chr>      <chr>     <chr>       <chr>     <chr>           <chr>    \r\n1 23/01/2020 PRINCIPAL 2020NE00001 VALOR DA~ 11-VENCIMENTOS~ 08.778.2~\r\n2 23/01/2020 PRINCIPAL 2020NE00002 VALOR DA~ 11-VENCIMENTOS~ 08.778.2~\r\n3 23/01/2020 PRINCIPAL 2020NE00003 VALOR DA~ 11-VENCIMENTOS~ 08.778.2~\r\n4 23/01/2020 PRINCIPAL 2020NE00004 VALOR DA~ 05-OUTROS BENE~ 08.778.2~\r\n5 23/01/2020 PRINCIPAL 2020NE00005 VALOR DA~ 11-VENCIMENTOS~ 08.778.2~\r\n6 23/01/2020 PRINCIPAL 2020NE00006 VALOR DA~ 11-VENCIMENTOS~ 08.778.2~\r\n# ... with 3 more variables: Valor_Despesa1 <chr>, Textbox7 <chr>,\r\n#   Valor_Despesa <chr>\r\n\r\nDurante o processo de exportação dos dados do site, algumas colunas indesejadas foram exportadas também, são elas: Textbox7 e Valor_Despesa. Vou eliminar essas colunas da base de dados:\r\n\r\n\r\nlibrary(dplyr)\r\nempenhos <- empenhos %>% \r\n              select(-Textbox7, -Valor_Despesa)\r\n\r\nglimpse(empenhos)\r\n\r\n\r\nRows: 6,781\r\nColumns: 7\r\n$ DT_EMPENHO      <chr> \"23/01/2020\", \"23/01/2020\", \"23/01/2020\", \"2~\r\n$ DS_TIPO         <chr> \"PRINCIPAL\", \"PRINCIPAL\", \"PRINCIPAL\", \"PRIN~\r\n$ NumNE           <chr> \"2020NE00001\", \"2020NE00002\", \"2020NE00003\",~\r\n$ CD_ORGAO        <chr> \"VALOR DA FOLHA DE PESSOAL - DEMAIS DA EDUCA~\r\n$ ElementoDespesa <chr> \"11-VENCIMENTOS E VANTAGENS FIXAS - PESSOAL ~\r\n$ DS_CREDOR       <chr> \"08.778.250/0001-69 - SECRETARIA DE ESTADO D~\r\n$ Valor_Despesa1  <chr> \"4.999.161,91\", \"2.388,72\", \"103.288,98\", \"1~\r\n\r\nA variável Valor_Despesa1 foi importada como caractere em vez de número. Com o uso de expressões regulares é possível fazer o pré-processamento com vistas à conversão para o formato numérico.\r\nVárias funções no R recebem como argumentos expressões regulares. No código a seguir vou utilizar a função str_replace_all() do pacote stringr para realizar a conversão da variável Valor_Despesa1 para o formato numérico.\r\nEssa função recebe como argumentos a variável contendo as strings, a expressão regular definindo o padrão a ser ‘casado’ e a string a ser utilizada para substituir a string que deu match com o padrão especificado.\r\nPara que a conversão possa ocorrer será necessário: (1) “remover” os pontos e (2) “substituir” as vírgulas por pontos.\r\nNo código a seguir eu utilizo a expressão regular \\\\. para ‘casar’ os pontos e substituir por uma string nula.\r\n\r\n\r\nempenhos <- empenhos %>% \r\n              mutate(Valor_Despesa1 = str_replace_all(Valor_Despesa1, \"\\\\.\", \"\"))\r\nglimpse(empenhos)\r\n\r\n\r\nRows: 6,781\r\nColumns: 7\r\n$ DT_EMPENHO      <chr> \"23/01/2020\", \"23/01/2020\", \"23/01/2020\", \"2~\r\n$ DS_TIPO         <chr> \"PRINCIPAL\", \"PRINCIPAL\", \"PRINCIPAL\", \"PRIN~\r\n$ NumNE           <chr> \"2020NE00001\", \"2020NE00002\", \"2020NE00003\",~\r\n$ CD_ORGAO        <chr> \"VALOR DA FOLHA DE PESSOAL - DEMAIS DA EDUCA~\r\n$ ElementoDespesa <chr> \"11-VENCIMENTOS E VANTAGENS FIXAS - PESSOAL ~\r\n$ DS_CREDOR       <chr> \"08.778.250/0001-69 - SECRETARIA DE ESTADO D~\r\n$ Valor_Despesa1  <chr> \"4999161,91\", \"2388,72\", \"103288,98\", \"14245~\r\n\r\nComo pode ser visto, os pontos foram removidos. A expressão regular \\\\. ‘casa’ os pontos. Como o ponto tem um significado especial em expressões regulares, para que seja possível casar o ‘ponto literal’ é necessário colocar as duas barras antes dele. Em expressões regulares, o ponto tem a função de ‘casar’ com qualquer caractere.\r\nAgora é necessário substituir a vírgula por ponto.\r\n\r\n\r\nempenhos <- empenhos %>% \r\n              mutate(Valor_Despesa1 = str_replace_all(Valor_Despesa1, \",\", \".\"))\r\nglimpse(empenhos)\r\n\r\n\r\nRows: 6,781\r\nColumns: 7\r\n$ DT_EMPENHO      <chr> \"23/01/2020\", \"23/01/2020\", \"23/01/2020\", \"2~\r\n$ DS_TIPO         <chr> \"PRINCIPAL\", \"PRINCIPAL\", \"PRINCIPAL\", \"PRIN~\r\n$ NumNE           <chr> \"2020NE00001\", \"2020NE00002\", \"2020NE00003\",~\r\n$ CD_ORGAO        <chr> \"VALOR DA FOLHA DE PESSOAL - DEMAIS DA EDUCA~\r\n$ ElementoDespesa <chr> \"11-VENCIMENTOS E VANTAGENS FIXAS - PESSOAL ~\r\n$ DS_CREDOR       <chr> \"08.778.250/0001-69 - SECRETARIA DE ESTADO D~\r\n$ Valor_Despesa1  <chr> \"4999161.91\", \"2388.72\", \"103288.98\", \"14245~\r\n\r\nAs vírgulas foram substituídas por pontos. Agora é só converter a variável para o formato numérico.\r\n\r\n\r\nempenhos <- empenhos %>% \r\n              mutate(Valor_Despesa1 = as.numeric(Valor_Despesa1))\r\nglimpse(empenhos)\r\n\r\n\r\nRows: 6,781\r\nColumns: 7\r\n$ DT_EMPENHO      <chr> \"23/01/2020\", \"23/01/2020\", \"23/01/2020\", \"2~\r\n$ DS_TIPO         <chr> \"PRINCIPAL\", \"PRINCIPAL\", \"PRINCIPAL\", \"PRIN~\r\n$ NumNE           <chr> \"2020NE00001\", \"2020NE00002\", \"2020NE00003\",~\r\n$ CD_ORGAO        <chr> \"VALOR DA FOLHA DE PESSOAL - DEMAIS DA EDUCA~\r\n$ ElementoDespesa <chr> \"11-VENCIMENTOS E VANTAGENS FIXAS - PESSOAL ~\r\n$ DS_CREDOR       <chr> \"08.778.250/0001-69 - SECRETARIA DE ESTADO D~\r\n$ Valor_Despesa1  <dbl> 4999161.91, 2388.72, 103288.98, 14245.66, 32~\r\n\r\nAgora a variável Valor_Despesa1 está no formato numérico.\r\nContinuando com nosso exemplo, vamos supor que eu queira filtrar a base de dados de forma que apenas os valores dos empenhos iniciados por “50” sejam selecionados. Usando a expressão regular já vista anteriormente posso fazer isso da seguinte forma:\r\n\r\n\r\nempenhos_50 <- empenhos %>% \r\n                  filter(str_detect(Valor_Despesa1, \"^50\"))\r\nknitr::kable(head(empenhos_50))\r\n\r\n\r\nDT_EMPENHO\r\nDS_TIPO\r\nNumNE\r\nCD_ORGAO\r\nElementoDespesa\r\nDS_CREDOR\r\nValor_Despesa1\r\n12/02/2020\r\nPRINCIPAL\r\n2020NE00253\r\nIMPORTANCIA EMPENHADA PARA A REALIZACAO DA DESPESA COM PAGAMENTO DE DIARIA CONFORME MAPA DE CONCESSAO EM ANEXO\r\n14-DIÁRIAS - CIVIL\r\n*.807.644- - JOSE PATRICIO DA SILVA\r\n50\r\n12/02/2020\r\nPRINCIPAL\r\n2020NE00254\r\nIMPORTANCIA EMPENHADA PARA A REALIZACAO DA DESPESA COM PAGAMENTO DE DIARIA CONFORME MAPA DE CONCESSAO EM ANEXO\r\n14-DIÁRIAS - CIVIL\r\n*.807.644- - JOSE PATRICIO DA SILVA\r\n50\r\n12/02/2020\r\nPRINCIPAL\r\n2020NE00257\r\nIMPORTANCIA EMPENHADA PARA A REALIZACAO DA DESPESA COM PAGAMENTO DE DIARIA CONFORME MAPA DE CONCESSAO EM ANEXO\r\n14-DIÁRIAS - CIVIL\r\n*.421.508- - TIAGO FRANCISO DE SOUSA DA SILVA\r\n50\r\n12/02/2020\r\nPRINCIPAL\r\n2020NE00258\r\nIMPORTANCIA EMPENHADA PARA A REALIZACAO DA DESPESA COM PAGAMENTO DE DIARIA CONFORME MAPA DE CONCESSAO EM ANEXO\r\n14-DIÁRIAS - CIVIL\r\n*.421.508- - TIAGO FRANCISO DE SOUSA DA SILVA\r\n50\r\n13/02/2020\r\nPRINCIPAL\r\n2020NE00261\r\nIMPORTANCIA EMPENHADA PARA A REALIZACAO DA DESPESA COM PAGAMENTO DE DIARIA CONFORME MAPA DE CONCESSAO EM ANEXO\r\n14-DIÁRIAS - CIVIL\r\n*.421.508- - TIAGO FRANCISO DE SOUSA DA SILVA\r\n50\r\n13/02/2020\r\nPRINCIPAL\r\n2020NE00262\r\nIMPORTANCIA EMPENHADA PARA A REALIZACAO DA DESPESA COM PAGAMENTO DE DIARIA CONFORME MAPA DE CONCESSAO EM ANEXO\r\n14-DIÁRIAS - CIVIL\r\n*.421.508- - TIAGO FRANCISO DE SOUSA DA SILVA\r\n50\r\n\r\nMais um exemplo. A variável DS_CREDOR possui a identificação do credor do empenho. Essa identificação consiste do número do CNPJ seguido do nome do fornecedor caso esse seja pessoa jurídica. No caso de pessoa física, essa descrição consiste de parte do CPF seguido do nome da pessoa.\r\nVamos supor que eu queira criar um novo campo na base de dados contendo apenas o CNPJ. Como eu posso fazer isso usando expressões regulares? Eu preciso criar uma expressão regular que defina o padrão de um CNPJ, que é: dois dígitos, ponto, três digitos, ponto, três dígitos, barra, três zeros, um, hífen, dois dígitos. Como eu crio uma expressão regular que “case” com esse padrão?\r\nUma possibilidade:\r\n\r\n\r\npadrao_cnpj <- \"\\\\d{2}\\\\.\\\\d{3}\\\\.\\\\d{3}/0001-\\\\d{2}\"\r\n\r\n\r\n\r\nExplicando um pouco. O metacaractere \\\\d{n} indica que o padrão buscado é n dígitos. O \\\\. indica que queremos ‘casar’ o ponto. Como o ponto possui um significado especial nas expressões regulares (é um metacaractere), é necessário precedê-lo com as duas barras.\r\nVamos agora criar a nova coluna.\r\n\r\n\r\nempenhos <- empenhos %>% \r\n              mutate(CNPJ_CREDOR = str_extract(DS_CREDOR, padrao_cnpj))\r\nglimpse(empenhos)\r\n\r\n\r\nRows: 6,781\r\nColumns: 8\r\n$ DT_EMPENHO      <chr> \"23/01/2020\", \"23/01/2020\", \"23/01/2020\", \"2~\r\n$ DS_TIPO         <chr> \"PRINCIPAL\", \"PRINCIPAL\", \"PRINCIPAL\", \"PRIN~\r\n$ NumNE           <chr> \"2020NE00001\", \"2020NE00002\", \"2020NE00003\",~\r\n$ CD_ORGAO        <chr> \"VALOR DA FOLHA DE PESSOAL - DEMAIS DA EDUCA~\r\n$ ElementoDespesa <chr> \"11-VENCIMENTOS E VANTAGENS FIXAS - PESSOAL ~\r\n$ DS_CREDOR       <chr> \"08.778.250/0001-69 - SECRETARIA DE ESTADO D~\r\n$ Valor_Despesa1  <dbl> 4999161.91, 2388.72, 103288.98, 14245.66, 32~\r\n$ CNPJ_CREDOR     <chr> \"08.778.250/0001-69\", \"08.778.250/0001-69\", ~\r\n\r\nA nova coluna foi criada apenas com o CNPJ como desejado. Mas vamos supor ainda que eu queira cruzar esta base de dados com uma outra tomando o CNPJ como chave para o cruzamento. Ocorre que nessa outra base o CNPJ está sem pontuação, ou seja, os CNPJ aparecem dessa forma: “08778250000169”. Assim eu preciso remover a pontuação na variável recém criada. Mais uma vez vou utilizar expressão regular.\r\n\r\n\r\nempenhos <- empenhos %>% \r\n              mutate(CNPJ_CREDOR = str_remove_all(CNPJ_CREDOR, \"[[:punct:]]\"))\r\nglimpse(empenhos)\r\n\r\n\r\nRows: 6,781\r\nColumns: 8\r\n$ DT_EMPENHO      <chr> \"23/01/2020\", \"23/01/2020\", \"23/01/2020\", \"2~\r\n$ DS_TIPO         <chr> \"PRINCIPAL\", \"PRINCIPAL\", \"PRINCIPAL\", \"PRIN~\r\n$ NumNE           <chr> \"2020NE00001\", \"2020NE00002\", \"2020NE00003\",~\r\n$ CD_ORGAO        <chr> \"VALOR DA FOLHA DE PESSOAL - DEMAIS DA EDUCA~\r\n$ ElementoDespesa <chr> \"11-VENCIMENTOS E VANTAGENS FIXAS - PESSOAL ~\r\n$ DS_CREDOR       <chr> \"08.778.250/0001-69 - SECRETARIA DE ESTADO D~\r\n$ Valor_Despesa1  <dbl> 4999161.91, 2388.72, 103288.98, 14245.66, 32~\r\n$ CNPJ_CREDOR     <chr> \"08778250000169\", \"08778250000169\", \"0877825~\r\n\r\nA expressão regular [[:punct:]] “casa” com os sinais de pontuação.\r\nMais um exemplo para finalizar. A variável CD_ORGAO contém a descrição da despesa objeto do empenho. Como poderíamos identificar os empenhos que se refiram a compra de merenda escolar, por exemplo?\r\nNo código a seguir vou usar expressão regular para “casar” a string MERENDA na descrição da despesa.\r\n\r\n\r\nempenhos_merenda <- empenhos %>% \r\n                      filter(str_detect(CD_ORGAO, \"MERENDA\")) \r\n\r\n\r\n\r\nO novo conjunto de dados empenhos_merenda contém apenas os registros referentes aos empenhos em que a string MERENDA aparece na descrição da despesa.\r\nEspero que estes exemplos tenham dado uma ideia do poder que as expressões regulares trazem para a análise de dados e, consequentemente, para a auditoria e que o post de modo geral tenha aguçado sua curiosidade para aprender mais sobre esta ferramenta fantástica.\r\nOnde aprofundar os conhecimentos\r\nCom uma rápida pesquisa na internet é possível encontrar uma grande quantidade de material sobre expressões regulares.\r\nListo a seguir alguns materiais que vão te ajudar a aprofundar o conhecimento sobre esse tópico.\r\nIntrodução ao regex com R\r\nBasic Regular Expressions in R - Cheat Sheet\r\nR for Data Science - Capítulo 14 Strings\r\nRegular expressions\r\nRegular Expressions in R - Albert Y. Kim\r\n\r\nNesse post vou adotar a seguinte terminologia: um caractere pode ser qualquer dígito, letra, pontuação, etc e uma string será um conjunto de caracteres. Assim, “@marcos2006”, “123456” e “Apt. 708” são strings↩︎\r\n",
    "preview": {},
    "last_modified": "2021-11-24T01:15:16-03:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-27-audit-analytics/",
    "title": "Audit Analytics",
    "description": "Neste _post_ meu objetivo é falar um pouco sobre o que é _Audit Analytics_ e onde encontrar material para estudo.",
    "author": [
      {
        "name": "Marcos F. Silva",
        "url": {}
      }
    ],
    "date": "2021-01-31",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nO que é Audit Analytics?\r\nA Análise de Dados Aplicada à Auditoria Hoje\r\nVantagens da Audit Analytics\r\nMaterial para Estudo\r\n\r\nO que é Audit Analytics?\r\nO uso de análise de dados ou métodos quantitativos ou machine learning em auditoria tem sido apresentado com a denominação de Audit Analytics ou Audit Data Analytics e um breve resumo e introdução a esta ‘disciplina’ pode ser consultado neste documento produzido pela Escola de Negócios da Universidade de Rutgers em 2013.\r\nO documento inicia declarando que “Audit analytics é o uso da tecnologia de análise de dados em Auditoria”. Depois, complementa informando que “Audit analytics é o processo de identificar, colher, validar, analisar e interpretar várias formas de dados dentro de uma organização para ajudar no desenvolvimento do propósito e missão da auditoria.”\r\nTambém é interessante elencar as possíveis aplicações da Audit Analytics no processo de auditoria, ainda segundo o documento em referência:\r\nRevisão analítica;\r\nAvaliação e testes de controles;\r\nTestes substantivos;\r\nDetecção de fraudes;\r\nAnálises em geral e produção de relatórios;\r\nTransações financeiras e não financeiras.\r\nO Guide to Audit Data Analytics da AICPA apresenta uma ‘definição’ de Audit Data Analytics (ADA) da qual gosto muito e que está no artigo Reimagining Auditing in a Wired World de autoria de Paul Byrnes e outros .\r\nDe acordo com os autores do artigo Audit Data Analytics é “a ciência e a arte de descobrir e analisar padrões, identificar anomalias e extrair outras informações úteis de dados subjacentes ou relacionados a um objeto de auditoria através de análise, modelagem ou visualização com o objetivo de planejar ou realizar a auditoria.”\r\nAinda de acordo com o Guia, a ADA “são técnicas que que podem ser usadas para realizar vários procedimentos de auditoria incluindo elementos de avaliação de risco, testes de controles, procedimentos substantivos (isto é, testes de detalhes ou procedimentos analíticos substantivos),ou procedimentos de fechamento de auditoria. ADAs e procedimentos analíticos estão interrelacionados mas nem todas as ADAs são procedimentos analíticos.”\r\nAs definições acima remetem a algumas ideias que me parecem centrais para o entendimento do que seja Audit Analytics.\r\nNo documento da Rutgers essas ideias aparecem bem explícitas na definição, que é o uso de tecnologia, para realizar análise de dados na área de auditoria.\r\nNa definição contida no artigo de Byrnes et all, aparecem as ideias de “descobrir e analisar padrões”, “identificar anomalias”, “extrair outras informações úteis de dados”, “através de análise, modelagem ou visualização” que estão associadas a análise de dados e, como decorrência, a todo o conhecimento técnico relacionado a essa área do conhecimento.\r\nOs dados objeto de análise são aqueles “subjacentes ou relacionados a um objeto de auditoria” com o objetivo de “planejar ou realizar a auditoria”, que fala da área de conhecimento na qual a análise de dados será aplicada, a auditoria. Também aqui, existe um corpo de conhecimento específico com o qual os profissionais oriundos das ciências contábeis estão mais familiarizados.\r\nNessa definição não há menção a nada que remeta à tecnologia. Mas me parece claro que essa omissão talvez tenha relação com o fato de que as tecnologias são passageiras, a todo momento estão surgindo novas e melhores ferramentas capazes de realizar a análise dos dados de forma satisfatória.\r\nA definição do Guia da AICPA faz alusão ao conceito de análise de dados ao associar a Audit Analytic a um conjunto de técnicas e enfatiza bastante a área de aplicação, ao detalhar onde na auditoria podem ser utilizadas.\r\nTambém aqui não há menção à tecnologia. O foco da definição está nas áreas de aplicação e faz todo o sentido que assim seja, visto que o objetivo do Guia é, declaradamente, “fornecer uma introdução e visão geral às técnicas de análise de dados para auxiliar os auditores de demonstrações financeiras na aplicação dessas técnicas durante realização de uma auditoria”.\r\nComo pode ser visto, o Guia da AICPA é direcionado a auditores, auditores de demonstrações financeiras, os quais usualmente são contadores cuja formação, ao menos aqui no Brasil, é deficiente em análise de dados e uso de tecnologia. Não é incomum que os livros texto nacionais de auditoria ainda ensinem a executar testes de auditoria utilizando papel colunado!\r\nPara esse público, o interesse está em saber como essas técnicas podem ser incorporadas em seu trabalho, quais são as vantagens, quais são os custos de adoção e desvantagens.\r\nNesse site, o meu foco será a implementação prática das técnicas de análise de dados em auditoria, utilizando como tecnologia o ambiente R. Não é meu objetivo discutir conceitos de auditoria financeira ou de qualquer outro tipo de auditoria. Para quem quiser conhecer esses conceitos recomento o Manual de Auditoria Financeira do TCU.\r\nUma questão que pode surgir é: que técnicas de análise de dados são essas?\r\nElas variam desde as técnicas mais básicas então conhecidas por Técnicas de Auditoria Assistidas por Computador - TAACs até as modernas técnicas de mineração de dados passando por visualização de dados.\r\nSem a pretensão de ser exaustivo, elenco a seguir algumas técnicas citadas na literatura como utilizadas em trabalhos de auditoria:\r\nTécnicas de Auditoria Assistidas por Computador - TAACs: estatistica descritiva, distribuição de frequencias e tabulação cruzada, sumarização, estratificação, análise de tendências, Aging, teste de duplicidade e gaps, lei de Benford, amostragem de auditoria, etc.\r\nVisualização de Dados:histograma, diagrama de dispersão, gráfico de linhas, gráfico de barras (simples, empilhados e justapostos), boxplot, etc.\r\nTécnicas Preditivas (aprendizado supervisionado): regressão (simples, múltipla, logística), árvores de decisão, máquinas de vetor suporte, etc.\r\nTécnicas Descritivas/Exploratórias (aprendizado não supervisionado): regras de associação, análise de cluster, análise de componentes principais, análise de redes sociais, mineração de texto, mineração de processos, etc.\r\n* etc.\r\nA Análise de Dados Aplicada à Auditoria Hoje\r\nNa introdução do livro Audit Analytics in the Financial Industry Jun Dai e Miklos Vasarhelyi colocam a seguinte questão: O que é Audit Analytics?\r\nEste livro é uma coletânea de artigos editada por Jun Dai, Miklos A. Vasarhelyi e Ann F. Medinets, publicado em 2019.\r\nOs autores observam que a tecnologia emergente da Audit Analytics (AA) vem sendo cada vez mais utilizada pelos auditores para extrair e processar dados oriundos de uma variedade de fontes para identificação de risco, coleta de evidências e, em última análise, dar suporte à tomada de decisão.\r\nAtualmente estão acessíveis aos auditores além dos dados oriundos dos sistemas contábeis dos clientes, dados públicos como os disponibilizados nas mídias sociais e na internet de modo geral, dados abertos governamentais, dados climáticos e dados oriundos da ‘internet das coisas’ (IoT).\r\nO auditor moderno não pode, e não deve, ficar limitado aos dados internos produzidos pela entidade auditada.\r\nOs autores chamam a atenção para o seguinte fato: “Os procedimentos analíticos tradicionais se baseiam fortemente em amostragens dos dados relacionados à auditoria. Não obstante, à medida que os sistemas de ERP estão rapidamente crescendo em popularidade entre as empresas, evidência suficiente não pode mais ser colhida apenas de uma amostra de dados. A Audit Analytics aumenta a população testada de amostras limitadas (subjetiva ou estatística) para milhões de transações na testagem de toda a população o que amplia a cobertura da auditoria de um pequeno percentual das transações para toda a população.”\r\nOs autores vêem a Audit Analytics como sucessora dos procedimentos analíticos que já a bastante tempo vem sendo utilizados pelos auditores externos como técnica para o planejamento, testes substantivos e fase de conclusão de auditoria.\r\nConsiderando que os procedimentos analíticos realizados na fase de planejamento da auditoria tipicamente usam dados agregados em alto nível os resultados obtidos com estes procedimentos fornecem apenas indicação geral inicial sobre a existência de erros materiais.\r\nPor outro lado as técnicas de AA podem ser utilizadas em dados ao nível de transações visto que estas técnicas mantém boa performance ainda quando utilizadas em grandes bases de dados e bases com alta dimensionaliade.\r\nComo resultado a AA pode aumentar a acurácia da avaliação de riscos e melhorar a qualidade do planejamento da auditoria.\r\nEm um artigo de 2003 chamado Audit at a Crossroads, Conan C. Albrech, em vista dos então recentes escândalos de fraude ocorridos em empresas como Enron, WorldCom, Homestore, Quest, Global Crossing, Adelphia, Xerox, Waste Management, Sunbeam e outras, que colocaram a atividade de auditoria independente em xeque e do claro gap de expectativa existente entre o que os auditores afirmam ser sua responsabilidade e o que os usuários das informações contábeis acreditam ser o produto de seu trabalho, já chamava a atenção para a necessidade de uma revisão no modelo de auditoria com vistas a focar na fraude de demostrações financeiras, propondo duas grandes modificações: análise de toda a população e detecção de fraude pró-ativa.\r\nNa visão do autor, existe uma necessidade de que métodos estatísticos e tecnológicos sejam inseridos no processo de auditoria com vistas a focar nas fraudes de demonstrações financeiras e para isso propõe as duas modificações acima elencadas.\r\nEssa proposição assenta no entendimento do autor de que “a disponibilidade de tecnologia e dados em formato digital torna possível realizar rotinas de mineração de dados de formas que historicamente tem sido muito custosa ou mesmo impossível.”\r\nDestaco que a aplicação de técnicas de mineração de dados na detecção de fraudes tem sido tão bem sucedida que uma disciplina própria chamada Fraud Analytics vem se desenvolvendo e ganhando cada vez mais espaço.\r\nVantagens da Audit Analytics\r\nVoltando ao artigo de Jun Dai e Miklos Vasarhelyi, as tecnologias emergentes de análise de dados possuem a capacidade de explorar vastas quantidades de dados em várias estruturas e formatos que não podem ser manipulados pelos procedimentos analíticos tradicionais.\r\nComo vantagens da AA sobre as técnicas mais tradicionais os autories citam: (1) audit data analytics tem um melhor custo benefício em termos de coleta de evidências, (2) muitas das técnicas de análise de dados são escaláveis, isto é, em geral ainda mantém boa performance quando trabalham com grandes quantidades de dados com alta dimensionalidade (muitas variáveis) e (3) algumas técnicas de AA também possuem a habilidade de identificar padrões nos dados com o uso de técnicas não supervisionadas, o que dispensa a necessidade de dados ‘rotulados’ (variáveis alvo) no conjunto de dados.\r\nMaterial para Estudo\r\nInfelizmente não há muito material de estudo em português sobre o tema Audit Data Analytics, mas em inglês já é possível encontrar alguns materiais interessantes.\r\nLivros\r\nAinda são poucos os livros dedicados ao assunto. Listo a seguir os que conheço:\r\nAudit Analytics in the Financial Industry\r\nAudit Analytics and Continuous Audit - Looking Toward the Future Coletânea de artigos disponível para download.\r\nGuide to Audit Data Analytics\r\nAudit Analytics - Data Science for the Accounting Profession\r\nBasic Audit Data Analytics with R\r\nData analytics for internal auditors\r\nData Analytics: Elevating Internal Audit’s Value\r\nTeses e Dissertações\r\nA universidade de Rutgers é um forte centro de pesquisa no uso das modernas técnicas de mineração de dados em auditoria financeira e ao longo dos últimos anos diversas teses de doutoramento foram produzidas abordando a aplicação de análise de dados em auditoria.\r\nListo abaixo algumas das teses que tratam da temática Análise de Dados em Auditoria as quais estão disponíveis para download e podem ser obtidas no seguinte link: https://rucore.libraries.rutgers.edu/\r\nTitulo\r\nAutor\r\nAno de Produção\r\nCluster Analysis for Anomaly Detection in Accounting\r\nSutapat Thiprungsri\r\nJan. 2011\r\nPredictive Audit Analytics: Evolving to a New Era\r\nSiripan Kuenkaikaew\r\nOut. 2013\r\nThe Application of Exploratory Data Analytis in Auditing\r\nQi Liu\r\nOut. 2014\r\nThe Application of Data Visualization in Auditing\r\nAbdullah Alawaddhi\r\nMai. 2015\r\nDeveloping Automated Applications for Clustering and Outlier Detection: Data Mining Implications for Auditing Practice\r\nPaul Eric Byrnes\r\nOut 2015\r\nAnalytics with Exception Prioritization, Consumer Search Volume, and Social Capital\r\nPei Li\r\nMai. 2016\r\nAuditing in Environments of Diverse Data\r\nBasma Moharram\r\nOut. 2016\r\nThree Essays on Audit Technology: Audit 4.0, blockchain, and Audit App\r\nJun Dai\r\nOut. 2017\r\nPublic Auditing, Analytics, and Big Data in the Modern Economy\r\nDeniz Appelbaum\r\nMai., 2017\r\nDesigning Continuous Audit Analytics and Fraud Prevention Systems Using Emerging Technologies\r\nYunsen Wang\r\nMai. 2018\r\nExploring New Audit Evidence: The Application of Process Mining in Auditing\r\nTiffany Chiu\r\nMai. 2018\r\nDeep Learning Applications in Audit Decision Making\r\nTing Sun\r\nMai. 2018\r\nThree Essays on Emerging Technologies in Accounting\r\nFeiqi Huang\r\nJan. 2019\r\nThree Essays on Audit Innovation: Using Social Media Information and Disruptive Technologies to Enhance Audit Quality\r\nAndrea M. Rozario\r\nMai. 2019\r\nApplying Textual Analysis to Auditing\r\nYue Liu\r\nMai. 2019\r\nThree Essays on Open Government Data and Data Analytics\r\nZamil S. Alzamil\r\nMai. 2019\r\nAudit Focused Process Mining: The Evolution of Process Mining and Internal Control\r\nAbddulrahman Alrefai\r\nMai. 2019\r\nThree Essays on the Adoption and Application of Emerging Technologies in Accounting\r\nZhaokai Yan\r\nOut. 2019\r\nArtigos\r\nTembem vale a pena dar uma conferida nos seguintes artigos disponíveis online:\r\nEmbracing the automated audit\r\nThe next frontier in data analytics\r\nIntroduction to Data Analysis for Auditors and Accountants\r\nRethinking the audit\r\nAudit at a Crossroads\r\nMateriais online diversos\r\nListo a seguir alguns materiais que estão disponíveis na internet:\r\nAudit Data Analytics - AICPA\r\nAudit Analytics - An innovative course at Rutgers\r\nAUDIT QUALITY THEMATIC REVIEW: THE USE OF DATA ANALYTICS IN THE AUDIT OF FINANCIAL STATEMENTS\r\nAudit Data Standards Python Example\r\nAudit Solution in R- Case Study: Analysis of General Ledger\r\nAudit Solution in R- Case Study: Analysis of Sales/AR\r\nAnalytical Procedures in R - Audit Data Analytics (ADA) Use Case\r\nraudit\r\nAudit Data Analytics (ADA) - stewartli\r\nData mining your general ledger with Excel\r\nAuditinsight\r\nAudit Analytics: Data Science for the Accounting Profession\r\nAudit Analytics with R - Jonathan Lin\r\nAudit Analytics: Data Science for the Accounting Profession\r\nJon Lin - repositório no GitHub\r\nCaso o leitor tenha conhecimento de algum material não elencado neste post, pode me encaminhar. Vou atualizando o post à medida que for tomando conhecimento de mais materiais.\r\nEspero que gostem.\r\nBoa leitura!\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-01-31T21:58:27-03:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-23-sobre-bigodes-e-violinos/",
    "title": "Sobre Bigodes e Violinos",
    "description": "Algumas considerações sobre alguns gráficos apresentados no capítulo Foundations of Audit Analystics do livro [Audit Analytics](https://www.springer.com/gp/book/9783030490904) do J. C. Westland.",
    "author": [
      {
        "name": "Marcos F. Silva",
        "url": {}
      }
    ],
    "date": "2021-01-24",
    "categories": [
      "Livro Audit Analytics"
    ],
    "contents": "\r\n\r\nContents\r\nConsiderações iniciais\r\nGráficos apresentados no capítulo\r\n\r\nConsiderações iniciais\r\nEm linhas gerais o capítulo trata da exploração grafica de variáveis em um conjunto de dados, dos tipos de variáveis e comenta sobre as estruturas de dados existentes no R (vetores, matrizes, arrays, data frames, listas e fatores) e comenta rapidamente sobre a importância de outras estruturas de dados importantes em contabilidade como as séries temporais, os dados geolocalizados e os grafos.\r\nUma novidade pra mim foi o pacote plotluck que eu não conhecia. O pacote se propõe a apresentar a melhor visualização dos dados com base em suas características.\r\nO livro possui um pacote associado chamado auditanalytics e pode ser instalado a partir do repositório do livo no GitHub onde estão disponíveis os dados utilizados, bem como notebooks com resumos dos capítulos do livro.\r\nO resumo do capítulo 2 está contido no arquivo ch_2_statistics_analytics.Rmd.\r\nA instalação do pacote pode ser feita da seguinte forma:\r\n\r\n\r\n# install.packages(\"devtools\")\r\ndevtools::install_github(\"westland/auditanalytics\")\r\n\r\n\r\n\r\nOs dados utilizados estão armazenados em arquivos csv no próprio pacote e nesse aspecto acho que um ponto de melhoria seria disponibilizá-los no formato .RData com a correspondente documentação como usualmente ocorre nos pacotes do R.\r\nA falta de documentação dos dados, tanto no livro como no repositório e no pacote é uma falha importante visto que uma boa análise de dados pressupõe um bom conhecimento dos mesmos.\r\nA disponibilização dos dados em arquivos csv torna a importação um pouco mais trabalhosa.\r\nGráficos apresentados no capítulo\r\nA motivação para eu escrever este post foi a percepção de que alguns dos gráficos apresentados no capítulo não me pareceram uma boa escolha ou talvez eu não tenha compreendido corretamente a proposta do autor.\r\nO capítulo apresenta ao leitor quatro tipos de gráficos: histograma, gráfico de violino, boxplot e diagrama de dispersão como formas de explorar os dados, o que me faz chamar a atenção para o fato de que os gráficos a serem utilizados pelo auditor em grande medida dependem do tipo de dado que se queira visualizar e que infelizmente o uso de gráficos pelos auditores como uma ferramenta de exploração é ainda bastante incipiente.\r\nO primeiro gráfico que eu gostaria de comentar consta da página 29 e tem por objetivo, nas palavras do autor:\r\n\r\n“Na figura a seguir estamos interessados em saber se a fraude em cartões de crédito é influenciada pelo valor pago ao auditor. A gente analisa uma variável binária examinando a variação em uma outra variável conforme os valores da mesma estejam associados ao valor 0 ou 1 da variável binária.”\r\n\r\nO gráfico em questão, utilizado com vistas a ilustrar a exploração de variáveis binárias, é o seguinte:\r\n\r\nShow code\r\nlibrary(auditanalytics)\r\nlibrary(ggplot2)\r\nlibrary(dplyr)\r\n\r\n# Importação dos dados\r\nsox_stats <- read.csv(system.file(\"extdata\", \"ch_2_data_types.csv\", package=\"auditanalytics\", mustWork=TRUE))\r\n\r\n# Gráfico\r\nggplot(sox_stats, aes(x=non_audit_fee, y=audit_fee, col=as.factor(card)))+\r\n  geom_violin() +\r\n  labs(col=\"Fraud = 1 (green)\")\r\n\r\n\r\n\r\n\r\nA primeira coisa a comentar é o uso do gráfico de violino, que por ser um gráfico muito pouco conhecido, certamente não seria uma opção para a grande maioria dos auditores.\r\nA ajuda da função geom_violin() diz que “O gráfico de violino é uma representação compacta de uma distribuição contínua. É uma mistura do geom_boxplot() e do geom_density(): o gráfico de violino é um gráfico de densidade ‘espelhado’ representado da mesma forma que um boxplot.”\r\nO “box and whisker plot” (gráfico de caixa e bigodes) ou simplesmente boxplot, é um gráfico que tem por objetivo apresentar a distribuição de uma variável quantitativa por intermédio dos quartis da distribuição e indicação de limites superiores e inferiores denominados “cercas”.\r\nO boxplot e o gráfico de violino tem função semelhante, sendo que o gráfico de violino tem a vantagem de mostrar além da variabilidade dos dados e os quartis a forma da distribuição da variável por intermédio de sua densidade.\r\nA seguir apresento o mesmo conjunto de dados usando um boxplot, um gráfico de densidade e um gráfico de violino para tentar realçar a diferença entre os dois. Os dados são apresentados em cinza para dar uma ideia da localização dos mesmos.\r\n\r\nShow code\r\n# Boxplot\r\nsox_stats %>% \r\n  filter(card %in% c(0, 1)) %>% \r\nggplot(aes(y=audit_fee, x=as.factor(card)))+\r\n  geom_boxplot() +\r\n  geom_jitter(color=\"grey\", width = 0.2)\r\n\r\n\r\nShow code\r\n# Density\r\nsox_stats %>% \r\n  filter(card %in% c(0, 1)) %>% \r\nggplot(aes(y=audit_fee))+\r\n  geom_density(fill=\"lightblue\") +\r\n  facet_wrap(~ as.factor(card)) \r\n\r\n\r\nShow code\r\n# Violino\r\nsox_stats %>% \r\n  filter(card %in% c(0, 1)) %>% \r\nggplot(aes(y=audit_fee, x=as.factor(card)))+\r\n  geom_violin(draw_quantiles = c(0.25, 0.5, 0.75), color=\"blue\") +\r\n  geom_jitter(color=\"grey\", width = 0.2)\r\n\r\n\r\n\r\n\r\nExaminando os três gráficos é possível perceber que o gráfico de violino é um gráfico de densidade refletido, o que lhe dá a simetria observada e é também parecido com um boxplot, mostrando a variabilidade dos dados e os quartis.\r\nOnde existe maior concentração de dados, o gráfico de violino expressa isso na “largura” da curva, ou seja, onde tem maior concentração de dados a curva é mais larga.\r\nVoltando ao gráfico apresentado pelo autor, comentei que o objetivo declarado desse gráfico seria avaliar se a fraude em cartões de crédito é influencidada pelo valor pago aos auditores contratados.\r\nO gráfico faz uso de 3 variáveis: audit_fee, non_audit_fee e card e como já disse a base de dados não está comentada, não possuindo informações sobre o significado de cada variável e a estrutura geral dos dados. Assim, será necessário deduzir algumas coisas, o que não é o que se deve fazer na prática. O auditor deve buscar compreender com a maior clareza possível o significado de cada variável e suas particularidades para que possa ter condições de identificar situações que fujam da normalidade.\r\nAparentemente a variável card indica se a observação refere-se à fraude ou não. As outras variáveis indicam o valor pago aos auditores em razão de serviços contratados de auditoria e serviços não relacionados a auditoria.\r\nA primeira coisa que não fica clara no gráfico apresentado é o uso da variável non_audit_fee, uma variável quantitativa contínua, no eixo dos x. Considerando que o objetivo de um gráfico de violino é representar a distribuição da variável no eixo y, que deve ser quantitativa, a variável non_audit_fee não traz informação adicional para o gráfico.\r\nO mesmo problema ocorre com o gráfico apresentado na página 30, e que reproduzo a seguir utilizando o código apresentado no livro:\r\n\r\nShow code\r\nlibrary(tidyr)\r\n\r\nsox_stats$card <- as.integer(sox_stats$card)\r\n\r\nsox_stats1 <- gather(sox_stats,\r\n                     key=\"metric\",\r\n                     value = value,\r\n                     effective_303,\r\n                     mat_weak_303,\r\n                     sig_def_303,\r\n                     effective_404,\r\n                     auditor_agrees_303)\r\n\r\nggplot(sox_stats1, aes(x=non_audit_fee, y=audit_fee, col=metric)) +\r\n  geom_violin() +\r\n  scale_x_continuous(trans = \"log2\") +\r\n  scale_y_continuous(trans = \"log2\")\r\n\r\n\r\n\r\n\r\nO gráfico mostra os “violinos” igualmente espaçados e mostrando a mesma distribuição para a variável audit_fee para todos os valores da variável metric.\r\nNovamente aqui a variável non_audit_fee parece não ter qualquer influência no gráfico. Chama a atenção também o fato dos “violinos” serem todos iguais. O fato de não conhecermos em mais detalhes a base de dados dificulta a inspeção em busca de confirmação quanto a correção do resultado apresentado.\r\nO fato é que não é possível extrair maiores informações nem confirmar a exatidão do resultado sem um maior conhecimento dos dados.\r\nTambém os gráficos apresentados nas páginas 31 e 32 não me pareceram uma boa escolha para o propósito desejado.\r\nAqui o objetivo do autor é ilustrar a análise de variáveis ordinais, mais especificamente omissões ou duplicidades em variáveis que possuem valores sequenciais, tais como os números das notas fiscais.\r\nO gráfico a seguir, apresentado pelo autor, tem o objetivo de permitir a identificação visual de faturas duplicadas.\r\n\r\nShow code\r\nlibrary(lubridate)\r\n#library(kableExtra)\r\n\r\n## função para gerar datas aleatórias no ano corrente\r\nrdate <- function(x,\r\n                  min = paste0(format(Sys.Date(), '%Y'), '-01-01'),\r\n                  max = paste0(format(Sys.Date(), '%Y'), '-12-31'),\r\n                  sort = TRUE) {\r\n  dates <- sample(seq(as.Date(min), as.Date(max), by = \"day\"), x, replace = TRUE)\r\n  if (sort == TRUE) {\r\n    sort(dates)\r\n  } else {\r\n    dates\r\n  }\r\n}\r\n\r\n## Cria um data frame com 2 coluna e preenche com os valores 1 a 1000 \r\ninvoice_no <- date <- 1:1000  ## placeholder\r\njournal_ent_no <- cbind.data.frame(invoice_no,date)\r\n\r\n# Sorteia 1000 datas entre 01-01-2021 e 31-12-2021 e ordena\r\ndate <- rdate(1000)\r\n\r\n# Substitui os valores no campo 'data' pelas datas sorteadas\r\njournal_ent_no$date <- date[order(date)]\r\n\r\n# Adiciona duplicidades \r\njournal_ent_no$invoice_no <- seq(1,1000) + rbinom(1000,1,.1) # add some errors\r\n\r\n# Cria um novo data frame com identificação das duplicidades.\r\nduplicates <- duplicated(journal_ent_no$invoice_no)\r\nraw <- seq(1,1000)\r\njournal_dups <- cbind.data.frame(raw,duplicates)\r\n\r\n# Faz o gráfico\r\nggplot(journal_dups, aes(x=invoice_no, y=raw, col=duplicates)) + \r\n  geom_point()\r\n\r\n\r\n\r\n\r\nComo pode ser visto as duplicidades, em azul, não sobressaem muito. Como o espaço para o gráfico é pequeno os pontos se sobrepõem, dificultando a visualização.\r\nVou mostrar os registros iniciais do conjunto de dados utilizado para fazer esse gráfico:\r\n\r\nShow code\r\nhead(journal_dups, 10)\r\n\r\n\r\n   raw duplicates\r\n1    1      FALSE\r\n2    2      FALSE\r\n3    3      FALSE\r\n4    4      FALSE\r\n5    5      FALSE\r\n6    6      FALSE\r\n7    7      FALSE\r\n8    8      FALSE\r\n9    9      FALSE\r\n10  10      FALSE\r\n\r\nO conjunto de dados consiste em uma coluna indicando a numeração sequencial de 1 a 1000 e outra indicando se o número está duplicado na base ou não. Esta base de dados é derivada do que seria a base “original” que apresento a seguir:\r\n\r\nShow code\r\nhead(journal_ent_no, 10)\r\n\r\n\r\n   invoice_no       date\r\n1           1 2021-01-01\r\n2           2 2021-01-01\r\n3           3 2021-01-04\r\n4           4 2021-01-04\r\n5           5 2021-01-04\r\n6           6 2021-01-04\r\n7           7 2021-01-04\r\n8           8 2021-01-05\r\n9           9 2021-01-05\r\n10         10 2021-01-05\r\n\r\nA base de dados possui o número sequencial (invoice_no) e a data de emissão (date). Será que tem uma forma melhor de “visualizar” as duplicidades?\r\nVou apresentar aqui minha proposta:\r\n\r\nShow code\r\nggplot(journal_dups) + \r\n  geom_vline(xintercept = invoice_no,\r\n             color=ifelse(duplicates, \"blue\", \"white\"))+\r\n  labs(x=\"Numeração Sequencial\")\r\n\r\n\r\n\r\n\r\nAs linhas em azul indicam as faturas duplicadas. Podemos ver que a distribuição das duplicidades não aparenta ter um padrão definido.\r\nNa minha opinião a visualização das duplicidades ficou um pouco melhor. Naturalmente que quanto maior a quantidade de dados mais difícil ficará a visualização, principalmente se a mesma referir-se a visualizar os dados inidividualmente, como é o caso apresentado pelo autor.\r\nÉ claro que com um simples filtro é possível obter exatamente os números das faturas duplicadas, mas seria difícil perceber qualquer padrão nos dados caso eles existissem.\r\nO gráfico na página 32 é muito parecido mas busca identificar faturas omitidas. Acredito que a mesma solução pode ser usada na visualização.\r\nBem, por ora é tudo. Espero que tenham gostado.\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-01-23-sobre-bigodes-e-violinos/sobre-bigodes-e-violinos_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2021-01-24T23:57:17-03:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  }
]
